name: Scrape & Analyze (PrettyData — multi-source)

on:
  schedule:
    # run every 30 min (change to */15 for 15 min once stable)
    - cron: "*/30 * * * *"
  workflow_dispatch: {}

concurrency:
  group: prettydata-scrape
  cancel-in-progress: true

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 75
    permissions:
      contents: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Ensure data folder
        run: mkdir -p data

      # ==============================
      # DECOR
      # ==============================
      - name: Reddit (decor · last 45 days)
        continue-on-error: true
        env:
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USER_AGENT: ${{ secrets.REDDIT_USER_AGENT }}
        run: |
          python scripts/scrape_reddit.py \
            --subs HomeDecorating InteriorDesign CozyPlaces \
            --days 45 --limit 800 --sleep 1.0 \
            -o data/decor_reddit_raw.csv

      - name: RSS (decor · 60d)
        run: |
          python scripts/ingest_rss.py \
            --feeds data/feeds_decor.txt --days 60 \
            -o data/decor_rss_raw.csv

      - name: YouTube (decor · 60d)
        env:
          YOUTUBE_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}
        run: |
          python scripts/ingest_youtube.py \
            --queries data/youtube_queries_decor.txt \
            --days 60 --max_per_query 30 \
            -o data/decor_youtube_raw.csv

      - name: Combine (decor → all sources)
        run: |
          python scripts/combine_sources.py \
            --inputs data/decor_reddit_raw.csv data/decor_rss_raw.csv data/decor_youtube_raw.csv \
            -o data/decor_all_raw.csv

      - name: Analyze (decor)
        run: |
          python scripts/analyze_prettydata.py \
            --raw data/decor_all_raw.csv \
            --terms-file data/terms_decor.json \
            --prefix decor

      - name: Google Trends (decor)
        run: |
          python scripts/ingest_trends.py \
            --terms-file data/terms_decor.json \
            --geo "" --timeframe "today 12-m" \
            -o data/decor_trends.csv

      # ==============================
      # FASHION
      # ==============================
      - name: Reddit (fashion · last 45 days)
        continue-on-error: true
        env:
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USER_AGENT: ${{ secrets.REDDIT_USER_AGENT }}
        run: |
          python scripts/scrape_reddit.py \
            --subs malefashionadvice femalefashionadvice streetwear frugalmalefashion \
            --days 45 --limit 800 --sleep 1.0 \
            -o data/fashion_reddit_raw.csv

      - name: RSS (fashion · 60d)
        run: |
          python scripts/ingest_rss.py \
            --feeds data/feeds_fashion.txt --days 60 \
            -o data/fashion_rss_raw.csv

      - name: YouTube (fashion · 60d)
        env:
          YOUTUBE_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}
        run: |
          python scripts/ingest_youtube.py \
            --queries data/youtube_queries_fashion.txt \
            --days 60 --max_per_query 30 \
            -o data/fashion_youtube_raw.csv

      - name: Combine (fashion → all sources)
        run: |
          python scripts/combine_sources.py \
            --inputs data/fashion_reddit_raw.csv data/fashion_rss_raw.csv data/fashion_youtube_raw.csv \
            -o data/fashion_all_raw.csv

      - name: Analyze (fashion)
        run: |
          python scripts/analyze_prettydata.py \
            --raw data/fashion_all_raw.csv \
            --terms-file data/terms_fashion.json \
            --prefix fashion

      - name: Google Trends (fashion)
        run: |
          python scripts/ingest_trends.py \
            --terms-file data/terms_fashion.json \
            --geo "" --timeframe "today 12-m" \
            -o data/fashion_trends.csv

      # ==============================
      # DESIGN
      # ==============================
      - name: Reddit (design · last 45 days)
        continue-on-error: true
        env:
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USER_AGENT: ${{ secrets.REDDIT_USER_AGENT }}
        run: |
          python scripts/scrape_reddit.py \
            --subs graphic_design design typography web_design industrialdesign \
            --days 45 --limit 800 --sleep 1.0 \
            -o data/design_reddit_raw.csv

      - name: RSS (design · 60d)
        run: |
          python scripts/ingest_rss.py \
            --feeds data/feeds_design.txt --days 60 \
            -o data/design_rss_raw.csv

      - name: YouTube (design · 60d)
        env:
          YOUTUBE_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}
        run: |
          python scripts/ingest_youtube.py \
            --queries data/youtube_queries_design.txt \
            --days 60 --max_per_query 30 \
            -o data/design_youtube_raw.csv

      - name: Combine (design → all sources)
        run: |
          python scripts/combine_sources.py \
            --inputs data/design_reddit_raw.csv data/design_rss_raw.csv data/design_youtube_raw.csv \
            -o data/design_all_raw.csv

      - name: Analyze (design)
        run: |
          python scripts/analyze_prettydata.py \
            --raw data/design_all_raw.csv \
            --terms-file data/terms_design.json \
            --prefix design

      - name: Google Trends (design)
        run: |
          python scripts/ingest_trends.py \
            --terms-file data/terms_design.json \
            --geo "" --timeframe "today 12-m" \
            -o data/design_trends.csv

      # ==============================
      # AESTHETICS (combine decor + fashion)
      # ==============================
      - name: Combine (aesthetics raw from decor+fashion)
        run: |
          python scripts/combine_sources.py \
            --inputs data/decor_all_raw.csv data/fashion_all_raw.csv \
            -o data/aesthetics_all_raw.csv

      - name: Analyze (aesthetics)
        run: |
          python scripts/analyze_prettydata.py \
            --raw data/aesthetics_all_raw.csv \
            --terms-file data/terms_aesthetics.json \
            --prefix aesthetics

      - name: Google Trends (aesthetics)
        run: |
          python scripts/ingest_trends.py \
            --terms-file data/terms_aesthetics.json \
            --geo "" --timeframe "today 12-m" \
            -o data/aesthetics_trends.csv

      # ==============================
      # COMMIT ARTIFACTS
      # ==============================
      - name: Commit & push data artifacts
        run: |
          git config user.name "pld-bot"
          git config user.email "pld-bot@users.noreply.github.com"
          git add data/*.csv || true
          git commit -m "chore: refresh multi-source data (reddit+rss+youtube+trends)" || echo "No changes"
          git pull --rebase || true
          git push || true
