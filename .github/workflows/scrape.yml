name: Scrape & Analyze (PrettyData â€” decor)

on:
  schedule:
    - cron: "*/15 * * * *"   # every 15 minutes (UTC)
  workflow_dispatch: {}       # allow manual runs

concurrency:
  group: prettydata-scrape
  cancel-in-progress: true

jobs:
  run:
    runs-on: ubuntu-latest
    permissions:
      contents: write   # required to commit CSVs back to the repo

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Ensure data folder
        run: mkdir -p data

      - name: Scrape Reddit (decor subs, last 90 days)
        env:
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USER_AGENT: ${{ secrets.REDDIT_USER_AGENT }}
        run: |
          python scripts/scrape_reddit.py \
            --subs HomeDecorating InteriorDesign CozyPlaces \
            --days 90 \
            --limit 4000 \
            -o data/decor_reddit_raw.csv

      - name: Analyze & export CSVs (PrettyData - decor)
        run: |
          python scripts/analyze_prettydata.py \
            --raw data/decor_reddit_raw.csv \
            --terms-file data/terms_decor.json \
            --prefix decor

      - name: Commit & push data artifacts
        run: |
          git config user.name "pld-bot"
          git config user.email "pld-bot@users.noreply.github.com"
          git add data/*.csv || true
          git commit -m "chore: refresh decor data" || echo "No changes"
          git pull --rebase || true
          git push || true
