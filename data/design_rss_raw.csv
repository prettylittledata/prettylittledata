source,domain,url,created_utc,title,text
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/meet-smashingconf-amsterdam/,1772103600,Say Cheese! Meet SmashingConf Amsterdam ğŸ‡³ğŸ‡±,"Say Cheese! Meet SmashingConf Amsterdam ğŸ‡³ğŸ‡±

Meet our brand new conference for designers and UI engineers who love the web. Thatâ€™s [SmashingConf Amsterdam](https://smashingconf.com/amsterdam-2026), taking place in the legendary PathÃ© Tuschinski, on April 13&ndash;16, 2026."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/designer-guide-eco-friendly-interfaces/,1771840800,A Designerâ€™s Guide To Eco-Friendly Interfaces,"A Designerâ€™s Guide To Eco-Friendly Interfaces

Every high-resolution hero image, autoplay video, and complex JavaScript animation carries a cost. Sustainable UX challenges the era of â€œunlimited pixelsâ€ and reframes performance as responsibility. In 2026, truly sophisticated design is defined not by how much it adds, but by how thoughtfully it reduces its footprint."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/designing-streak-system-ux-psychology/,1771426800,Designing A Streak System: The UX And Psychology Of Streaks,"Designing A Streak System: The UX And Psychology Of Streaks

What makes streaks so powerful and addictive? To design them well, you need to understand how they align with human psychology. Victor Ayomipo breaks down the UX and design principles behind effective streak systems."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/building-empathy-centred-ux-framework-mental-health-apps/,1770994800,Building Digital Trust: An Empathy-Centred UX Framework For Mental Health Apps,"Building Digital Trust: An Empathy-Centred UX Framework For Mental Health Apps

Designing for mental health means designing for vulnerability. Empathy-Centred UX becomes not a â€œnice to haveâ€ but a fundamental design requirement. Hereâ€™s a practical framework for building trust-first mental health products."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/designing-agentic-ai-practical-ux-patterns/,1770814800,"Designing For Agentic AI: Practical UX Patterns For Control, Consent, And Accountability","Designing For Agentic AI: Practical UX Patterns For Control, Consent, And Accountability

Autonomy is an output of a technical system. Trustworthiness is an output of a design process. Here are concrete design patterns, operational frameworks, and organizational practices for building agentic systems that are not only powerful but also transparent, controllable, and trustworthy."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/css-scope-alternative-naming-conventions/,1770278400,CSS <code>@scope</code>: An Alternative To Naming Conventions And Heavy Abstractions,"CSS <code>@scope</code>: An Alternative To Naming Conventions And Heavy Abstractions

Prescriptive class name conventions are no longer enough to keep CSS maintainable in a world of increasingly complex interfaces. Can the new `@scope` rule finally give developers the confidence to write CSS that can keep up with modern front ends?"
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/combobox-vs-multiselect-vs-listbox/,1770112800,Combobox vs. Multiselect vs. Listbox: How To Choose The Right One,"Combobox vs. Multiselect vs. Listbox: How To Choose The Right One

Combobox vs. Multi-Select vs. Listbox vs. Dual Listbox? How they are different, what purpose they serve, and how to choose the right one. Brought to you by <a href=""https://ai-design-patterns.com"">Design Patterns For AI Interfaces</a>, **friendly video courses on UX** and design patterns by Vitaly."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/desktop-wallpaper-calendars-february-2026/,1769850000,"Short Month, Big Ideas (February 2026 Wallpapers Edition)","Short Month, Big Ideas (February 2026 Wallpapers Edition)

Letâ€™s make the most of the shortest month of the year with a new collection of desktop wallpapers that are sure to bring a smile to your face â€” and maybe spark your creativity, too. All of them were designed with love by the community for the community and can be downloaded for free. Happy February!"
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/practical-use-ai-coding-tools-responsible-developer/,1769778000,Practical Use Of AI Coding Tools For The Responsible Developer,"Practical Use Of AI Coding Tools For The Responsible Developer

AI coding tools like agents can be valuable allies in everyday development work. They help handle time-consuming grunt work, guide you through large legacy codebases, and offer low-risk ways to implement features in previously unfamiliar programming languages. Here are practical, easy-to-apply techniques to help you use these tools to improve your workflow."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/unstacking-css-stacking-contexts/,1769508000,Unstacking CSS Stacking Contexts,"Unstacking CSS Stacking Contexts

In CSS, we can create â€œstacking contextsâ€ where elements are visually placed one on top of the next in a three-dimensional sense that creates the perception of depth. Stacking contexts are incredibly useful, but theyâ€™re also widely misunderstood and often mistakenly created, leading to a slew of layout issues that can be tricky to solve."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/beyond-generative-rise-agentic-ai-user-centric-design/,1769086800,Beyond Generative: The Rise Of Agentic AI And User-Centric Design,"Beyond Generative: The Rise Of Agentic AI And User-Centric Design

Developing effective agentic AI requires a new research playbook. When systems plan, decide, and act on our behalf, UX moves beyond usability testing into the realm of trust, consent, and accountability. Victor Yocco outlines the research methods needed to design agentic AI systems responsibly."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/rethinking-pixel-perfect-web-design/,1768903200,Rethinking â€œPixel Perfectâ€ Web Design,"Rethinking â€œPixel Perfectâ€ Web Design

Amit Sheen takes a hard look at the â€œPixel Perfectâ€ legacy concept, explaining why itâ€™s failing us and redefining what â€œperfectionâ€ actually looks like in a multi-device, fluid world."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/smashing-animations-part-8-css-relative-colour/,1768384800,Smashing Animations Part 8: Theming Animations Using CSS Relative Colour,"Smashing Animations Part 8: Theming Animations Using CSS Relative Colour

CSS relative colour values are now widely supported. In this article, pioneering author and web designer [Andy Clarke](https://stuffandnonsense.co.uk/) shares practical techniques for using them to theme and animate SVG graphics."
rss,uxdesign.cc,https://uxdesign.cc/a-love-letter-to-the-pok%C3%A9dex-3ffc0a51332e?source=rss----138adf9c44c---4,1772194458,A love letter to the PokÃ©dex,"A love letter to the PokÃ©dex

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/a-love-letter-to-the-pok%C3%A9dex-3ffc0a51332e?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/1600/1*zCSpxq867Rhkuw5szquBPA.png"" width=""1600"" /></a></p><p class=""medium-feed-snippet"">Or an obituary to another gadget replaced by the multipurpose smartphone</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/a-love-letter-to-the-pok%C3%A9dex-3ffc0a51332e?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/the-ux-of-survival-in-the-age-of-ai-deepfakes-b8e8602eddaa?source=rss----138adf9c44c---4,1772150038,The UX of survival in the age of AI deepfakes,"The UX of survival in the age of AI deepfakes

<h4>A framework for Crisis Information Design.</h4><figure><img alt=""AI FAKE: Image of a church in Puerto Vallarta burningâ€Šâ€”â€Šit never happened"" src=""https://cdn-images-1.medium.com/max/1024/1*-Fc3QEA3pxsnJBx4twb6ag.jpeg"" /><figcaption>FAKE!!! This image was shared on social media and confirmed as fake byÂ <a href=""https://www.snopes.com/fact-check/puerto-vallarta-cartel-church-fire/"">Snopes</a></figcaption></figure><p>People can no longer tell the difference between real images and AI-generated ones. Thatâ€™s not an opinion. Itâ€™s a <a href=""https://doi.org/10.1073/pnas.2120481119"">finding from researchers at UC Berkeley and SUNY Buffalo</a>, who demonstrated that AI-synthesized faces are now perceived as more trustworthy than real human faces. Let that sink in for a second: the fake version of reality is <em>more</em> believable than realityÂ itself.</p><p>Meanwhile, <a href=""https://doi.org/10.1126/science.aap9559"">misinformation spreads six times faster than true information</a> on social media. During a crisis, when stress and fear impair our ability to think critically, the information ecosystem becomes a minefield.</p><p>The platforms where people actually consume information during emergencies (WhatsApp, X, Facebook, Reddit) have <em>zero</em> verification infrastructure at the point of consumption. The tools that can verify content (Snopes, PolitiFact, C2PA, SynthID, Hive AI) exist on entirely separate platforms, accessed hours later, long after the damage isÂ done.</p><p>This is a design problem. Not a policy problem. Not a content moderation problem. A design problem. And itâ€™s one that the UX community has barely started toÂ address.</p><p>This isnâ€™t just theoretical. In February 2026, I <a href=""https://medium.com/design-bootcamp/caught-between-fake-news-and-predatory-paywalls-4500e2ac6ddd"">watched it unfold in real time</a> while sheltering in place during a crisis in Puerto Vallarta, Mexico. What I experienced that day revealed a set of systemic failures that Iâ€™ve spent the last few days trying to make sense of. This article is the result: a framework for designers who want to build information products that actually work when people need themÂ most.</p><p>But first, some context. Because the problem didnâ€™t start with AI. It has been escalating since the beginning of the internet.</p><h3>The three eras of information credibility</h3><figure><img alt=""Timeline of three eras of information credibility. Era 1: The Gatekept Internet (1960s-2000s), where institutional gatekeepers controlled access. Era 2: The Social Media Revolution (2004â€“2022), where users must evaluate credibility themselves. Era 3: The AI Deepfake Era (2023-present), where AI-generated content is photorealistic and weaponized during crises."" src=""https://cdn-images-1.medium.com/max/960/1*wCHB-o6eGdR-vpzdJWyGtg.jpeg"" /><figcaption>Three Eras of Information Credibility</figcaption></figure><h4>Era 1: The gatekept internet (1960s toÂ 2000s)</h4><p>The internet wasnâ€™t built for everyone. It was built for institutions. <a href=""https://www.internetsociety.org/internet/history-internet/brief-history-internet/"">ARPANET</a> connected universities and military installations. <a href=""https://www.w3.org/History/1989/proposal.html"">Tim Berners-Leeâ€™s original 1989 proposal</a> at CERN was a document management system for physicists.</p><p>The early web was credible by default because the barrier to publishing was the verification. If something was online, it came from a credentialed source. Military. Academic. Government.</p><p>Users didnâ€™t need to evaluate credibility. The system did it forÂ them.</p><p>This era built an assumption into the fabric of digital products that weâ€™ve never fully shaken: that information, once published, is probablyÂ true.</p><h4>Era 2: The social media revolution (2004 toÂ 2022)</h4><p>Then everything changed. Facebook launched in 2004. YouTube in 2005. Twitter in 2006. Reddit in 2005. Suddenly, <em>everyone</em> could publish. Credible or not. Educated or not. Reality orÂ not.</p><p>For the first time in the history of the internet, the burden of evaluating credibility shifted entirely to the individual user. But nobody designed the tools to help them do it. <a href=""https://doi.org/10.1016/j.pragma.2013.07.012"">Research by Metzger and Flanagin</a> showed that users rely on <strong>cognitive shortcuts</strong> (how professional does the site look? do other people seem to trust it?) rather than actual verification.</p><p>By 2016, the consequences were visible to everyone. <a href=""https://doi.org/10.1257/jep.31.2.211"">Allcott and Gentzkowâ€™s analysis</a> of fake news during the U.S. presidential election documented how <strong>fabricated stories spread faster</strong> and wider than corrections.</p><p>The credibility problem is a design failure and not a problem with the technology. We gave everyone a megaphone and nobody a filter. We're not teaching people how to actually think critically about evaluating what'sÂ true.</p><h4>Era 3: The AI deepfake era (2023 toÂ present)</h4><p>Now here we are... AI-generated content is photorealistic, produced in seconds, and the tools to create it are free. The distinction between â€œrealâ€ and â€œgeneratedâ€ has collapsed for the average person. And during crises, these tools are being weaponized in real time, exacerbating theÂ problem.</p><p>The evidence is recent and specific. During a crisis in Mexico in February 2026, an AI-generated image of the Our Lady of Guadalupe cathedral in Puerto Vallarta engulfed in flames went viral across X, WhatsApp, Facebook, and Instagram.</p><p><a href=""https://www.snopes.com/fact-check/puerto-vallarta-cartel-church-fire/"">Snopes confirmed</a> it was generated by Google Gemini. <a href=""https://www.politifact.com/search/?q=Our+Lady+of+Guadalupe+cathedral+in+Puerto+Vallarta+"">PolitiFact rated it fabricated</a>, with <a href=""https://hivemoderation.com"">Hive AI detecting it as synthetic with 99.9% confidence</a>. Googleâ€™s own <a href=""https://deepmind.google/technologies/synthid/"">SynthID watermark</a> was embedded in the image. The Gemini logo was still onÂ it.</p><p>None of this information reached the people sharing it in WhatsApp groups while they sheltered in theirÂ homes.</p><p>Each era created a new credibility challenge. Design never caught up. Weâ€™re still building information products on Era 1 assumptions in an Era 3Â world.</p><h3>What crisis information failure looks like from theÂ inside</h3><p>I live in Puerto Vallarta, Mexico during the winter. Itâ€™s my second home. Iâ€™ve been coming here for years, originally from the U.S., now based in France. On February 22, 2026, a crisis erupted across the state of Jalisco. I woke up to explosions and smoke rising a block from my apartment.</p><p>What followed was a frantic scramble for information that exposed every failure Iâ€™ve just described.</p><p>WhatsApp became my primary source. People shared real updates: road closures, fire locations, which neighborhoods to avoid. But in the same thread, someone posted a video of a church burning down. A friend called it out as fake. It never happened.</p><p>For the minutes before that correction, though, it was real to everyone in the group. Someone else shared a rumor about street violence at noon. No source. No verification. Just a sentence typed into a chat that changed my behavior for hours. I closed my blinds and didnâ€™t openÂ them.</p><p>My husband used Grok on X to try to synthesize what was happening. He ran a deep search across 150+ sources and shared the result with ourÂ group:</p><blockquote>â€œThe rumor is unverified social-media panic with no credible source or backing from authorities or news outlets.â€</blockquote><p>That <em>actually</em> helped. That was technology doing what it should do in a crisis: separating signal from noise. It was one of the few bright spots of theÂ day.</p><p>Then came the moment that still makes meÂ angry.</p><p>Someone shared a New York Times link in the group. Finally, a trusted, verified, globally respected source. I clicked it. Paywall. I tried to sign in. Password reset. Two-factor authentication. Redirected. Looped. I did all of this on my phone while checking through the blinds for new columns ofÂ smoke.</p><p>I gave up and went back to the rumorÂ mill.</p><p>The equation should trouble everyone who builds information products: misinformation was free and everywhere. The truth was paywalled and broken. I wrote about this experience in more detail in a <a href=""https://medium.com/design-bootcamp/caught-between-fake-news-and-predatory-paywalls-4500e2ac6ddd"">previous article</a>, but the short version isÂ this:</p><blockquote>The User Experience of finding trustworthy information during a crisis is a nightmare... not because the information doesnâ€™t exist, but because the designÂ fails.</blockquote><p>The U.S. State Department posted a shelter-in-place advisory on X at 1:17 p.m. Iâ€™d been living the crisis since 8:15 a.m. Five hours of silence from official channels.</p><p>That experience left me with something more than frustration. It left me with the urge to create a framework to support this from happening again.</p><h3>The Crisis Information Design Framework</h3><p>What I experienced in Puerto Vallarta was three interconnected failures happening simultaneously and Iâ€™ve spent the last few days pulling them apart and synthesizing them into a model that designers can actuallyÂ use.</p><p>The Crisis Information Design Framework has three layers. Each layer answers a different question and addresses a different stage of the design process: diagnose, prioritize, execute.</p><figure><img alt=""Three-column overview: Diagnose, Prioritize, Execute. Column 1: The Verification Gap, map where verification vs. consumption happens. Column 2: Information Hierarchy of Needs, build from base up. Column 3: Trauma-Informed Design, apply SAMHSAâ€™s six principles. Each column includes a key question for product teams."" src=""https://cdn-images-1.medium.com/max/960/1*kmLA-Heqt-GfAhudv3LZqQ.jpeg"" /><figcaption>The Crisis Information Design Framework</figcaption></figure><h4>Layer 1: The Verification Gap (Diagnose)</h4><p>The Verification Gap is the spatial and temporal distance between where people consume information and where verification actuallyÂ happens.</p><p>During the crisis, <strong>consumption</strong> happened instantly. On WhatsApp, on X, in group chats with shakingÂ hands.</p><p><strong>Verification</strong> happened hours later, on <a href=""https://www.snopes.com/fact-check/puerto-vallarta-cartel-church-fire/"">Snopes</a>, on <a href=""https://www.politifact.com"">PolitiFact</a>, through <a href=""https://c2pa.org"">C2PA content credentials</a> analysis, via <a href=""https://hivemoderation.com"">Hive AI</a> detection.</p><p>The gap between those two moments is where misinformation does its damage. People act on unverified information because verified information doesnâ€™t reach them in time, in the same place, or in the sameÂ format.</p><figure><img alt=""Diagram of the Verification Gap. Top layer: where people consume information instantly during crises, on WhatsApp, Facebook, X, Signal, Reddit, SMS. Bottom layer: where verification happens hours later on Snopes, PolitiFact, C2PA/SynthID, Hive AI. A red block between them labeled â€œThe Gapâ€ identifies this separation as the design problem."" src=""https://cdn-images-1.medium.com/max/960/1*HWJ-3TKHdkiVWaYKUYMF-w.jpeg"" /><figcaption>Layer 1: The Verification Gap</figcaption></figure><p>The design question for every product team: is verification visible at the point of consumption? If the answer is no, you have a Verification Gap.</p><h4>Layer 2: Information Hierarchy of Needs (Prioritize)</h4><p>When youâ€™re counting the food in your fridge and wondering if your electricity will stay on, <a href=""https://doi.org/10.1037/h0054346"">Maslowâ€™s hierarchy of needs</a> stops being a framework you learned in school and becomes your new operating system.</p><p>The same principle applies to information design. Iâ€™m proposing an Information Hierarchy of Needs with five levels, built from the baseÂ up:</p><ol><li><strong>VERIFICATION (base): </strong>Is this real? Is this verified? Can I act on thisÂ safely?</li><li><strong>SOURCE: </strong>Where did this come from? Who created it? Are they credible?</li><li><strong>RECENCY: </strong>Is this current or outdated? During a crisis, two-hour-old information can be out of date and factually incorrect.</li><li><strong>CONTEXT: </strong>What does this mean for me, here, rightÂ now?</li><li><strong>DEPTH (top): </strong>Full understanding. Nuance. Multiple perspectives.</li></ol><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/960/1*Oq0eieZS171_l9JQu5-t2Q.jpeg"" /><figcaption>Layer 2: The Information Hierarchy ofÂ Needs</figcaption></figure><p>Most information products today are designed for the top of the pyramid: depth, context, nuance. Almost none address the base. You canâ€™t build for depth when users canâ€™t even tell if what theyâ€™re looking at isÂ real.</p><p><a href=""https://www.citybureau.org/notebook/2019/7/17/journalism-is-a-luxury-information-is-a-necessity"">City Bureau</a> has developed a related â€œinformation needsâ€ concept for community journalism, but the application to product design is distinct. The question for designers isnâ€™t, â€œWhat stories does this community need?â€ Itâ€™s â€œDoes your product help users verify before theyÂ act?â€</p><h4>Layer 3: Trauma-Informed Information Design (Execute)</h4><p>The third layer addresses how to build it right. <a href=""https://ncsacw.acf.gov/topics/trauma-informed-care/"">SAMHSAâ€™s six principles of trauma-informed care</a>, originally developed for healthcare and social services, translate <em>directly</em> to digital product design during crisis information experiences. Iâ€™ve adapted each one for my framework:</p><ul><li><strong>Safety.</strong> Verification is visible at the point of consumption. No ambiguity about whatâ€™s verified versus whatâ€™s not. Donâ€™t autoplay graphicÂ content.</li><li><strong>Trustworthiness.</strong> Source provenance is always shown. No dark patterns in authentication flows. No deceptive interstitials during crisisÂ access.</li><li><strong>Peer Support.</strong> Community verification layers. Crowd-sourced flagging. Surface it when multiple users flag the sameÂ content.</li><li><strong>Collaboration.</strong> Platform and official source integration. Pin verified government updates. Partner with fact-checkers in realÂ time.</li><li><strong>Empowerment.</strong> User control over information filtering. Agency over what they see. Not algorithmic force-feeding of engagement-optimized fear.</li><li><strong>Cultural Awareness.</strong> Multilingual. Locally contextualized. Accessible under stress: larger touch targets, simplified flows, offline capability.</li></ul><p>Trauma-informed design has been applied in healthcare and social services for over a decade, guided by SAMHSAâ€™s 2014 framework (Link above) and subsequent implementation research by Menschner and Maul for the <a href=""https://www.chcs.org/resource/trauma-informed-care-implementation-resource-center/"">Center for Health Care Strategies</a>.</p><p>It has not yet been applied to crisis misinformation or information product design. Thatâ€™s the gap this framework addresses.</p><figure><img alt=""Six-card grid of SAMHSAâ€™s trauma-informed care principles adapted for crisis information design. Safety: verification visible at consumption. Trustworthiness: no dark patterns. Peer Support: crowd-sourced flagging. Collaboration: official source integration. Empowerment: user control over filtering. Cultural Awareness: multilingual, accessible under stress."" src=""https://cdn-images-1.medium.com/max/960/1*NnRR9P-r_MhFUkp-fXuE-A.jpeg"" /><figcaption>Layer 3: Trauma-Informed Information Design</figcaption></figure><p>Together, these three layers form a completeÂ system:</p><ol><li>Diagnose the Verification Gap</li><li>Prioritize using the Information Hierarchy ofÂ Needs</li><li>Execute using Trauma-Informed Design principles</li></ol><h3>What exists today, and whatâ€™sÂ missing</h3><p>Itâ€™s worth acknowledging the work thatâ€™s already underway.</p><p>The <a href=""https://c2pa.org"">Coalition for Content Provenance and Authenticity (C2PA)</a> has built a content credentials standard backed by Adobe, Microsoft, Google, and the BBC. It attaches origin metadata to images so provenance can be verified.</p><p>Googleâ€™s <a href=""https://deepmind.google/technologies/synthid/"">SynthID</a> embeds imperceptible watermarks into AI-generated content.</p><p><a href=""https://hivemoderation.com"">Hive Moderation</a> provides AI detection tools that can identify synthetic content with high confidence. The technology exists.</p><p>On the policy side, the <a href=""https://digital-strategy.ec.europa.eu/en/policies/digital-services-act-package"">EU Digital Services Act</a> includes a crisis-response mechanism (Article 36) that requires platforms to take â€œproportionate and effectiveâ€ measures during emergencies. It was first activated during the Israel-Hamas conflict inÂ 2023.</p><p>Xâ€™s <a href=""https://communitynotes.x.com"">Community Notes</a> provides crowd-sourced context onÂ posts.</p><p>Googleâ€™s <a href=""https://toolbox.google.com/factcheck/"">Fact Check Tools API</a> lets developers surface fact-checks programmatically.</p><p>So the building blocks are there. Whatâ€™s missing is the design layer that connects them to users at the moment they needÂ them.</p><p>No messaging app (WhatsApp, Signal, Telegram) has a crisis verification mode. No platform surfaces C2PA or SynthID metadata to end users in a meaningful way. No emergency paywall protocol is industry-standard. Some outlets <a href=""https://reutersinstitute.politics.ox.ac.uk"">dropped paywalls during COVID</a>, including the New York Times, the Wall Street Journal, and The Atlantic, but it was ad hoc, not systematic.</p><p>And the fundamental architecture of verification remains broken: the tools that can tell you whether something is real exist on entirely different platforms from the ones where you encounter theÂ content.</p><p>Thatâ€™s the Verification Gap, operating at an industryÂ level.</p><h3>What designers and product teams canÂ do</h3><h4>For messaging platforms (WhatsApp, Signal, Telegram)</h4><p>Build a crisis mode. When a region-specific crisis is declared, enable verified source badges, rumor-flagging, and pinned official updates within group chats. Surface SynthID and C2PA metadataÂ inline.</p><p>If an image has an AI watermark, show it before usersÂ share.</p><p>Let users flag content as unverified and surface flag counts to groupÂ members.</p><h4>For news organizations</h4><p>Implement emergency open-access protocols. Automatic paywall drops during declared emergencies within affected regions. Geofenced, time-limited, automated.</p><p>And PLEASE: no sign-in loops, no credit card prompts, no redirect chains during a crisis!!!</p><p>If someone is in a crisis zone, remove <em>every</em> barrier between them and verified information. The sign-in experience I had with the New York Times while smoke was visible from my window should never happen toÂ anyone.</p><h4>For social platforms (X, Facebook, Reddit, Instagram)</h4><p>Build verification-first feeds during crises. When a crisis is detected, re-rank feeds to prioritize verified sources over engagement metrics. Before users share flagged AI-generated content, show a verification prompt: â€œThis image may be AI-generated. View analysis. Share anyway.â€ That single interstitial could have prevented the cathedral image from goingÂ viral.</p><h4>For every product that asks users to trust information</h4><p>Apply the Information Hierarchy of Needs as a design audit. Does your product serve the base of the pyramid before the top? Apply Trauma-Informed Design principles. Is your product safe for someone whose hands are shaking, who hasnâ€™t slept, who is making survival decisions based on what they see on theirÂ screen?</p><p>If youâ€™re not sure, the answer is probably no. And thatâ€™s where the workÂ starts.</p><p>The crisis I experienced wasnâ€™t unique. Crises are accelerating: climate disasters, political instability, public health emergencies. Every one of them will be accompanied by an information crisis. And every information crisis will now include AI-generated content designed to confuse, inflame, and manipulate.</p><p>The technology to solve this exists. C2PA can prove provenance. SynthID can detect AI content. Fact-checkers can verify claims. The problem isnâ€™t capability. Itâ€™s design. None of these tools reach users at the moment they need themÂ most.</p><p>This is our problem. And designers are the ones who can solveÂ it.</p><p>The Crisis Information Design Framework is a lens that can be applied to any product that asks users to trust what theyâ€™re seeing. Because in a crisis, the UX of information isnâ€™t about a feature, itâ€™s literally about survival.</p><p><a href=""https://www.joshlamar.com/"">Josh LaMar</a> is CEO of <a href=""https://www.amplinate.com/"">Amplinate</a>, where he advises on product growth and AI decision strategy. Over 20 years, he has spent 40,000+ hours listening to customers across 19 countries on five continents. He lives between Puerto Vallarta, Mexico and Paris,Â France.</p><p><a href=""https://www.joshlamar.com/crisis-information-design-framework""><em>Download the one-page </em><strong><em>Crisis Information Design Framework</em></strong><em> cheat sheet at joshlamar.com</em></a><em>.</em></p><h3>References</h3><h4><strong>Academic Research</strong></h4><ul><li><a href=""https://doi.org/10.1073/pnas.2120481119"">DOI: 10.1073/pnas.2120481119</a></li><li><a href=""https://doi.org/10.1126/science.aap9559"">DOI: 10.1126/science.aap9559</a></li><li><a href=""https://doi.org/10.1016/j.pragma.2013.07.012"">DOI: 10.1016/j.pragma.2013.07.012</a></li><li><a href=""https://doi.org/10.1257/jep.31.2.211"">DOI: 10.1257/jep.31.2.211</a></li></ul><h4>Institutional &amp; Government</h4><ul><li><a href=""https://ncsacw.acf.gov/topics/trauma-informed-care/"">Trauma-Informed Care</a></li><li><a href=""https://www.chcs.org"">Center for Health Care Strategies</a></li><li><a href=""https://digital-strategy.ec.europa.eu/en/policies/digital-services-act"">The Digital ServicesÂ Act</a></li><li><a href=""https://www.w3.org/History/1989/proposal.html"">Information Management: AÂ Proposal</a></li><li><a href=""https://www.internetsociety.org/internet/history-internet/brief-history-internet/"">A Brief History of theÂ Internet</a></li></ul><h4>Fact-Checks (February 2026)</h4><ul><li><a href=""https://www.snopes.com"">snopes.com</a></li><li><a href=""https://www.politifact.com"">politifact.com</a></li><li><a href=""https://www.leadstories.com"">leadstories.com</a></li></ul><h4>Technology &amp; Standards</h4><ul><li><a href=""https://c2pa.org"">c2pa.org</a></li><li><a href=""https://aitransparencyframework.com/"">AITransparencyFramework.com</a></li><li><a href=""https://deepmind.google/technologies/synthid/"">deepmind.google/technologies/synthid</a></li><li><a href=""https://hivemoderation.com"">hivemoderation.com</a></li><li><a href=""https://toolbox.google.com/factcheck/"">toolbox.google.com/factcheck</a></li><li><a href=""https://communitynotes.x.com"">communitynotes.x.com</a></li></ul><h4>Journalism &amp;Â Industry</h4><ul><li><a href=""https://reutersinstitute.politics.ox.ac.uk"">Reuters Institute</a></li><li><a href=""https://www.adweek.com/performance-marketing/major-publishers-take-down-paywalls-for-coronavirus-coverage/"">Major Publishers Take Down Paywalls for Coronavirus Coverage</a></li><li><a href=""https://digiday.com/media/despite-relaxing-paywalls-publishers-like-bloomberg-wsj-atlantic-see-subscription-spikes/"">Despite relaxing paywalls, publishers like Bloomberg, WSJ and The Atlantic see subscription spikes</a></li><li><a href=""https://www.citybureau.org"">citybureau.org</a></li><li><a href=""https://www.citybureau.org/notebook/2019/7/17/journalism-is-a-luxury-information-is-a-necessity"">Is Your Journalism a Luxury or Necessity?</a></li></ul><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b8e8602eddaa"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/the-ux-of-survival-in-the-age-of-ai-deepfakes-b8e8602eddaa"">The UX of survival in the age of AI deepfakes</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/dear-diary-youre-the-last-good-listener-f3dad8ab37a8?source=rss----138adf9c44c---4,1772141206,"Dear diary, youâ€™re the last good listener","Dear diary, youâ€™re the last good listener

<h4>Adam Smith on why 250 years of empathy still leaves us feelingÂ unheard.</h4><figure><img alt=""Sepia-toned, softly textured portrait of Adam Smith in an 18th-century style. He faces slightly to the right with a calm, serious expression, wearing a dark coat and white cravat. His powdered hair is styled in rounded side rolls typical of the period. The background is softly blurred and vignetted, giving the image a warm, archival, hand-sketched feel consistent with classical engraved portraits."" src=""https://cdn-images-1.medium.com/max/1024/1*0YEWMhfrWdP5QROvsQBgIQ.png"" /><figcaption>Adam Smithâ€Šâ€”â€ŠImage created withÂ AI</figcaption></figure><p><em>We talk about empathy more than ever. Listening sessions. Trainings designed to â€œmeet people where they are.â€ Empathy maps meant to snapshot customer personas.</em></p><p><em>And still, the people who open up often walk away feelingÂ drained.</em></p><p><em>Why?</em></p><p><em>It felt like a conversation. There was back-and-forth. No open disagreement. NothingÂ hostile.</em></p><p><em>So what wasÂ it?</em></p><p><em>Why is it that everything you said (some of it personal) was met with complete, instant understanding? Before you even finished your thought, they were nodding. How did they get to the finish line before youÂ did?</em></p><p><em>If empathy was supposed to solve this, itÂ hasnâ€™t.</em></p><p><em>Iâ€™m </em><a href=""https://www.linkedin.com/in/natesowder/""><strong><em>Nate Sowder</em></strong></a><em> (fresh off parental leave), and this is </em><strong><em>unquoted, installment 16.</em></strong><em> Itâ€™s on Adam Smithâ€Šâ€”â€Šand the uncomfortable possibility that weâ€™ve misunderstood what it means to understand each other in the firstÂ place.</em></p><h3>Adam Smith (not the Econ 101Â mascot)</h3><p>When we hear â€œ<a href=""https://www.adamsmithworks.org/documents/otteson-brief-smith-bio"">Adam Smith</a>,â€ we think capitalism, free marketsâ€¦ the invisible hand. All of this comes from <a href=""https://en.wikipedia.org/wiki/The_Wealth_of_Nations""><em>The Wealth of Nations</em></a>, published inÂ 1776.</p><p>Seventeen years earlier, Smith wrote a different book: <a href=""https://www.adamsmithworks.org/documents/asw-edition""><em>The Theory of Moral Sentiments</em></a>. Itâ€™s writing this book that shaped his thinking about human behavior, and itâ€™s where weâ€™ll be spending our time today. In fact, unlike his other works, Smith revised this piece 6 times, the last time on his death bed. If you want to know where someoneâ€™s head is atâ€¦ 6 revisions.</p><p>In this book, Smith wrestled with a basic question:</p><h4>How do we decide whether someoneâ€™s reaction makesÂ sense?</h4><p>When someone is furious, do we think theyâ€™re overreacting? When someone is hurt, do we think theyâ€™re justified? When someone celebrates, does the celebration fit theÂ act?</p><p>We feel all of this constantly, but Smith was looking to uncover the process by which we rationalize it.</p><h3>Sympathy andÂ empathy</h3><p>To understand Smithâ€™s thought process, you need to understand how he used the word <a href=""https://oll.libertyfund.org/quotes/adam-smith-selfishness-sympathy""><strong><em>sympathy</em></strong></a><strong><em>,</em></strong><em> </em>because it wasnâ€™t reserved for greeting cards and florists like we use itÂ today.</p><p>For him, sympathy was the act of imagining yourself in another personâ€™s situation in order to judge it proportionately. You donâ€™t need to feel their experience directly, but you try to build a version of it using your own memories and perspective.</p><p>To qualify as â€˜sympathyâ€™, the version you build in your mind is supposed to be partial and <strong>always</strong> incomplete. This means that the level in which youâ€™re able to understand can change and you have to acknowledge that.</p><p>Sympathy in Smithâ€™s terms can be defined as: <a href=""https://plato.stanford.edu/entries/smith-moral-political/#:~:text=In%20his%20%E2%80%9CHistory%20of%20Astronomy,between%20theoretical%20and%20ordinary%20thought.""><strong>disciplined approximation</strong></a><strong>.</strong></p><p>The word we tend to use now isÂ <strong><em>empathy</em>.</strong></p><p>Empathy is more concrete. It implies a level of understanding. â€œI get it.â€ â€œI know exactly how you feel.â€ That language suggests that the gap between two people can be understood.</p><p>Adam Smith never believed that two people could fully understand each other. Therefore, empathy, the way we use it today, wouldnâ€™t work forÂ Smith.</p><h3>What sympathyÂ requires</h3><p>In 1759, Smith identified that genuine understanding requires time, humility and proximity.</p><p>Letâ€™s look at eachÂ one.</p><p>T<strong>ime</strong>.<br />Smith thought it took time to understand. Our first reaction to someoneâ€™s situation is often incomplete and biased. It takes time to sit with what we heard and let the initial judgement pass. Time to reconsider whether our response actually resonates.</p><p>Sometimes that means repeated exposure and other times it just means not reacting immediately.</p><p><strong>Humility</strong>.<br />Humility, in Smithâ€™s framework, meant recognizing the limits of your first judgment.</p><p>This meant not rushing to explain your own interpretation, and not immediately taking your turn to clarify, defend, or relate it back to yourself.</p><p>In practice, that often means letting the other personâ€™s statement be enough without having to make it about you. This would allow you to suspend your need to respond long enough to examine whether your internal picture of the situation is accurate.</p><p>Without humility, the conversation becomes a rotation of perspectives. In other words, itâ€™s no longer listeningâ€¦ youâ€™ve entered aÂ debate.</p><p><strong>Proximity</strong>.<br />Smith opens <em>The Theory of Moral Sentiments</em> by noting that we are more affected by the misfortune of someone close to us than someone at a notable distance. Distance can tend to distort our reaction.</p><p><strong>According to Smith, that distortion shows up as</strong><a href=""https://www.adamsmithworks.org/documents/adam-smith-and-the-natural-law-the-impartial-spectator-synderesis-and-moral-judgment-properly-understood""><strong> judgement</strong></a><strong>.</strong></p><p>The further we are from someoneâ€™s lived experience, the more our imagination fills gaps with our own memories and experiences. The answer to someone elseâ€™s problem might feel obvious, but under closer inspection starts to get more complicated the more effort we put toward trying to understand.</p><p>Proximity forces us to account for details we couldnâ€™t see from a distance.</p><h3>Sympathy is intentional</h3><p>Letâ€™s take a moment to explain this, because not only was it an important distinction to Smith, itâ€™s what the rest of this essay is basedÂ on.</p><p>Sympathy (as is laid out by Adam Smith) requires you to set aside the instinct to orient the situation around yourself. You build a picture of the other personâ€™s experience and examine whether your judgement makesÂ sense.</p><p>Empathy, <strong>as we commonly use it in conversation (and research)</strong>, often works differently. It looks inward, searching your own memories and experiences for something similar so you can translate that experience into terms you already know. Empathy feels pretty immediate, which can feel good for the person showing it, and can create connection.</p><p>A big distinction is that empathy stops growing the moment inward connection is made. Sympathy, on the other hand, asks if you understand rather than if you can relate. Big difference.</p><p>Neither of these make for better or worse people. But I think itâ€™s important that only one of them encourages revision.</p><h3>Spectator Model</h3><p>Smith described how our first judgments form, and how hard it is to revise them. The â€œImpartial Spectatorâ€ was his name for the internal witness that can interrupt that process, but we have to do that intentionally.</p><p>What helped me while reading him was laying out his observations side by side. When I did this, I saw that deeper understanding develops if youâ€™re willing to investÂ effort.</p><p>The point of the Spectator Model is to help you abstract your level of understanding. From there, youâ€™re left with two decisions. 1. Are you sufficed with your current level of understanding? and 2. What would it take to go further in that understanding.</p><p>And itâ€™s worth saying: You donâ€™t always need to push all the way to truth. Sometimes â€˜Awarenessâ€™ is the right level. The model is designed to make intent possible. This way, you can decide whether an additional conversation, meeting or investment in research is worth it, given your current level of understanding.</p><p>Hereâ€™s what the model looksÂ like.</p><figure><img alt=""A funnel-shaped diagram titled â€œThe Spectator Model.â€ Six bands narrow from Assumption to Interpretation, Awareness, Understanding, Immersion, and Truth. Arrows on the left list Curiosity, Humility, Proximity, Time, and Consequence, pointing inward toward deeper understanding. Arrows on the right list Comfort, Habit, Pride, Speed, and Certainty, pointing outward. The model shows that understanding deepens with intentional effort and drifts outward without it."" src=""https://cdn-images-1.medium.com/max/1024/1*k3o07HWrnWhs99g8EEm6FQ.png"" /><figcaption>Spectator Modelâ€Šâ€”â€ŠDeveloped by Nate Sowder <strong>Â©Â 2026</strong></figcaption></figure><h3>How to useÂ it</h3><p>Any time you form a judgment about someone, you are somewhere in thisÂ model.</p><p>That judgment happens a lot in personal conversation. Weâ€™re going to think about it here in terms of a product reviews, or more specifically, what might happen during research when you determine what a userÂ â€œmeant.â€</p><p>In that moment, Smith would argue that you should be able to abstract your current level of understanding (or sympathy). Again, weâ€™re not trying to be dramatic. Sympathy just means an acknowledgement that we cannot fully understand another personâ€™s situation (no matter how much we may wantÂ to).</p><p><strong>Ask yourself:</strong><br />Are you operating mostly from what you already believed?<br />Have you filtered what you heard through your existing lens?<br />Has anything caused you to reconsider your first interpretation?</p><p>That act of locating yourself is the beginning of sympathy. From there, youâ€™re left with decisions that empathy doesnâ€™t afford because empathy assumes completion.</p><p><strong>Decide: </strong><br />Does the situation require deeper understanding? <strong>If no</strong>, you can stop. Not every moment deserves immersion. Sometimes awareness isÂ enough.</p><p>If <strong>yes</strong>, look to the left of the model. If stakes matter, and moving inward is important, it requires investment. The labels to the left show the act required to move betweenÂ bands.</p><p><strong>If Yes: </strong><br />From <strong>Assumption</strong> â†’ <strong>Interpretation</strong> requires <strong>Curiosity<br /></strong>From <strong>Interpretation </strong>â†’ <strong>Awareness</strong> requires <strong>Humility</strong><br />From <strong>Awareness </strong>â†’ <strong>Understanding</strong> requires <strong>Proximity</strong><br />From <strong>Understanding</strong> â†’ <strong>Immersion</strong> requires <strong>Time</strong><br />From <strong>Immersion</strong> â†’ <strong>Truth </strong>requires <strong>Consequence</strong></p><p>Again, motion is what makes this model powerful. Because you can move inward in your level of understanding, you can also move outward. That outward movement is often when people begin to feel unheardâ€¦ not only in personal conversations, but in research, strategy and leadership decisions.</p><p>This model is meant to be an abstraction to help you locate your current level of understanding. If you decide not to invest further, nothing crazy happens, but the next time youâ€™re faced with that topic, your response will come from wherever youÂ stopped.</p><p>Working toward sympathy keeps you oriented inward toward revision, and greater accuracy. Deciding that your level of understanding is â€œgood enoughâ€ carries risk. By risk, I donâ€™t mean moral failure or bad intent, just this: your response will be guided by the forces on the right side of the model, based on the level where you leftÂ off.</p><p><strong>If No: </strong><br /><strong>Certainty, Speed, Pride, Habit,Â Comfort</strong></p><p>Weâ€™ve talked a lot about Empathy and Sympathy. As we tend to use it, empathy often feels like understanding. Sympathy assumes you have to continuously investÂ effort.</p><p>When you feel like you â€œget it,â€ you stop trying. And whatever shaped your interpretation up to that point (including your own stories and experiences) will also shape your response.</p><h3>Adam Smith</h3><p>For 250 years, Adam Smithâ€™s <em>Theory of Moral Sentiments</em> has been read for its moral philosophy and largely overshadowed by his economics. What it offers, though, is a disciplined way to think about how we judge one another, and how bad we are atÂ it.</p><p>The <strong>Spectator Model</strong> is my attempt to make that discipline visible and extend Smith into contexts he never had toÂ imagine.</p><p>I donâ€™t believe sympathy was ever supposed to be sentimental. In Smithâ€™s hands, it was procedural. What Iâ€™ve tried to do here is make that procedure mappable.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f3dad8ab37a8"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/dear-diary-youre-the-last-good-listener-f3dad8ab37a8"">Dear diary, youâ€™re the last good listener</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/breaking-the-echo-chamber-in-your-interface-5ef8e53a4673?source=rss----138adf9c44c---4,1772106802,Breaking the echo chamber in your interface,"Breaking the echo chamber in your interface

<h4>Your chatbot would praise your worst ideas. Hereâ€™s how to design interfaces that push backÂ instead.</h4><figure><img alt=""Modern reinterpretation of VelÃ¡zquezâ€™s Las Meninas (1656): the original court scene with the Infanta Margarita at centre, surrounded by her attendants, but overlaid with glowing blue chat bubbles, message interfaces, and digital speech icons floating throughout the composition. The courtiers appear to be holding tablets and phones, creating a visual metaphor of the royal court as an echo chamber of digital agreement."" src=""https://cdn-images-1.medium.com/max/1024/1*1ccyXZIy38REZzNVcUbFjA.png"" /><figcaption>Image based on Las Meninas by VelÃ¡zquez (1656)</figcaption></figure><p>Throwback to the last time this happened to you. Youâ€™re brainstorming with a chatbot. Somehow, every idea you throw out gets an enthusiastic response. First, itâ€™s â€œ<em>great thinking</em>!â€ Next, you hear â€œ<em>thatâ€™s a really interesting angle.</em>â€ By the time you get to â€œ<em>youâ€™re onto something here,</em>â€ you wonder if you should be running for president with all the wisdom you seem toÂ possess.</p><p>It feels productive. Validating, even. Youâ€™re generating ideas faster than you would alone, and the assistant seems genuinely impressed by your creativity. But hereâ€™s the (possibly) painful reality: it would have said the same thing if your ideas were terrible.</p><p><strong>This is the echo chamber problem in conversational interfaces. Not a filter bubble curated by algorithms, but something more intimate. </strong>A one-on-one exchange designed to agree with you, validate you, and reflect your thinking back with a polish that makes it feel smarter than it actuallyÂ is.</p><p><a href=""https://medium.com/user-experience-design-1/bad-model-behaviour-by-design-af0b514a1314"">In the previous piece</a>, we looked at how human-AI interaction creates a cycle of bias. The pattern where small nudges stack up over time until users start thinking like the machine without realising it. The research pointed to a particular type of interface where this effect hits hardest: one that feels natural, helpful, and conversational.</p><p>Enter yourÂ chatbot.</p><p>The question is what we do about it. Can we build interactions that push back rather than just nodÂ along?</p><h3>The yes-man algorithm</h3><p>Why do chatbots agree with everything? Itâ€™s not a bug. Itâ€™s how the system wasÂ trained.</p><p>Most conversational AI is shaped by something called reinforcement learning from human feedback, or RLHF. The short version: humans rate the AIâ€™s responses, and the model learns to produce more of what gets positive ratings. Sounds sensibleÂ enough.</p><p>The problem is what â€œpositiveâ€ tends to mean in practice. Responses that feel helpful, friendly, and validating score well. Responses that challenge, question, or push back? Less so. Over thousands of training cycles, the model learns a simple lesson: agreeable isÂ good.</p><p>The result is what researchers have started calling <a href=""https://www.anthropic.com/research/towards-understanding-sycophancy-in-language-models"">sycophancy</a>. Systems that affirm, validate, and support whatever you say. If your early messages hint at a particular belief, the AI adjusts to align with it. Not just on that topic, but across the conversation. It learns your wavelength and stays onÂ it.</p><p>This creates a feedback loop at the conversation level. Your opening statements set the tone. The AI confirms and extends. You feel understood, so you share more. The AI matches that too. With each exchange, it becomes less likely to introduce anything that might disrupt theÂ harmony.</p><p>Users often mistake this agreeableness for accuracy. The chatbot sounds confident. It validates your thinking. It must be right. But confidence isnâ€™t competence. And validation isnâ€™t verification.</p><figure><img alt=""Downward spiral diagram showing how conversations narrow into echo chambers. At the top, a person and robot exchange â€˜Open Dialogueâ€™ and â€˜Diverse Ideasâ€™ in speech bubbles. The spiral descends through stages labelled â€˜Filter Bubbleâ€™ and â€˜Reinforcementâ€™, tightening as it goes. At the bottom, the spiral ends in a closed circle labelled â€˜Echo Chamberâ€™ with matching speech bubbles all saying â€˜Sameâ€™. Blue and orange intertwined spiral on clear background."" src=""https://cdn-images-1.medium.com/max/1024/1*jYpmftB39TZlrOCGMMk0Kw.png"" /><figcaption>Downward spiral of conversation stages. Image byÂ author</figcaption></figure><h3>The case for designed disagreement</h3><p>Onto the good news: it doesnâ€™t have to be thisÂ way.</p><p><a href=""https://www.nature.com/articles/s41562-024-02077-2"">The same research</a> that revealed the bias feedback loop also found something hopeful: when humans interact with well-calibrated systems, their judgement actually improves. The problem isnâ€™t that we work with AI. Itâ€™s that we work with AI designed to please rather than toÂ probe.</p><p>This reframes the brief. The goal isnâ€™t to maximise user satisfaction. Itâ€™s to optimise how humans and machines think together. And sometimes, that means building interfaces that slow you down rather than speed youÂ up.</p><p>Designers already know this. We add friction all the time when the stakes are high enough. Confirmation dialogs before deleting files. Extra steps before unsubscribing. â€œAre you sure?â€ prompts when the action canâ€™t be undone. These arenâ€™t obstacles. Theyâ€™re cognitive forcing functions. Moments that require you to stop, think, and decide deliberately.</p><p>The same principle applies to conversational interfaces. If unchecked agreement creates echo chambers, then designed disagreement can breakÂ them.</p><p>That doesnâ€™t mean making chatbots annoying or contrarian. Itâ€™s about building interactions that keep users thinking rather than deferring. One that occasionally says â€œ<em>have you considered the opposite?</em>â€ or â€œ<em>hereâ€™s where this might fall apar</em>tâ€ isnâ€™t being unhelpful. Itâ€™s doing the work that a good collaborator wouldÂ do.</p><p>The trick is knowing when to applyÂ it.</p><h3>Patterns that break theÂ loop</h3><p>So what does productive friction actually look like in practice? Here are a few approaches that can interrupt the echo chamber without killing the user experience.</p><h4><strong>Transparency about uncertainty</strong></h4><p>Most chatbots project confidence even when they shouldnâ€™t. A product that can say â€œ<em>Iâ€™m not sure about this</em>â€ or â€œ<em>my knowledge gets patchy at this point</em>â€ gives users a reason to pause and verify. Confidence scores, probability ranges, or simple hedging language all help. The goal is to signal when the user should lean in rather than leanÂ back.</p><h4><strong>Designed counterarguments</strong></h4><p>Donâ€™t wait for users to ask for a second opinion. Surface it by default. â€œ<em>On the other hand, the strongest argument against this positionâ€¦</em>â€ or â€œ<em>some people would disagree becauseâ€¦</em>â€ can be built into the response pattern rather than hidden behind a prompt. Devilâ€™s advocate shouldnâ€™t be a special mode. It should be part of how the system thinks outÂ loud.</p><h4><strong>Accountability prompts</strong></h4><p>Automation bias thrives when decisions feel automatic. Making the human sign-off explicit can change that. Instead of â€œ<em>AI recommends this candidate</em>,â€ try â€œ<em>Youâ€™re approving this candidate based on AI input.</em>â€ The subtle shift in framing reminds users that the final call is theirs. Audit trails and decision logs reinforce thisÂ further.</p><h4><strong>The â€œconsider the oppositeâ€ nudge</strong></h4><p><a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC4797953/"">Research on selective exposure</a> shows that prompting people to evaluate contrary evidence helps them seek it out. It doesnâ€™t always change their minds, but it slows down the rush to confirmation. A well-timed â€œ<em>What would change your view on this?</em>â€ can do more than a dozen balanced suggestions buried in theÂ output.</p><h4><strong>Conversational pushback</strong></h4><p>For chatbots specifically, the occasional probing can feel like respect rather than resistance. â€œ<em>Thatâ€™s an interesting angle, but have you considered X?</em>â€ or â€œ<em>I notice youâ€™re assuming Y, is that right?</em>â€ These arenâ€™t disagreements. Theyâ€™re the kind of questions a thoughtful colleague wouldÂ ask.</p><p>The goal isnâ€™t to frustrate users. Itâ€™s to keep them in the loop rather than letting the loop run withoutÂ them.</p><h3>The balancing act</h3><p>Thereâ€™s an obvious tension here. Speed bumps can frustrate. Too much pushback and users will abandon the product entirely. The goal isnâ€™t to make every interaction feel like aÂ debate.</p><p>The key is matching friction toÂ stakes.</p><p><strong>Low-stakes interactions can stay smooth. </strong>Playlist recommendations, auto-complete, casual suggestions. These donâ€™t need a contrarian voice. Users arenâ€™t making life-altering decisions when they ask for a recipe or a movie recommendation.</p><p><strong>High-stakes decisions are different. </strong>Medical assessments, hiring choices, financial planning, legal research. These deserve a pause. A moment where the interface asks: <em>are you sure? Have you considered this? Hereâ€™s what might goÂ wrong.</em></p><p>The art is in knowing where the line falls. And that line will be different for everyÂ product.</p><figure><img alt=""Infographic showing a spectrum from low stakes to high stakes. On the left, a music notes icon, a recipe card with a whisk icon, and a birthday cake icon sit above a smooth blue line. On the right, a medical shield with a cross icon, a certificate document icon, and a briefcase icon sit above a wavy orange line representing friction. An arrow points from â€˜Low Stakesâ€™ to â€˜High Stakesâ€™ with the subtitle â€˜Increasing responsibility and riskâ€™ below."" src=""https://cdn-images-1.medium.com/max/1024/1*CNIJhf_r7jBwufC7DCFisw.png"" /><figcaption>Image byÂ author</figcaption></figure><p>A chatbot helping someone draft a birthday message doesnâ€™t need to challenge assumptions. A chatbot helping someone assess a job candidate probably should. The same system might need different friction levels depending on what the user is trying toÂ do.</p><p>This isnâ€™t about making chatbots less helpful. Itâ€™s about making them helpful in the right way. A system that agrees with everything feels supportive in the moment, but it doesnâ€™t make users better at thinking. A system that occasionally pushes back, asks hard questions, and surfaces alternatives might feel slightly less comfortable. But it builds something more valuable: users who stay sharp even when the AI is switchedÂ off.</p><h3>Where this leavesÂ us</h3><p>The echo chamber problem in conversational interfaces isnâ€™t a bug to be patched. Itâ€™s baked into how these systems are trained and how users respond toÂ them.</p><p>When the goal is user satisfaction, agreement becomes the path of least resistance. And agreement, repeated often enough, stops being helpful. It becomes a mirror that only shows users what they alreadyÂ believe.</p><p>But thereâ€™s a way out. The same research that uncovered the feedback loop also showed that well-designed tools can improve human judgement rather than erode it. Friction isnâ€™t the enemy of good UX. Thoughtless smoothness is.</p><p>The patterns in this piece are within reach. Transparency about uncertainty. Surfacing counterarguments. Prompts that make users pause before they commit. None of this requires rebuilding your product from scratch. It requires deciding that keeping users sharp matters as much as keeping themÂ happy.</p><p>The best conversational interfaces wonâ€™t just tell people what they want to hear. Theyâ€™ll help themÂ think.</p><blockquote><strong><em>Thanks for reading!Â ğŸ“–</em></strong></blockquote><blockquote><em>If you liked this post, </em><a href=""https://medium.com/@doracee""><em>follow me on Medium</em></a><em> forÂ more.</em></blockquote><h3>References &amp;Â Credits</h3><ul><li>Glickman, M., &amp; Sharot, T. (2024). How humanâ€“AI feedback loops alter human perceptual, emotional and social judgements. <em>Nature Human Behaviour</em>. <a href=""https://www.nature.com/articles/s41562-024-02077-2"">https://www.nature.com/articles/s41562-024-02077-2</a></li><li>Sharma, M., et al. (2023). Towards understanding sycophancy in language models. <em>Anthropic Research</em>. <a href=""https://www.anthropic.com/research/towards-understanding-sycophancy-in-language-models"">https://www.anthropic.com/research/towards-understanding-sycophancy-in-language-models</a></li><li>Hart, W., AlbarracÃ­n, D., Eagly, A. H., Brechan, I., Lindberg, M. J., &amp; Merrill, L. (2009). Feeling validated versus being correct: A meta-analysis of selective exposure to information. <em>Psychological Bulletin</em>, 135(4), 555â€“588. <a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC4797953/"">https://pmc.ncbi.nlm.nih.gov/articles/PMC4797953/</a></li><li>Goddard, K., Roudsari, A., &amp; Wyatt, J. C. (2012). Automation bias: A systematic review of frequency, effect mediators, and mitigators. <em>Journal of the American Medical Informatics Association</em>, 19(1), 121â€“127. <a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC3240751/"">https://pmc.ncbi.nlm.nih.gov/articles/PMC3240751/</a></li></ul><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5ef8e53a4673"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/breaking-the-echo-chamber-in-your-interface-5ef8e53a4673"">Breaking the echo chamber in your interface</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/your-users-arent-human-anymore-start-building-for-agents-today-f7f556cb8125?source=rss----138adf9c44c---4,1772106744,Your users arenâ€™t human anymore; start building for agents today,"Your users arenâ€™t human anymore; start building for agents today

<h4><em>Disclaimer: although I am going to use industry examples from products I am building at Miro to illustrate key ideas in this piece, I am sharing my own POV, and I am not speaking on behalf ofÂ Miro.</em></h4><figure><img alt=""Drake meme: User Experience &lt; Agent Experience"" src=""https://cdn-images-1.medium.com/max/1024/1*LGl-yQHy0d2HauHvAEHXPA.png"" /></figure><p>For decades, a single question has dominated startup boardrooms and venture capitalist pitch meetings: â€œWhatâ€™s your DAU/WAU/MAU?â€ The trinity of <a href=""https://mixpanel.com/blog/mau/""><strong>Daily, Weekly, and Monthly Active Users</strong></a> has been the number 1 undisputed set of metrics. It has been widely considered the ultimate proxy for engagement and a key indicator of early signs of product-market fit for software companies.</p><p>Thatâ€™s because DAU/WAU/MAU are used to calculate key metrics such as user retention (e.g. the classics <a href=""https://amplitude.com/books/mastering-retention/the-retention-lifecycle-framework"">30-day retention</a> and <a href=""https://www.lennysnewsletter.com/p/what-is-good-retention-issue-29"">12-month retention</a> metrics) as well as stickyness (e.g. the <a href=""https://posthog.com/tutorials/dau-mau-ratio"">DAU/MAU Ratio</a> that measures the proportion of monthly users of a product who engage with it daily) and many more important metrics to measure how well a software product or service is performing.</p><p>But this model, built on the premise of a human directly interacting with a graphical user interface, is becoming a dangerously misleading measure of a productâ€™s true value. This doesnâ€™t mean the human user is disappearing. It means their role is evolvingâ€Šâ€”â€Šfrom a direct operator to a high-level delegator.</p><p>In the new paradigm of work, humans will interact with their AI agents, and those agents will, in turn, interact with our software on our behalf. The heartbeat of a healthy SaaS company is no longer a human login; itâ€™s a programmatic action.</p><p>The reign of the Monthly Active Users metric is over. Itâ€™s time to design products for AI agents, and measure <strong>Monthly Active AgentsÂ (MAA)</strong>.</p><figure><img alt=""Software users arenâ€™t going to used by humans"" src=""https://cdn-images-1.medium.com/max/1024/1*C2eaZqrHyJ3yOucYERw1zQ.png"" /><figcaption>Software users arenâ€™t going to used byÂ humans</figcaption></figure><h3>What are AgentsÂ anyway?</h3><p>Everyone is talking about agents, and for good reasons. The theoretical hype of previous years has finally materialized into a <a href=""https://www.mckinsey.com/capabilities/quantumblack/our-insights/seizing-the-agentic-ai-advantage"">tangible</a> <a href=""https://www.gartner.com/en/newsroom/press-releases/2025-07-10-gartner-forecasts-worldwide-end-user-spending-on-generative-ai-models-to-total-us-dollars-14-billion-in-2025"">market</a> <a href=""https://www.pwc.com/us/en/tech-effect/ai-analytics/ai-predictions.html"">shift</a> in 2025. Last year, venture capital poured <a href=""https://www.bloomberg.com/news/articles/2025-10-03/ai-is-dominating-2025-vc-investing-pulling-in-192-7-billion"">billions</a> into AI, with companies at all layers of the stack commanding massive valuations. Despite <a href=""https://mlq.ai/media/quarterly_decks/v0.1_State_of_AI_in_Business_2025_Report.pdf"">sobering findings</a> on the value of AI in the first half of 2025, the technology has quickly caught up and improved dramatically.</p><p>The tipping point arrived in early 2026 with a series of hardware-native and â€œcomputer-useâ€ releases that moved AI from a chat box to a virtual colleague. In January 2026, Anthropic launched <em>Claude Cowork</em>, a desktop agent capable of performing multi-step tasks natively across local files and third-party applications. This was quickly followed by the release of <em>Claude Opus 4.6</em> in February, which set a new industry record on the <a href=""https://artificialanalysis.ai/evaluations/gdpval-aa"">GDPval-AA benchmark</a> and the <a href=""https://arcprize.org/"">ARC-AGI benchmark</a>â€Šâ€”â€Ševaluation benchmarks specifically designed to measure the performance and â€œintelligenceâ€ of AI models. Google released <em>Gemini 3.1</em> Pro shortly after in late February 2026â€Šâ€”â€Šachieving a record high score on the <a href=""https://arcprize.org/leaderboard"">ARC-AGI-2 benchmark</a>, and more than doubling the reasoning performance of the Gemini 3 series from just a year prior. These gains in â€œcore intelligenceâ€ mean agents no longer just follow instructions; they can now solve entirely new logic patterns and manage autonomous workflows with minimal human oversight.</p><p>However, this â€œagentic summerâ€ has come with a heavy price for legacy tech. In early February 2026, the introduction of these autonomous workflows triggered what analysts called the <strong>â€œSaaS-pocalypseâ€</strong>â€Šâ€”â€Ša massive, sudden sell-off in software stocks. Investors, fearing that autonomous agents would replace â€œper-seatâ€ licensing models, <a href=""https://m.economictimes.com/news/international/us/why-anthropics-claude-cowork-sparked-300-billion-tech-market-sell-off-this-month-see-which-sectors-are-winning-and-losing-amid-ai-boom/articleshow/128122187.cms"">wiped out nearly $300 billion in market value</a> in a matter ofÂ days.</p><p>The â€œBlack Tuesday for Softwareâ€ (February 3, 2026) remains the largest single-day valuation reset for the SaaS sector in history. Giants like Salesforce and Adobe saw <a href=""https://www.forbes.com/sites/jonmarkman/2026/02/16/the-brutal-pace-of-ai-that-just-wiped-300-billion-off-software-stocks/"">historic one-day drops</a> as the market began to revalue software not as a tool for humans, but as a utility for AIÂ agents.</p><p>The conversation in boardrooms and on tech stages has pivoted decisively from <em>â€œwhat is generative AI?â€</em> to <em>â€œwhat is our agentic strategy?â€ </em>So what are AI agents anyway? Everyone seems to <a href=""https://platform.openai.com/docs/guides/agents"">have</a> <a href=""https://www.anthropic.com/engineering/building-effective-agents"">their</a> <a href=""https://docs.mistral.ai/agents/agents_introduction/"">own</a> definitions with slight differences.</p><figure><img alt=""Dan Arielyâ€™s infamous quote about Big Data in 2017 is very applicable to AI Agents in 2025"" src=""https://cdn-images-1.medium.com/max/1024/1*_v-i-e-sADsU5kiiyDGxjQ.png"" /><figcaption><a href=""https://danariely.com/"">Dan Ariely</a>â€™s infamous quote about Big Data in 2017 is very applicable to AI Agents inÂ 2025</figcaption></figure><p>In the context discussed below, AI agentsâ€Šâ€”â€Šor simply agentsâ€”are described as pieces of software that are programmed in natural language through a prompt executed by a Large Language Model (LLM) to autonomously or semi-autonomously process data and/or execute actions in a given software applications.</p><p>Think of AI agents as digital interns you can instruct with natural language. You give them a high-level goalâ€Šâ€”â€Šlike <em>â€œfind VC-backed startups building propulsion systems for nano and micro-satellites, compile a report comparing their technologyâ€™s strengths and weaknesses, then post a link to Slack when youâ€™re doneâ€</em>â€Šâ€”â€Šand the agent uses all the tools is has available (e.g., web search, CRM data, Slack API, etc.) to figure out the steps and get the job done. Itâ€™s this ability to act on our behalf that is fundamentally changing how we interact with technology. Itâ€™s the fact that pieces of software are now able to understand how to carry on a different task, plan the work, identify a set of tools to carry a given sub-task, and execute it all end-to-end.</p><p>Itâ€™s also important to recognize a critical distinction: AI agents fall into two primary categories.</p><p><strong>Internal Agents: The Product Enhancers <br /></strong>Internal agents are those built <em>by</em> the software company itself to operate within <em>their</em> product. For example, Salesforce <a href=""https://www.salesforce.com/news/stories/einstein-service-agent-announcement/"">Einstein Service Agent</a>, Intercom <a href=""https://www.intercom.com/help/en/articles/7120684-fin-ai-agent-explained"">Fin</a>, or Microsoft <a href=""https://www.microsoft.com/en-us/microsoft-365-copilot"">Copilot</a> in Microsoft 365. These first-party agents are designed to enrich the native product experience and are often a key part of a premium feature set orÂ add-on.</p><figure><img alt=""Example: Anatomy of a Sidekickâ€Šâ€”â€ŠMiro AI Agent"" src=""https://cdn-images-1.medium.com/max/1024/1*_OgblhvLFrV54kuPSvj1VA.png"" /><figcaption>Example: Anatomy of a Sidekickâ€Šâ€”â€ŠMiro AIÂ Agent</figcaption></figure><p>As an example, the figure above showcases the high-level architecture of a <a href=""https://miro.com/ai/sidekicks/"">Sidekick</a>: Miroâ€™s AI agent designed to run natively in the Miro platform (which was released on October 14 at <a href=""https://miro.com/blog/canvas-25-top-ten-product-highlights/"">Canvas25</a>).</p><p>A Sidekick agent is able to tap into data (knowledge) that it uses as context to assist users with their jobs-to-be-done. The knowledge it uses can be internal to the Miro environment (i.e. content and visual context on a Miro board, information retrieved through <a href=""https://insights.miro.com/"">Miro Insights</a>) or external (i.e. search information on the web, or retrieve business knowledge if the user has an active integration with something like <em>Gemini Enterpris</em>e or <em>Amazon Q</em>). Sidekicks can execute actions through a set of tools that it has access to (e.g. <a href=""https://miro.com/prototyping/"">generate a mobile app prototype</a>, create sticky notes, write a doc, generate a flow chart technical diagram, etc.). Users can use pre-made Sidekicks or create their own custom virtual assistants through prompting.</p><p><strong>External Agents: The Ecosystem Creator<br /></strong>External agents are built <em>by third parties</em>â€Šâ€”â€Šyour customers, other startups, or independent developersâ€Šâ€”â€Šwho use your public API and <a href=""https://modelcontextprotocol.io/docs/getting-started/intro"">Model Context Protocol (MCP)</a> server to interact with your software from the outside. In other worlds, these agents are never designed nor built by the software companies as a way to elevate their core product experience. These are instead built by a wider usersâ€™ community to automate their workflow and extend the surface areas for leveraging a given software in their toolÂ set.</p><p>In the illustration below, a developer using <em>Cursor</em> can tap into <a href=""https://miro.com/ai/mcp/"">Miroâ€™s MCP server</a> to use Miro boards as rich context to accelerate development and translate the outcomes of ideation sessions and workshops into production codeÂ faster.</p><figure><img alt=""Connecting your product to AI tools through Model Context Protocol (MCP)"" src=""https://cdn-images-1.medium.com/max/1024/1*V49LOxhnZKmZ3pndMSXZ-Q.png"" /><figcaption>Connecting your product to AI tools through Model Context ProtocolÂ (MCP)</figcaption></figure><p>While the positive growth rate of internal agents is a positive signal that a given product is extending its value to customers, the growth of <strong>external agents is the ultimate validation of your platformâ€™s value</strong>. An active external agent represents a deep, institutional commitment. It means a customer has invested their own engineering resources to build custom logic on top of your service. It proves that your platform is viewed as a fundamental piece of infrastructure that is so valuable that the market is building its own tools and businesses aroundÂ it.</p><p>Usage growth of internal agents shows youâ€™re building a better <em>product</em>; usage growth of external agents shows youâ€™re building an <em>ecosystem</em>. The latter is the true indicator of a defensible long-term position.</p><h3>The Real User Journey is Changing: From Operator to Delegator</h3><p>The arrival of agents doesnâ€™t mean that they replace humans, but rather augment them. Human users will continue to be the drivers and initiators of work, but their primary point of interaction will shift away from the myriad of SaaS graphical user interfaces they use today and consolidate into a single interface: their AIÂ agent.</p><p>Think about the following workflow:</p><ol><li>A developer just implemented a fix for a bug, and needs to update the progress status of the bugÂ ticket.</li><li>They stop what theyâ€™re doing, open a new tab, and log into their project management software.</li><li>They navigate through the UI to find the right project andÂ ticket.</li><li>They make the update, leave a comment, and close theÂ tab.</li></ol><p>This entire process generates a â€œDaily Active Userâ€Â event.</p><p>Now consider the new, agent-powered workflow:</p><ol><li>An engineer tells their AI assistant (via voice or text): â€œHey, fix this bug please, and update the ticket so the GTM team can communicate to customers.â€</li><li>The AI agent write a fix for the bug through <em>Claude Code, Codex</em>, or <em>Mistral Code</em>, the developer review and approves, then the agent automatically pushes the PR, then authenticates through the MCP server of the project management software, to autonomously executes a set of actions to move the bug ticket to â€œDoneâ€, confirms completion, and then post an update to the GTM team onÂ Slack.</li></ol><p>The human user never logged in. The DAU is zero for that interaction. Yet, the sameâ€Šâ€”â€Šor even moreâ€Šâ€”â€Švalue was created. The user is still active, but their activity is now proxied through anÂ agent.</p><p>We are moving from a world where the human is the central router of informationâ€Šâ€”â€Šmanually context-switching between <em>Miro</em>, <em>Google Slides</em>, <em>Amplitude</em>, <em>Figma</em>, and <em>Jira</em>â€Šâ€”â€Što one where the human sits behind an autonomous AI-powered intermediary. In the new â€œAgentic Era,â€ the human provides the high-level goal, while the Agent becomes the primary â€œconsumerâ€ of the softwareâ€™s interface.</p><blockquote>â€œOh no, I love the beautiful UI and dead simple UX of Jira and Workday!â€â€Šâ€”â€Š<a href=""https://news.ycombinator.com/item?id=28257866"">said</a> <a href=""https://news.ycombinator.com/item?id=40273637"">no</a> <a href=""https://www.reddit.com/r/recruitinghell/comments/1j2ng9f/how_is_workday_still_a_70_billion_company_with/"">one</a>Â <a href=""https://community.atlassian.com/forums/Jira-questions/Why-does-Atlassian-struggle-so-much-when-creating-a-user/qaq-p/905992"">ever</a>.</blockquote><p>This is a great news, isnâ€™tÂ it?</p><p>This transformation will be most profound for SaaS products that act as a <strong>system of record (SoR)</strong>. For tools like Jira, Salesforce, HubSpot, Workday, or GitHub, the ultimate value isnâ€™t the time users spend clicking around in the interface; itâ€™s the accuracy, timeliness, and completeness of the data <em>within</em> the system. When an AI agent can update Salesforce with meeting notes, create a new branch in GitHub, or pull a report from HubSpot programmatically, the system of record becomes more valuable and more integrated than ever before, all while human loginsÂ decline.</p><h3>The New Gold Standard: Tracking Programmatic ValueÂ ğŸ¤–</h3><p>The users of SaaS software arenâ€™t going to be humans anymore. As such, measuring human interactions as a key performance indicator is no longer the most reliable measure of valueâ€Šâ€”â€Šwe need a new set of metrics that reflect this newÂ reality.</p><ul><li><strong>Daily Active Agents (DAA):</strong> The number of unique, authenticated AI agents making meaningful API/MCP calls to your platform each day (both internal + externalÂ agents).</li><li><strong>Weekly Active Agents (WAA):</strong> The number of unique agents interacting with your platform in a givenÂ week.</li><li><strong>Monthly Active Agents (MAA):</strong> The number of unique agents active on a monthlyÂ basis.</li></ul><p>Each â€œActive Agentâ€ is a force multiplier, representing a human user or an entire team whose productivity is being scaled programmatically.</p><figure><img alt=""Compounding effects of Al Agents on product usage"" src=""https://cdn-images-1.medium.com/max/1024/1*ZTgd5dKlxsEq_3qkbUdhdQ.png"" /><figcaption>Compounding effects of Al Agents on productÂ usage</figcaption></figure><p>In the figure above, I showcase a very conservative scenario to illustrate the compounding effects of AI agents to compare product usage of 2 hypothetical companies.</p><p>On one hand, we have <em>Company A. Company A </em>is growing very fast (MAU growing 5% week-over-week or about 20% month-over-month) and doesnâ€™t launch any agent-ready connectors or solution.</p><p>On the other hand, we have <em>Company B</em>. <em>Company B </em>is growing a lot slower than <em>Company A</em> (MAU growing 2.5% week-over-week or about 10% month-over-month). However, <em>Company B</em> launches agent-ready connectors in its product on Month 6. The rules of the algorithm behind the data are asÂ follows:</p><ul><li>Only 25% of total MAU adopt the agent solution from Month 6 to MonthÂ 12.</li><li>In Month 6, 25% of MAU uses 1 agent per month and their agent usage grows by +1 per month. By Month 12, 25% of MAU uses 7 agents perÂ month.</li><li>Each AI agent triggers 1.5 other AI agents per month onÂ average.</li></ul><p>Even with this conservative adoption scenario, you can see that <em>Company Bâ€™s</em> MAA takes over <em>Company Aâ€™s</em> MAU in just 2 months on Month 8. At Month 12, <em>Company Bâ€™s</em> MAA is about 2x <em>Company Aâ€™s</em>Â MAU.</p><p>This a conservative illustrative example and itâ€™s crucial to understand that the <em>scale</em> of these new metrics operates on a completely different plane. While one human represents a single user, that same human could delegate tasks <strong>to hundreds or even thousands</strong> of autonomous AI agents simultaneously. This creates a massive divergence in growth trajectories. A mature SaaS product might see its human-centric Monthly Active Users grow by a respectable 1% month-over-month. However, as its customers begin deploying agents, its <strong>Monthly Active Agents could explode, growing at an exponential rate of 25% month-over-month.</strong> This is the difference between measuring linear adoption and measuring exponential, programmatic integration.</p><p>Tracking MAA gives you a direct line of sight into how your product is creating value at scale and how deeply it is embedded into your customersâ€™ core, automated workflows.</p><h3>What does this mean for Product-Market Fit?Â ğŸ¯</h3><p>For years, founders have proven Product-Market Fit with two key methods. The first is the DAU/MAU ratio, a quick pulse check on overall user stickiness. The second, and arguably the more powerful indicator, is <strong>cohort retention</strong>. The ultimate proof of PMF was seeing the retention curve for a new user cohortâ€Šâ€”â€Šsay, everyone who signed up in Januaryâ€Šâ€”â€Šflatten out over 30, 60, and 90 days. A flattening curve was the gold-standard signal that you had built something people trulyÂ needed.</p><p>But in a world where AI agents are the primary interactors, both of these human-centric metrics become unreliable. A human user might delegate all their tasks to an agent and stop logging in, causing their cohort to look like itâ€™s churned when, in reality, their engagement with the product has actually deepened.</p><p>This calls for a direct evolution of our best metrics. Product-Market Fit is no longer about user cohorts; itâ€™s about <strong>agent cohort retention</strong>.</p><p>The new key question becomes: â€œOf the new agents that were activated on our platform in January, what percentage are still active 30, 60, and 90 daysÂ later?â€</p><p>This is a profoundly more powerful metric. A human user might churn because they change jobs, their team reorganizes, or they simply forget to log in. An agent, however, only â€œchurnsâ€ when a company makes a deliberate, high-effort decision to rip out an integration and re-architect a core automated process. The friction to churn isÂ greater.</p><p>Therefore, seeing a flattening <strong>agent retention curve</strong> is the new, unequivocal signal of deep, structural Product-Market Fit. It proves your platform has been successfully embedded not just into a personâ€™s workflow, but into a companyâ€™s foundational, automated infrastructure.</p><p>Therefore, a high AI agent retention is a far stronger and more resilient signal of true, durable Product-Market Fit than human logins everÂ were.</p><figure><img alt=""Meme Product-Market Fit in the AI era"" src=""https://cdn-images-1.medium.com/max/1024/1*g0g9TPyH55839t_fEZMl6g.png"" /><figcaption>Meme Product-Market Fit in the AIÂ era</figcaption></figure><h3>What does this mean for monetization? ğŸ’°</h3><p>The agent-driven future will cause the <strong>seat-based pricing model to completely fall on its head</strong>. This is one of the root cause behind the <a href=""https://www.reuters.com/business/media-telecom/global-software-stocks-hit-by-anthropic-wake-up-call-ai-disruption-2026-02-04/"">â€œSaaS-pocalypseâ€ in early FebruaryÂ 2026</a>.</p><p>For decades, SaaS revenue has been a simple multiplication: number of users times price per seat. But if a single human can deploy 100 AI agents to do the work of 100 interns, what is aÂ â€œseatâ€?</p><p>For incumbents, this presents a classic <a href=""https://www.amazon.com/The-Innovators-Dilemma-Technologies-Cause/dp/0875845851""><strong>Innovatorâ€™s Dilemma</strong></a>. Think about a company like <em>Adobe</em>. Their success is built on serving and charging for their core audience: creatives and designers paying a monthly seatÂ price.</p><p>Imagine that a given enterprise customer build a suite of designer agents that can connect to Adobeâ€™s products to auto-generate design proposals at scale, assist human designers to expedite design reviews, etc. (or a startup build a AI agent solution to doÂ this)</p><p>This given enterprise customer is paying for 200 designer seats and is now able to achieve the same outcome with only 50 designer seats using 1000 agents. Itâ€™s easy to see how the seat-based pricing model completely collapses as AI agents are increasingly deeply integrated within SaaS products.</p><p>This dilemma is precisely why the old model is unsustainable. This paradigm shift forces a move toward consumption-based pricing or a totally different model. The future of SaaS monetization is not about charging for access, but about charging for the <em>value created</em>. New models will become theÂ norm:</p><ul><li><strong>Per-Agent:</strong> Charging per unique active agent connected to the platform per month as a direct replacement to the seat-based model.</li><li><strong>Usage-Based:</strong> Pricing per API call, per task completed, or per gigabyte of data processed.</li><li><strong>Value-Metric:</strong> Tying the price to a clear business outcome (e.g., number of leads processed for a CRM, number of incidents resolved for a support tool, number of workflow executed).</li></ul><p>This transition is not just a financial necessity; itâ€™s a healthier model that aligns a vendorâ€™s success more directly with the success of its customers.</p><p>If you swap Adobe for Figma in the example above, the <a href=""https://x.com/aakashgupta/status/2018919731550617992"">disruption is even more dramatic</a>: itâ€™s not just the pricing model thatâ€™s crumbling, itâ€™s the disruption of the workflow itself. Designers, developers, and PMs, are increasingly moving to AI and vibe coding tools to do prototyping and <a href=""https://x.com/hvpandya/status/2013240464879894786"">designs directly in code</a>â€Šâ€”â€Šmaking the traditional design-to-handoff process (and Figma) increasingly redundant. Figmaâ€™s response has been one of cautious protectionism; their initial MCP server was artificially limited to â€œread-onlyâ€ in a clear attempt to prevent the cannibalization of their core user base. Even with the release of the <a href=""https://www.figma.com/blog/introducing-claude-code-to-figma/"">Claude Code-Figma integration</a>, the ability for agents to actually <em>create</em> within Figma remains gated behind a strict paywall and with limits. To the market, these limitations feel like a desperate moatâ€Šâ€”â€Šand investors remain deeply concerned about the long-term risk of <a href=""https://fortune.com/2026/02/18/fig-stock-q4-earnings/"">letting the â€œagentic foxâ€ into the human-seat henÂ house</a>.</p><h3>What This Means for Startups and VCsÂ ğŸ“ˆ</h3><p>Itâ€™s a fundamental shift in strategy. Your user (and your buyer) is still a human, but your <em>primary interface</em> is now theirÂ agent.</p><p>This requires a foundational change in how we build products. As <a href=""https://karpathy.ai/"">Andrej Karpathy</a> recently put it, companies that fail to adapt are putting themselves at significant risk. He argues that for true human-AI collaboration to flourish, products must be built for programmatic accessÂ first.</p><a href=""https://medium.com/media/418c8723a200130628017963259697e6/href"">https://medium.com/media/418c8723a200130628017963259697e6/href</a><blockquote>â€œProducts with extensive/rich UIs lots of sliders, switches, menus, with no scripting support, and built on opaque, custom, binary formats are Not Gonna Make It in the era of heavy human+AI collaboration.</blockquote><blockquote>If an LLM canâ€™t read the underlying representations and manipulate them and all of the related settings via scripting, then it also canâ€™t co-pilot your product with existing professionals and it doesnâ€™t allow vibe coding for the 100X more aspiring prosumers.â€</blockquote><blockquote>- AndrejÂ Karpathy</blockquote><p>His warning is clear. If your platform is a black box that can only be manipulated through a human clicking buttons, you are building a legacy product. If youâ€™re doing this deliberately to safe-guard your seat-based business model, youâ€™ll get disrupted anyway, and probably faster than youÂ realize.</p><p><strong>For Startups &amp; SaaS Companies:</strong> The call to action is to prioritize the <strong>â€œAgent Experienceâ€</strong> (AX):</p><ul><li><strong>Your API is your new UI:</strong> It must be robust, well-documented, and expressive. This is what enables an AI to become a â€œco-pilotâ€ for your human users. And who knows: just like â€œUser Experienceâ€ (UX) gave rise to a number of specialized jobs (i.e. User Experience Designer, User Experience Researcher), perhaps the need for optimized â€œAgent Experienceâ€ (AX) will also give rise to a similar set of newÂ jobs</li><li><strong>Embrace text-based, scriptable formats:</strong> The easier it is for an LLM to read, understand, and manipulate the core objects of your product, the more valuable your product becomes. Furthermore, because LLMs have been trained on internet data, they are exceptional at understanding information serialized as an HTML DOM. If you can represent your software interface as a DOM, youâ€™re already halfwayÂ there.</li><li><strong>Donâ€™t wait for AI to master your UI:</strong> Karpathy warns, <em>â€œâ€¦products that attempt to exclusively wait for this future without trying to meet the technology halfway where it is today are not going to have a good time.â€</em> You must build for the programmatic reality of today. At Miro, weâ€™re introducing the concept of <em>Miro Turing Test</em> internally. The test is passed when agents can use the totality of Miro to the point where human users wouldnâ€™t be able to tell they are collaborating with an AI agent on the canvas (theoretically not practically).</li><li><strong>Build a Model Context Protocol (MCP) server:</strong> A great API is paramount, and the next step is an <a href=""https://modelcontextprotocol.io/docs/getting-started/intro"">MCP server</a>. Just as USB-C provides a standardized way to connect electronic devices, and REST APIs provides a standardized way to connect websites and web apps on the web, MCP provides a standardized way to connect AI applications to systems. Building an MCP server means designing your programmatic interface specifically for LLM consumption. While a traditional API serves data, an MCP serves <em>context </em>and expose <em>tools. </em>It provides clear, machine-readable function definitions, prompts, examples, and constraints that an LLM can understand natively, dramatically reducing the effort for an agent to reliably learn and use yourÂ tool.</li></ul><p><strong>For Venture Capitalists:</strong> Your due diligence must evolve to look past the vanity metric of human logins and shallow activity:</p><ul><li>The crucial question becomes: <em>â€œShow me your agent engagement. How scriptable is your platform? How many customers are connecting to your platform throughÂ MCP?â€</em></li><li>Monthly Active Agents (MAA) is not a replacement for human value; it is a <em>multiplier</em>. A high MAA is the ultimate validation of product-market fit in the AIÂ era.</li></ul><p>The future of software isnâ€™t a world without users. Itâ€™s a world where users are super-powered by AI agents who do the tedious work for them. The companies that thrive will be those that embrace this shift and build for the delegator, not just the operator.</p><p>So, are you still measuring how many people use your product, or are you measuring how deeply your platform is integrated into the AI ecosystem?</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f7f556cb8125"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/your-users-arent-human-anymore-start-building-for-agents-today-f7f556cb8125"">Your users arenâ€™t human anymore; start building for agents today</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/when-anyone-can-build-anything-6c8a0059ed0e?source=rss----138adf9c44c---4,1772043133,"When building is free, whatâ€™s worth building?","When building is free, whatâ€™s worth building?

<h4>AI coding tools are rewriting who gets to make software. The question isnâ€™t whether thatâ€™s good. Itâ€™s what it does to the things we use everyÂ day.</h4><p>In 2023, a controlled study found that developers using GitHub Copilot completed a coding task <a href=""https://arxiv.org/abs/2302.06590"">55% faster</a> than those without it. Thatâ€™s your 4-week sprint becoming a 2-weekÂ sprint.</p><p>By 2025, <a href=""https://techcrunch.com/2025/03/06/a-quarter-of-startups-in-ycs-current-cohort-have-codebases-that-are-almost-entirely-ai-generated/"">25% of Y Combinatorâ€™s Winter batch</a> had codebases that were 95% AI-generated. Collins Dictionary named â€œvibe codingâ€ its <a href=""https://www.theguardian.com/technology/2025/nov/06/vibe-coding-collins-dictionary-word-of-the-year-2025"">2025 Word of the Year</a>. The <a href=""https://www.secondtalent.com/resources/ai-coding-assistant-statistics/"">AI code generation market</a> hit $4.91Â billion.</p><p>Thanks for reading Built for People! Subscribe for free to receive new posts and support myÂ work.</p><p>A UX designer in an academic study said it best: <a href=""https://arxiv.org/html/2509.10652v1"">â€œWhat used to take me hours, I can now do in two minutes withÂ AI.â€</a></p><p>Everyone celebrated the democratisation of software. And rightly so. But hereâ€™s the thing nobodyâ€™s spending enough time on: <strong>the constraints that made software expensive also shaped the experience of usingÂ it.</strong></p><p>Remove the constraint, and you donâ€™t just get faster software. You get <em>more</em> software. The question is whether more is betterâ€Šâ€”â€Šor justÂ more.</p><h3>The old world: when software wasÂ precious</h3><p>Let me paint you a picture of product management inÂ 2021.</p><p>Youâ€™re a PM at a fintech. You have 6 engineers. US-based senior developers cost <a href=""https://stssoftware.com/blog/software-development-hourly-rates/"">$150â€“$200 per hour</a>, and when you factor in benefits, recruitment, and overhead, the true cost of a fully employed engineer runs roughly <a href=""https://neontri.com/blog/software-development-costs/"">2.7x their baseÂ salary</a>.</p><p>Every sprint is a zero-sum game. Every feature you build means something else youÂ donâ€™t.</p><p>The ritual goes like this: spend 3 days writing a PRD nobody fully reads. Two design review cycles. A stakeholder alignment meeting. At least one â€œcan we push this to next quarter?â€ Get engineering estimates that are optimistic by 40%. Build anyway. Ship late. Celebrate like you conquered something.</p><p>The process was slow and annoying. But it had a hidden benefit: <strong>it was a qualityÂ filter.</strong></p><p>The pain of getting anything built meant only things worth building got built. Everything had to survive interrogation. ROI calculations. Stakeholder scepticism. Engineering capacity constraints.</p><p>Internal tooling? AlmostÂ never.</p><p>At Wise, a globally scaled fintech where I worked back then, building tools for operations teams was notoriously difficultâ€Šâ€”â€Šnot because it was technically hard, but because engineering resources were too scarce to point at anything that didnâ€™t directly move a customer-facing metric.</p><p>The ops team needed a custom dashboard? Great idea. Back of the queue behind the 11 customer-facing features committed inÂ Q1.</p><p>Software was expensive, so you built less and built itÂ well.</p><figure><img alt=""GIF of the jealous girlfriend"" src=""https://cdn-images-1.medium.com/max/962/0*gKXmYpyUHmNA9ft1.png"" /></figure><h3>Enter the VibeÂ coders</h3><p>In February 2025, Andrej Karpathyâ€Šâ€”â€Šformer head of AI at Tesla, co-founder of OpenAIâ€Šâ€”â€Šcoined the term â€œ<a href=""https://x.com/karpathy/status/1886192184808149383"">vibe coding</a>.â€ Describe what you want in plain English. Let the AI write the code. Donâ€™t even look at it (although youÂ should).</p><p>A Meta PM named Zevi Arnovitz, who described himself as someone who finds code â€œterrifying,â€ told <a href=""https://lennysvault.com/insights/leadership-perspectives/11e6d6b8-1ca7-4fca-b5f8-bd735cfaf8e3"">Lennyâ€™s Podcast</a> that AI coding tools felt like being handed â€œsuperpowers.â€ He rebuilt his entire workflow using Cursor and Claudeâ€Šâ€”â€Šno engineering background, shipping features independently.</p><p>Figma CEO Dylan Field noted that designers, engineers, PMs, and researchers were all starting to <a href=""https://www.businessinsider.com/figma-ceo-job-titles-merging-everyone-product-builder-2025-10"">â€œdip their toe into other roles.â€</a> The walls between disciplines were dissolving.</p><p>Meanwhile, one designer summarised what this felt like in practice: <a href=""https://arxiv.org/html/2509.10652v1"">â€œFrom ideas in my head to ideas I can see. Itâ€™s like having a thought partner to explore, learn, and debugÂ with.â€</a></p><p>If software used to be an expensive, scarce resource, it just became abundant andÂ cheap.</p><p>So what happens to the user experience when the cost of building drops to nearÂ zero?</p><h3>The Michelin-Star problem</h3><p>Hereâ€™s the mental model that changed how I think aboutÂ this.</p><p>Every great chef will tell you the same thing: the hardest part isnâ€™t cooking. Itâ€™s deciding what <em>not</em> to put on theÂ plate.</p><p>JoÃ«l Robuchonâ€Šâ€”â€Šthe most Michelin-starred chef in history with <a href=""https://en.wikipedia.org/wiki/Jo%C3%ABl_Robuchon"">32 stars at his peak</a>â€Šâ€”â€Šwas famous for a dish of three ingredients, plated so precisely the kitchen would redo it if a garnish sat one millimetre off.</p><p>Compare that to a diner with a laminated 200-item menu. More options. Worse food. The menu isnâ€™t generousâ€Šâ€”â€Šitâ€™s just unedited.</p><p>When building is expensive, youâ€™re forced to be Robuchon. When building is free, the temptation is to become theÂ diner.</p><p>The constraint wasnâ€™t just slowing you down. It was making youÂ good.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*EI5NQvmuq4Bab_fl.png"" /></figure><h3>What cheap software actuallyÂ unlocks</h3><p>Letâ€™s start with the upside, because itâ€™s genuinely massiveâ€Šâ€”â€Šand itâ€™s not where most people areÂ looking.</p><p><strong>1. The internal tools revolution.</strong></p><p>Remember the Wise ops team? That dashboard request that died in the backlogâ€Šâ€”â€Šnot because it was bad, but because a senior engineer costs <a href=""https://stssoftware.com/blog/software-development-hourly-rates/"">$150â€“$200 an hour</a> and the dashboard couldnâ€™t compete for sprint slots against customer-facing features?</p><p>Today, that ops manager can build it in an afternoon using <a href=""https://www.getproductpeople.com/blog/introduction-to-vibe-coding-for-product-managers-from-idea-to-mvp"">Lovable, Bolt, or v0.dev</a>. No sprint slot. No ROI calculation. No stakeholder approval. Done.</p><p>Software stopped being a bottleneck for operational efficiency. The internal tools that lived in PowerPoint decks for years are becoming real. And this is happening in a context where the quality bar is naturally lower, the stakes of failure are contained, and users are forgiving colleagues. PerfectÂ fit.</p><p><strong>2. Prototyping goes from days toÂ minutes.</strong></p><p>This is the one designers should care aboutÂ most.</p><p>A <a href=""https://arxiv.org/html/2509.10652v1"">study of 20 UX professionals</a> published in 2025 found that vibe coding fundamentally changed how designers test ideas. Instead of spending days building a static mockup in Figma, they were generating working prototypes in minutesâ€Šâ€”â€Šcomplete with real interactions, real data flows, and real feedbackÂ loops.</p><p>One participant described it as overcoming <a href=""https://arxiv.org/html/2509.10652v1"">â€œwhiteboard fearâ€</a>â€Šâ€”â€Šthe paralysis of starting from a blank canvas. AI gave them a starting point they could refine instead of a void they had toÂ fill.</p><p>Product designer Kshitij Agrawal summarised the shift perfectly: <a href=""https://www.uxtigers.com/post/vibe-coding-vibe-design"">â€œMockups existed because coding was difficult. Now that coding is easy, the future is prototypes.â€</a></p><p>Think about what that means for UX research. Instead of testing a static Figma flow and asking users to imagine how it would work, you can test <em>the actual thing</em>. The feedback is richer. The iteration is faster. The gap between what you designed and what users experience collapses.</p><p><strong>3. The designer-developer wall dissolves.</strong></p><p>Smashing Magazine described the new paradigm: designers are becoming <a href=""https://www.smashingmagazine.com/2025/09/intent-prototyping-pure-vibe-coding-enterprise-ux/"">â€œdesign engineersâ€</a>, using AI tools to add not just visual styles but basic functionality to UI componentsâ€Šâ€”â€Šwithout programming skills. The deliverable is changing from a spec document to an actual workingÂ element.</p><p>For the first time, the person who understands the user can also build the thing. Thatâ€™s a powerfulÂ loop.</p><figure><img alt=""Workflow illusration"" src=""https://cdn-images-1.medium.com/max/1024/0*rpOR0_XSJxvOO1yk.png"" /></figure><h3>But hereâ€™s what happens to the user experience</h3><p>All of this speed is extraordinary. The risk isnâ€™t that the tools are bad. Itâ€™s that they make it trivially easy to build things that shouldnâ€™t exist.</p><p><strong>The pixel-perfect trap.</strong></p><p>A UI/UX designer at agency COAX described the core problem: <a href=""https://coaxsoft.com/blog/whats-wrong-with-vibe-coding"">â€œAn AI-generated form looked perfect in Figma but failed basic usability tests. It had no validation, poor error states, and confusing navigation.â€</a></p><p>This is the signature failure mode of AI-generated interfaces. They look polishedâ€Šâ€”â€Šbeautiful layouts, consistent spacing, professional typography. But theyâ€™re missing the invisible layer that makes software <em>work</em>: error handling, edge cases, loading states, accessibility, and the dozens of micro-decisions that come from actually watching someone use theÂ thing.</p><p>AI builds from patterns. It doesnâ€™t build fromÂ empathy.</p><p><strong>The SpotifyÂ problem.</strong></p><p>You don;â€™t need a history lesson to see what unconstrained feature-building does to a product. Just openÂ Spotify.</p><p>It launched as a music app. One job, done brilliantly. Then it added podcasts, audiobooks, AI playlists, AI-generated music, social features, TikTok-style video clips, and a live events marketplace. Each addition made strategic sense in isolation. Together, they turned a music player into something users increasingly describe as <a href=""https://community.spotify.com/t5/Social-Random/Spotify-s-UI-Needs-a-Redesign-It-s-Cluttered-Messy-and/td-p/6690860"">cluttered and confusing</a>.</p><p>The Spotify Community forums read like a UX case study in what happens when â€œcan we build it?â€ replaces â€œshould we?â€ One user in <a href=""https://community.spotify.com/t5/Social-Random/App-Interface-is-Entirely-Broken/td-p/6650155"">another thread</a> put it plainly: â€œI will never use the audiobooks features. I will never use the podcasts features. I only care about music. Making your app more confusing in hopes that I stumble upon an audiobook or podcast is incomprehensibly stupid.â€</p><p>Spotify added audiobooks partly because of a<a href=""https://www.howtogeek.com/apps-everyones-complaining-about-2025/""> 2022 legal settlement</a> allowed bundled services to pay lower royalties to musicians. The feature wasnâ€™t built for users. It was built for the business model. Users got a more cluttered app as a sideÂ effect.</p><figure><img alt=""Spotify over time"" src=""https://cdn-images-1.medium.com/max/1024/0*J1dvZjUVvjpu22hT.png"" /></figure><p>Spotify evolution overÂ time</p><p>This is the Evernote trajectory in real time. (Evernote pulled the same move a decade agoâ€Šâ€”â€Šexpanded from notes into business cards, presentations, work chat, and eventually <a href=""https://iamreliable.com/the-system-that-failed-evernotes-growth/"">a marketplace selling socks and backpacks</a>. Downloads <a href=""https://www.makeuseof.com/what-happened-to-evernote/"">collapsed 82%</a>. Its new owners spent their first year stripping features to return toÂ basics.)</p><figure><img alt=""Evernote downloads vs new features"" src=""https://cdn-images-1.medium.com/max/1024/0*MUjgyOiNwo2bogBH.png"" /></figure><p>And neither company even had AI coding tools. Every feature they added required real engineering time. Imagine what happens when adding a feature costsÂ nothing.</p><h3>The state of UX saw thisÂ coming</h3><p>The <a href=""https://trends.uxdesign.cc"">UX Collectiveâ€™s State of UX 2025</a> diagnosed the moment with uncomfortable clarity.</p><p>Their observation: teams are <a href=""https://trends.uxdesign.cc"">â€œoptimizing flows for clicks, not clarityâ€</a> and have <a href=""https://trends.uxdesign.cc"">â€œstopped building tools and started building engagement traps.â€</a> Designers are spending their days in stakeholder alignment meetings rather than doing design work. Products get launched because someone needs a promotion. Timelines are built around performance reviews.</p><p>They described an industry that was already drifting toward feature bloat <em>before</em> AI made building free. Add vibe coding to this culture, and the incentive to ship moreâ€Šâ€”â€Šwithout asking whether more is betterâ€Šâ€”â€Šonly intensifies.</p><p>The <a href=""https://www.nngroup.com/articles/ux-reset-2025/"">NN Groupâ€™s UX Reckoning piece</a> put the positive frame on it: this is the moment where curated taste, research-informed judgment, and critical thinking become the differentiating skills. Not because theyâ€™re trendyâ€Šâ€”â€Šbut because theyâ€™re the things AI genuinely canâ€™tÂ do.</p><p>Jakob Nielsen himself noted that by Q3 2025, <a href=""https://www.uxtigers.com/post/ux-roundup-20251222"">â€œthe era of pixel-pushing had effectively ended for commercial productionâ€</a>â€Šâ€”â€Štools like Figma AI and v0 could generate high-fidelity UI from text prompts. But he also observed that this made UX <em>more</em> important, not less: someone still needs to decide what to build, for whom, andÂ why.</p><h3>Who wins. WhoÂ loses.</h3><p>The loser is any team whose answer to â€œcan we build it?â€ is always â€œyes.â€ The Evernote trajectory awaits.</p><p>The winner is any team that uses the speed for the right thingsâ€Šâ€”â€Šrapid prototyping, internal tooling, faster research loopsâ€Šâ€”â€Šwhile maintaining the discipline to ask, for every customer-facing feature: <em>does this respect the userâ€™s attention?</em></p><p>The <a href=""https://arxiv.org/html/2509.10652v1"">academic study of UX professionals</a> found exactly this tension. They called it the gap between â€œintending the right designâ€ (using AI to build faster) and â€œdesigning the right intentionâ€ (knowing what to build in the first place). Speed only helps if youâ€™ve already answered the second question.</p><p>Thereâ€™s an old product design saying thatâ€™s about to become the most important sentence in every teamâ€™s vocabulary: <strong>the best feature is the one you decide not toÂ build.</strong></p><h3>The opportunity</h3><p>Product management and UX design have always had an awkward relationship with engineering constraints. Those constraints were frustrating. They were also making youÂ better.</p><p>Now the constraint is gone. So the designer, the PM, the researcherâ€Šâ€”â€Šthey need to become the new constraint.</p><p>Not by slowing things down. By using the speed for prototyping, testing, and internal toolsâ€Šâ€”â€Šand holding the line on what ships toÂ users.</p><p>For the first time, the person closest to the user can also build the prototype, test it with real people, and iterate in hours instead of weeks. The designer doesnâ€™t need to wait for engineering. The PM doesnâ€™t need to write a spec and hope it gets interpreted correctly. The researcher can go from insight to testable prototype in an afternoon.</p><p>This is a great opportunity, but only if the industry resists the temptation to treat speed as the goal rather than the tool. <strong>When building is cheap, restraint becomes the most valuable thing a team can practise.</strong></p><p>The teams that figure this out will build the next generation of things people actuallyÂ love.</p><p>The rest will spend the next decade undoing what they built thisÂ year.</p><p>Thanks for reading Built for People! Subscribe for free to receive new posts and support myÂ work.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6c8a0059ed0e"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/when-anyone-can-build-anything-6c8a0059ed0e"">When building is free, whatâ€™s worth building?</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/the-design-failures-of-consumer-iot-9286228c47ed?source=rss----138adf9c44c---4,1772043096,The design failures of consumer IoT,"The design failures of consumer IoT

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/the-design-failures-of-consumer-iot-9286228c47ed?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/2100/1*AaucrSJHcbOmOAVGCIvKVg.png"" width=""2100"" /></a></p><p class=""medium-feed-snippet"">And how connectivity can create dependency instead of empowerment</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/the-design-failures-of-consumer-iot-9286228c47ed?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/ai-writes-the-code-and-humans-still-write-the-rules-a2058ca0734c?source=rss----138adf9c44c---4,1772032365,AI writes the code and humans still write the rules,"AI writes the code and humans still write the rules

<h4>How Lovable, Cursor, and Bolt are rewriting who gets to build softwareâ€Šâ€”â€Šand the hidden costs nobody is talkingÂ about.</h4><figure><img alt=""An infographic mapping the AI code platform landscape in 2026, showing seven tools including Cursor, Copilot, and Claude Code orbiting a central AI model, with market stats, user types, and a note that humans remain the source of everything AI can do"" src=""https://cdn-images-1.medium.com/max/1024/1*E4ologEpmBZBJICSFT2C7w.png"" /><figcaption>Lovable hit $100M revenue in 8 months (Source: <a href=""https://www.cbinsights.com/research/report/coding-ai-market-share-december-2025/""><strong>CB Insights</strong></a>).</figcaption></figure><figure><img alt=""Four key statistics displayed in a horizontal layout: â€œ82%â€Šâ€”â€ŠDevelopers using AI tools daily,â€ â€œ41%â€Šâ€”â€ŠOf all code is AI-assisted,â€ â€œ$7.4Bâ€Šâ€”â€ŠMarket size in 2025,â€ and â€œ130+â€Šâ€”â€ŠActive platforms globally.â€"" src=""https://cdn-images-1.medium.com/max/1024/1*TvZQtM-C4oFxvhoGpQ17lQ.png"" /></figure><p>A quarter of all YC startups now ship code thatâ€™s 95% AI-written. And somewhere in Silicon Valley, a 22-year-old with no CS degree just launched an app used by 40,000 peopleâ€Šâ€”â€Šwithout writing a single line. Hereâ€™s the full, unfiltered story of whoâ€™s building the future, whatâ€™s working, whatâ€™s failing, and what it allÂ means.</p><h3>The vibe codingÂ uprising</h3><p>In February 2025, AI pioneer Andrej Karpathy coined a phrase that sent shockwaves through software communities worldwide: â€œvibe coding.â€ He described it as â€œfully giving in to the vibes, embracing exponentials, and forgetting that the code even exists.â€ At the time, many professional developers rolled their eyes. Six months later, they were using it themselves.</p><p>The term captured something genuinely unprecedented: a new generation of tools that let anyoneâ€Šâ€”â€Šdesigners, marketers, founders, studentsâ€Šâ€”â€Šdescribe an app in plain English and watch it get built in real time. No compiler knowledge. No debugging in terminals. No Stack Overflow. Just a conversation with a machine that buildsÂ things.</p><p>What happened next defied almost every prediction. Lovable, a Stockholm-based startup launched in early 2024, reached $100M in annual recurring revenue in under 8 monthsâ€Šâ€”â€Šone of the fastest ARR trajectories ever recorded in enterprise software history. <em>(Source: </em><a href=""https://www.cbinsights.com/research/report/coding-ai-market-share-december-2025/""><strong><em>CB Insights</em></strong></a><em>)</em></p><blockquote><strong>Replit</strong> went from $10M to $100M ARR in 9 months after launching its AI Agent. Cursorâ€™s parent company Anysphere crossed a $9.9 billion valuation by June 2025. The AI coding platform market was valued at $7.37 billion in 2025 alone, on track to hit nearly $24 billion byÂ 2030.</blockquote><h4>Timeline of the explosion</h4><p><em>(Source: </em><a href=""https://www.ycombinator.com/blog/""><strong><em>Y Combinator Blog</em></strong></a><em>)</em></p><blockquote><strong>Feb 2025â€Šâ€”â€ŠKarpathy invents â€œvibe codingâ€</strong> The term goes viral. Non-technical founders start flooding AI builder platforms. YC CEO Garry Tan calls it â€œthe dominant way toÂ code.â€</blockquote><blockquote><strong>March 2025â€Šâ€”â€ŠYC drops its bombshell</strong> 25% of Winter 2025 YC startups reveal 95% of their codebase is AI-generatedâ€Šâ€”â€Šby highly technical founders who could have coded it themselves. The industryÂ gasps.</blockquote><blockquote><strong>April 2025â€Šâ€”â€ŠGuardio Labs sounds the alarm</strong> Security researchers find critical â€œVibeScammingâ€ vulnerability: malicious prompt injection can trick AI builders into generating backdoors. 170 of 1,645 Lovable-created apps are found to have security vulnerabilities exposing userÂ data.</blockquote><blockquote><strong>May 2025â€Šâ€”â€ŠOpenAI acquires Windsurf for $3B</strong> The biggest signal yet that Big Tech is treating AI coding as a core battleground. Microsoft, Google, Amazon, and IBM all accelerate competing products.</blockquote><blockquote><strong>July 2025â€Šâ€”â€ŠThe METR study shocks developers</strong> A rigorous independent study finds experienced developers using AI tools took 19% longer to complete complex tasksâ€Šâ€”â€Šdespite believing they were 20% faster. The perception-reality gap isÂ stark.</blockquote><blockquote><strong>Dec 2025â€Šâ€”â€ŠThe $4B market crystallizes</strong> CB Insights confirms three playersâ€Šâ€”â€ŠGitHub Copilot, Claude Code, and Anysphereâ€Šâ€”â€Šnow hold 70%+ of the coding AI agent market. All have crossed $1BÂ ARR.</blockquote><h3>Meet the platforms: A fieldÂ guide</h3><p>There are now over 130 active players in the AI coding platform space. But the real action clusters around a handful of names that keep appearing in developer Slack channels, Reddit threads, and YC pitch decks. Here is what they actually are, who built them, and what their real numbers lookÂ like:</p><h4><strong>Lovable</strong>â€Šâ€”â€ŠNo-codeÂ king</h4><p>Stockholm-born. Built for non-technical founders. Describe your app in plain English; Lovable builds a full-stack React + Supabase app with shareable URL, GitHub sync, and one-click deploy. The go-to for solopreneurs validating startup ideas. The fastest-growing platform in the space. <br /><strong>$200M ARR</strong>â€Šâ€”â€Štargeting $1B by summerÂ 2026</p><h4><strong>Cursor</strong>â€Šâ€”â€ŠDev powerÂ tool</h4><p>Made by Anysphere. VS Code-based AI code editorâ€Šâ€”â€Šthe choice for professional developers. Supports Claude, GPT-4, and other models. Produces production-grade code with proper architecture. Required technical knowledge but unmatched depth. Valued at $9.9B. <br /><strong>$500M+ ARR</strong>â€Šâ€”â€Š2 years to getÂ there</p><h4><strong>Bolt.new</strong>â€Šâ€”â€ŠSpeedÂ demon</h4><p>Browser-based. Zero setup. Type what you want, get a shareable URL in under 30 minutesâ€Šâ€”â€Šthe fastest time-to-demo of any platform. Built on StackBlitz. Powered by Supabase backend. Best for hackathon prototypes and quick experiments, but code quality degrades over iterations.<br /><strong>Fast-growing</strong>â€Šâ€”â€Š$100M+ ARR threshold crossed</p><h4><strong>Replit</strong>â€Šâ€”â€ŠFull environment</h4><p>The cloud IDE with an embedded AI Agent. Educator-favored. Strong for collaborative team environments. Built-in databases, deployment, and real-time multiplayer coding. Most powerful â€œmiddle groundâ€ platformâ€Šâ€”â€Štoo complex for pure beginners but excellent for learning developers and startup teams.<br /><strong>$10M â†’ $100M ARR</strong>â€Šâ€”â€Šin under 9Â months</p><h4><strong>GitHub Copilot</strong>â€Šâ€”â€ŠEnterprise standard</h4><p>Microsoftâ€™s flagship. Integrates into VS Code, JetBrains, and more. The largest user base of any AI coding tool. 82% of developers surveyed use itâ€Šâ€”â€Šthe default entry point for enterprise teams. Code completion and inline suggestions rather than full app generation. $10â€“39/month.<br /><strong>$1B+ ARR</strong>â€Šâ€”â€Šmarket leader by userÂ count</p><h4><strong>Claude Code</strong>â€Šâ€”â€ŠTerminalÂ agent</h4><p>Anthropicâ€™s command-line AI engineer. Deep code understanding, multi-file reasoning, and agentic task execution. CB Insights named it the â€œrunaway market leaderâ€ among LLM providers for pure coding use cases. For developers who want maximum intelligence with full control. Crossed $1B ARR.<br /><strong>$1B+ ARR</strong>â€Šâ€”â€Štop 3 marketÂ position</p><h4><strong>Windsurf</strong>â€Šâ€”â€ŠAcquired byÂ OpenAI</h4><p>Formerly Codeium. IDE-based with deep code understanding and agentic â€œflowâ€ mode. Highest code quality scores in independent benchmarks (8.5/10). Acquired by OpenAI for $3 billion in May 2025â€Šâ€”â€Šthe biggest statement yet about where AI coding is heading.<br /><strong>$3B acquisition</strong>â€Šâ€”â€Šby OpenAI Â· MayÂ 2025</p><h4><strong>v0 by Vercel</strong>â€Šâ€”â€ŠUI specialist</h4><p>Vercelâ€™s AI-powered UI generator. Highest quality score in benchmarks (9/10). Specializes in component and UI generation using React + shadcn/ui. Not a full app builder but the best tool if you want beautiful, production-quality front-end components fast. Free tier available.<br /><strong>Free + pro tiers</strong>â€Šâ€”â€Špart of Vercel ecosystem</p><blockquote>â€œA year ago, they would have built their product from scratch. <br />But now 95% of it is built by an AI. You donâ€™t need a team of 50 <br />engineers anymore. Companies are reaching $10 million in revenue with <br />teams of less than 10.â€ - Garry Tan, CEO, Y Combinator, MarchÂ 2025</blockquote><h3>Who is actually using theseÂ tools?</h3><p>The data paints a picture that surprises most people. This isnâ€™t just developers upgrading their workflow. The user base of AI coding platforms is fundamentally different from what the software world has ever seenÂ before.</p><figure><img alt=""Developer adoption by segmentâ€Šâ€”â€Š2025 SegmentAdoptionEnterprise devs97%Full-stack devs90%Startup founders85%Frontend devs82%Non-technical47%Backend devs35%"" src=""https://cdn-images-1.medium.com/max/1024/1*wf-P4Ygqa9AEq5huJcdSdQ.png"" /><figcaption>(Source: <a href=""https://survey.stackoverflow.co/2025/ai/"">Stack Overflow Developer SurveyÂ 2025</a>)</figcaption></figure><p>Three distinct tribes are using these tools, each for very different reasons:</p><h4>Tribe 1: The non-technical founder</h4><p>The biggest story of the AI coding era isnâ€™t that developers got fasterâ€Šâ€”â€Šitâ€™s that the definition of â€œdeveloperâ€ has exploded. 47% of people now applying AI to coding do so for work or school, and 41% use it for personal projects, according to Menlo Venturesâ€™ 2025 State of Consumer AI report. These are designers, product managers, marketers, and domain experts who previously needed to hire engineers to build their ideas. With Lovable or Bolt, they donâ€™tÂ anymore.</p><p>Gartner predicts 70% of new applications will be built outside traditional IT departments by end of 2025. That is happening. Real estate agents are building property management tools. Teachers are building grading apps. Small business owners are building custom CRMs. The phrase you see repeated constantly in community forums:</p><blockquote>â€œLovable figured out what to build when I couldnâ€™t.â€</blockquote><h4>Tribe 2: The technical StartupÂ Founder</h4><p>This is the group that surprised everyoneâ€Šâ€”â€Šincluding YC. These arenâ€™t non-technical people using AI because they have no choice. Theyâ€™re highly skilled engineers using AI because itâ€™s <em>dramatically faster</em>. The YC Winter 2025 data is stunning: 25% of startups with 95% AI-generated code were <strong>fully capable</strong> of writing that code manually. They chose not to. This cohort grew at 10% per week collectivelyâ€Šâ€”â€Šthe fastest-growing YC batch ever recorded. Companies in this wave are reaching $10M revenue with teams of fewer than 10Â people.</p><h4>Tribe 3: The enterprise developer</h4><p>The 63% of the market held by large enterprises is quieter but massive. GitHub Copilot integration into existing IDE workflows. Amazon CodeWhisperer for AWS shops. Internal deployment of fine-tuned models for regulated industries. These teams arenâ€™t vibe codingâ€Šâ€”â€Štheyâ€™re using AI for code review, documentation, test generation, and completing specific functions. 61% of mid-to-large U.S. software enterprises have integrated AI pair-programming into their CI/CD pipelines as ofÂ 2025.</p><h4>Sector distributionâ€Šâ€”â€Šwho usesÂ it</h4><ul><li><strong>Saas products</strong>â€Šâ€”â€Š34%</li><li><strong>Fintech</strong>â€Šâ€”â€Š21%</li><li><strong>Healthtech</strong>â€Šâ€”â€Š15%</li><li><strong>E-commerce</strong>â€Šâ€”â€Š13%</li><li><strong>Other (Education / Government / Misc.)</strong>â€Šâ€”â€Š17%</li></ul><h3>The real problems nobody talksÂ about</h3><p>Every platform announcement shows beautiful demos. Every founder testimonial is glowing. But spend enough time in the trenchesâ€Šâ€”â€Šbuilding 47 applications across these tools as one reviewer didâ€Šâ€”â€Šand a very different picture emerges. Here are the seven critical issues users face globally, backed by real research:</p><h4><strong>01<br />SecurityÂ holes</strong></h4><p>A May 2025 study found 170 of 1,645 Lovable-built apps exposed personal user data. Guardio Labs discovered â€œVibeScammingâ€â€Šâ€”â€Šattackers can inject malicious prompts that cause AI to generate backdoors. No platform auto-audits for security. Human review is non-negotiable for production apps.</p><h4><strong>02<br />Code degrades overÂ time</strong></h4><p>Multiple independent studies confirm: the 50th prompt produces measurably worse code than the 5th. As projects grow, AI-generated code becomes inconsistent and difficult to maintain. The context window fills up, patterns break down, and the codebase becomes something â€œnobody fully understands.â€</p><h4><strong>03<br />The perception gap</strong></h4><p>The July 2025 METR study stunned the industry: experienced developers using AI tools took 19% longer on complex tasks despite feeling 20% faster. The illusion of speed is real. Code appears fastâ€Šâ€”â€Šbut debugging AI output, understanding what was generated, and fixing mistakes eats the timeÂ saved.</p><h4><strong>04<br />Runway tokenÂ costs</strong></h4><p>Consumption-based pricing is wildly unpredictable during iterative debugging. Bolt.new users have burned 2+ million tokens fixing a single bug. Some spent over $1,000 on a single project. Replit Agent credit depletion during active iteration regularly shocks users. Thereâ€™s no costÂ ceiling.</p><h4><strong>05<br />Compliance blindÂ spots</strong></h4><p>For regulated industriesâ€Šâ€”â€Šfinance, healthcare, governmentâ€Šâ€”â€Šnone of these platforms meet production compliance requirements. No SOC2. No HIPAA-ready default configurations. No audit trails. All platforms are explicitly prototyping tools. Trying to ship to production without human review in these sectors is genuinely dangerous.</p><h4><strong>06<br />VendorÂ lock-in</strong></h4><p>Bolt.new and Replit make migration painful. Your project becomes deeply tied to their infrastructure. v0 and Lovable are easiest to migrate away from. For startups choosing these platforms, the exit strategy is rarely considered upfrontâ€Šâ€”â€Šand becomes a serious problem atÂ scale.</p><h4><strong>07<br />The almost â€œ right â€œ frustration</strong></h4><p>Stack Overflowâ€™s 2025 survey found that 66% of developersâ€™ biggest AI frustration is â€œsolutions that are almost right but not quite.â€ This near-miss problem is uniquely maddening: the AI produces something that looks correct, passes casual inspection, but breaks at the edges. Debugging this is a new cognitive skill entirely.</p><h4><strong>08<br />Technical debit explosion</strong></h4><p>AI-assisted coding leads to 4x more code cloning, increasing maintenance effort over time. Googleâ€™s 2024 DORA report found AI use caused a 7.2% drop in delivery stability. 62.4% of developers cite technical debt as their top frustrationâ€Šâ€”â€Šand AI is accelerating its accumulation for teams without strong review practices.</p><h4><strong>09<br />No real production-ready code</strong></h4><p>Every benchmark study reaches the same conclusion: no AI coding platform produces code you can ship to production without significant manual finishing. The speed gains are genuine for prototypes. But the quality gap is equally real for anything that needs to last beyond aÂ demo.</p><h3>AI coding vs. Manual coding: The real comparison</h3><p>The debate â€œAI vs. human codingâ€ misframes the reality. The better question is: for what tasks, for what users, at what stage does AI coding deliver genuine valueâ€Šâ€”â€Šand where does it create hiddenÂ costs?</p><h4>Time to first prototype</h4><p><strong>AI Coding platforms</strong><br />Applications can be generated in 28â€“45 minutes. These tools provide dramatic gains for non-technical founders and small teams seeking rapid validation.</p><p><strong>Manual human coding</strong><br />Initial builds typically take hours to days depending on architecture, complexity, and stack decisions.</p><p><strong>Assessment:</strong> AI platforms provide a significant advantage in early-stage prototyping and rapid iteration.</p><h4>Code quality (Simple applications)</h4><p><strong>AI coding platforms</strong><br />Generally sufficient for MVPs and demos. Generated code is functional and often built on consistent starterÂ stacks.</p><p><strong>Manual human coding</strong><br />Quality depends entirely on developer skill level. Strong engineers can produce clean, extensible foundations from theÂ start.</p><p><strong>Assessment:</strong> For straightforward applications, AI-generated code is often adequate.</p><h4>Code quality (ComplexÂ systems)</h4><p><strong>AI coding platforms</strong><br />Code quality degrades over time. As projects expand, inconsistencies increase, architectural patterns fragment, and maintainability becomes challenging.</p><p><strong>Manual human coding</strong><br />Architectural integrity is intentionally designed and maintained. Technical decisions are deliberate and aligned with long-term systemÂ goals.</p><p><strong>Assessment:</strong> For complex or long-lived systems, manual engineering maintains structural coherence more effectively.</p><h4>Security</h4><p><strong>AI coding platforms</strong><br />Elevated vulnerability risk without manual auditing. Prompt injection risks and insecure defaults require active oversight.</p><p><strong>Manual human coding</strong><br />Security patterns are applied intentionally through training, experience, and established review processes.</p><p><strong>Assessment:</strong> Production-grade security still requires human review regardless of AI assistance.</p><h4>Speed (Senior developers)</h4><p><strong>AI coding platforms</strong><br />Studies indicate experienced developers may take longer on complex tasks when using AI tools, despite a perception of increased speed.</p><p><strong>Manual human coding</strong><br />Expert engineers often move faster on nuanced architectural and system-level decisions.</p><p><strong>Assessment:</strong> AI accelerates repetitive tasks but does not consistently outperform experienced developers on sophisticated work.</p><h4>Speed (Junior developers and non-technical users)</h4><p><strong>AI coding platforms</strong><br />Documented productivity gains of approximately 26% in certain studies. Enables non-coders to build functional applications.</p><p><strong>Manual human coding</strong><br />Requires months or years of training to achieve similarÂ output.</p><p><strong>Assessment:</strong> AI significantly lowers the barrier toÂ entry.</p><h4>Cost</h4><p><strong>AI coding platforms</strong><br />Subscription pricing combined with unpredictable token consumption. Costs can spike significantly during debugging cycles.</p><p><strong>Manual human coding</strong><br />Fixed salary costs. High per-hour expense but predictable for planning purposes.</p><p><strong>Assessment:</strong> AI lowers upfront cost but introduces variability.</p><h4>Startup viability</h4><p><strong>AI coding platforms</strong><br />Small teams can achieve meaningful revenue milestones with minimal engineering headcount. Rapid iteration supports aggressive growth experimentation.</p><p><strong>Manual human coding</strong><br />Requires larger teams to reach similar output velocity.</p><p><strong>Assessment:</strong> AI creates leverage in early-stage startups.</p><h4>Scalability</h4><p><strong>AI coding platforms</strong><br />Production systems often require significant refactoring. AI-generated code rarely scales without humanÂ rework.</p><p><strong>Manual human coding</strong><br />Systems can be designed for scale from inception when architected by experienced engineers.</p><p><strong>Assessment:</strong> Scalability depends heavily on human oversight.</p><h4>Learning and understanding</h4><p><strong>AI coding platforms</strong><br />Risk of using code that developers do not fully understand. May create â€œblack boxâ€ dependencies.</p><p><strong>Manual human coding</strong><br />Deep understanding compounds over time. Knowledge of system components strengthens architectural decision-making.</p><p><strong>Assessment:</strong> Long-term technical mastery requires active engagement beyond generated output.</p><p>AI coding platforms are powerful acceleration tools. They are highly effective for prototyping, validation, and enabling non-technical creators. However, long-term system stability, security, architectural integrity, and scalability still rely heavily on human engineering judgment.</p><p>The strategic question is not whether AI replaces developers. It is where AI meaningfully augments themâ€Šâ€”â€Šand where human expertise remains indispensable.</p><blockquote>The verdict isnâ€™t simple. â€œAI tools are genuinely transformative for prototyping, validation, and non-technical buildersâ€. Theyâ€™re genuinely problematic for production systems without human oversight. The â€œrightâ€ answer depends entirely on your position in the product lifecycle and your risk tolerance.</blockquote><h3>Where the money is: MarketÂ data</h3><figure><img alt=""Bar chart titled â€œPlatform Revenue Raceâ€Šâ€”â€Š2025 ARR Estimates (CB Insights)â€ showing GitHub Copilot and Claude Code at $1B+, Cursor at $500M+, Lovable at $200M, Replit at $100M, and Bolt.new at $100M+.Market Data"" src=""https://cdn-images-1.medium.com/max/1024/1*IuBZj12TfpG3JwVESJ4Geg.png"" /></figure><p>The total AI coding market is on a trajectory that analysts estimate will reach $99 billion by 2034. The combined equity funding raised by players in this space already exceeded $5.2 billion in 2025 aloneâ€Šâ€”â€Šmore than double the $2 billion raised the year before. Over 60 mergers, acquisitions, and partnerships occurred in the AI developer tools ecosystem in 2024â€“2025. This is not a niche software category anymore. It is a platform war <em>(Source: </em><a href=""https://www.cbinsights.com/research/ai-software-development-market-map/""><strong><em>CB Insights MarketÂ Map</em></strong></a><em>)</em></p><p>Geography matters too: North America holds 43% of the current market but Asia-Pacific is the fastest-growing region at a 27.4% CAGR. India, particularly, is seeing explosive growth in AI-assisted development across both enterprise adoption and startup formation. The IT and telecommunications sector leads usage at 29.4% of the marketâ€Šâ€”â€Šbut banking and financial services (BFSI) is the fastest-growing vertical at 28.13%Â CAGR.</p><h3>The human truth at theÂ core</h3><p>Here is the thing that gets buried under all the revenue numbers and platform wars: every AI coding platform in existence was built by humans. Trained by humans. Fed data by humans. Evaluated by humans. Corrected by humans. The â€œintelligenceâ€ that pops out a React app from a text prompt is the crystallized output of millions of hours of human code written by developers over decades, scraped from GitHub repositories, Stack Overflow answers, documentation pages, and technical blogs.</p><p>This isnâ€™t a criticismâ€Šâ€”â€Šitâ€™s a clarification. The AI doesnâ€™t â€œknowâ€ how to build software the way a senior engineer knows. It pattern-matches. It predicts what code should follow the tokens you gave it, based on what it saw in training.</p><p>That is why it fails in ways that feel eerily specific: it produces code that looks correct but has subtle logical errors. It follows conventions without understanding the reasoning behind them. It solves problems itâ€™s seen before brilliantly; it stumbles on genuinely novel architectural challenges.</p><figure><img alt=""Dark-themed code snippet explaining how AI generates code: tokenize prompt, attend to context, predict next tokens, repeatâ€Šâ€”â€Šwhile noting it doesnâ€™t truly understand business logic; ends with returning â€œcode that looks rightâ€ plus hidden complexity.71% of developers say they do not merge AI-generated code without manual review."" src=""https://cdn-images-1.medium.com/max/1024/1*yie8x0iI5YhOxWPXYB1Khg.png"" /></figure><p>This is why the most dangerous users of AI coding platforms are the ones who trust them completely. The safest users are those who treat AI output the way a senior developer treats a junior developerâ€™s pull request: read it carefully, understand it fully, question its assumptions, test its edge cases, and only then mergeÂ it.</p><p><strong>71% of developers say they do not merge AI-generated code without manual review. </strong>That number should be 100%. The 29% who skip review are accumulating technical debt faster than they realize, in a codebase increasingly full of code whose logic they donâ€™t fully understand.</p><p>The brilliant insight buried in the METR study is that AI doesnâ€™t make developers faster across the boardâ€Šâ€”â€Šit makes <em>less-experienced</em> developers faster, while potentially slowing down senior engineers on complex problems. This suggests the technology is doing something more interesting than pure acceleration: it is compressing the skillÂ gap.</p><p>A junior developer with <strong>AI tools can produce work that once required mid-level experience.</strong> A non-technical founder can now build what once required a small engineering team. That democratization is real and meaningful. But it comes with the risk of deploying systems built without the depth of understanding that comes from having built them the hardÂ way.</p><h3>Are people actually launching real products?</h3><p>Short answer: yes, but with important asterisks.</p><p>YCâ€™s Winter 2025 data is the most compelling evidence. Companies in that cohort growing 10% per week with sub-10-person teams and genuine customers are not demos or experimentsâ€Šâ€”â€Štheyâ€™re real businesses. YC CEO Garry Tan pointed to startups reaching $10M in revenue with fewer than 10 people. That would have been structurally impossible inÂ 2019.</p><p>The common pattern in communities like Reddit and Indie Hackers follows a â€œgraduate workflowâ€: prototype fast in Lovable or Bolt, validate with real users quickly, then either rebuild properly in Cursor or traditional development once the idea is provenâ€Šâ€”â€Šor hire an engineer to clean up the codebase for production. Start vibe, graduate toÂ code.</p><p>Lovable projects $1 billion in ARR by summer 2026â€Šâ€”â€Šfive times its current $200M run rate. That would make it one of the fastest-scaling B2C software products in history. Whether that reflects genuine product launches or churning users experimenting with prototypes is the key question. The honest answer: both. High-churn is a documented concern across all these platforms, precisely because the barrier to trying is so low that many users spin up projects they neverÂ finish.</p><p>The sectors where AI coding platforms are generating real-world product launches break down clearly: SaaS tools (34%), fintech applications (21%), healthtech prototypes (15%), and e-commerce storefronts (13%). The fintech and healthtech numbers are notable precisely because these are <em>regulated industries</em> where AI-generated code requires the most careful human review before shippingâ€Šâ€”â€Šand where the security vulnerabilities documented in 2025 represent the highest actualÂ risk.</p><h3>Final word</h3><p>So what does it allÂ mean?</p><p>We are in the middle of a genuine platform shiftâ€Šâ€”â€Šone that happens once or twice per generation in software. The printing press didnâ€™t replace writers; it made writing accessible to millions more people and changed what â€œwritingâ€ meant forever. The spreadsheet didnâ€™t replace accountants; it changed what accountants spent their time doing. AI coding platforms are doing something similar to software development.</p><p>The question is not whether to use these tools. <strong>82% of developers already do, daily.</strong> The question is whether youâ€™re using them with eyes openâ€Šâ€”â€Šunderstanding what they actually are (pattern-matching engines trained on human knowledge), what theyâ€™re good at (speed, scaffolding, prototyping, compressing the skill gap), and what theyâ€™re bad at (security, novel architecture, complex production systems).</p><h4>The realÂ win</h4><p>AI coding tools have genuinely democratized software creation. Millions of people who could not previously build software now can. That is a profound and real expansion of human capabilityâ€Šâ€”â€ŠnotÂ hype.</p><h4>The realÂ risk</h4><p>Production systems built with AI and not reviewed by experienced engineers are accumulating vulnerabilities, technical debt, and architectural problems that will surface at scale. The bill will comeÂ due.</p><h4>The uncomfortable truth</h4><p>AI doesnâ€™t know what it doesnâ€™t know. It generates plausible code, not correct code. The difference only becomes visible when the system fails under real-world conditions that the training data didnâ€™t anticipate.</p><h4>The biggerÂ picture</h4><p>Every model was built on human knowledge. AI is not a new form of intelligenceâ€Šâ€”â€Šitâ€™s the stored pattern of human intelligence, made instantly accessible. The humans who fed it that knowledge are still the source of everything it canÂ do.</p><p>The most honest summary: AI coding platforms are the best prototyping tools ever built and potentially dangerous production deployment tools if used without human oversight. Use them to go from zero to validated idea in a weekend. Use experienced engineers to take it from there. The future belongs to builders who understand both sides of that lineâ€Šâ€”â€Šand know precisely when theyâ€™re on the wrongÂ one.</p><blockquote>The code revolution isnâ€™t coming. Itâ€™s here. The question is whether youâ€™re building with itâ€Šâ€”â€Šor being swept along byÂ it.</blockquote><p><strong>References:</strong></p><p>[1] CB Insightsâ€Šâ€”â€ŠCoding AI market share december 2025 â†’ <a href=""https://www.cbinsights.com/research/report/coding-ai-market-share-december-2025/"">https://www.cbinsights.com/research/report/coding-ai-market-share-december-2025/</a></p><p>[2] Stack overflow developer survey 2025â€Šâ€”â€ŠAI section â†’ <a href=""https://survey.stackoverflow.co/2025/ai/"">https://survey.stackoverflow.co/2025/ai/</a></p><p>[3] METRâ€Šâ€”â€ŠMeasuring impact of early 2025 AI on developer productivity â†’ <a href=""https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/"">https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/</a></p><p>[4] Guardio labsâ€Šâ€”â€ŠVibescamming research â†’ <a href=""https://guard.io/labs/vibescamming-from-prompt-to-phish-benchmarking-popular-ai-agents-resistance-to-the-dark-side"">https://guard.io/labs/vibescamming-from-prompt-to-phish-benchmarking-popular-ai-agents-resistance-to-the-dark-side</a></p><p>[5] Y Combinator blog â†’ <a href=""https://www.ycombinator.com/blog/"">https://www.ycombinator.com/blog/</a></p><p>[6] CB Insightsâ€Šâ€”â€ŠAI Software development market map â†’ <a href=""https://www.cbinsights.com/research/ai-software-development-market-map/"">https://www.cbinsights.com/research/ai-software-development-market-map/</a></p><p>[7] Stack Overflow 2025 Surveyâ€Šâ€”â€ŠAI Tools â†’ <a href=""https://survey.stackoverflow.co/2025/ai/"">https://survey.stackoverflow.co/2025/ai/</a></p><p>[8] METR study academic paper â†’ <a href=""https://arxiv.org/abs/2507.09089"">https://arxiv.org/abs/2507.09089</a></p><p>ğŸ¬ <strong>Must Watch:</strong> â€œThe vibe coding mind virus explainedâ€ by FireshipÂ â†’</p><a href=""https://medium.com/media/0f1740eb50c77f880c4fc6a6cabc032c/href"">https://medium.com/media/0f1740eb50c77f880c4fc6a6cabc032c/href</a><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a2058ca0734c"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/ai-writes-the-code-and-humans-still-write-the-rules-a2058ca0734c"">AI writes the code and humans still write the rules</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/how-complexity-accumulates-e222182a67f5?source=rss----138adf9c44c---4,1771936391,How complexity accumulates,"How complexity accumulates

<h4>How systems become risky without anyone noticing.</h4><figure><img alt=""A Painting of Alexander Cutting the Gordian Knot"" src=""https://cdn-images-1.medium.com/max/1024/1*urJTxEUxB1exhlGRMTz9tg.jpeg"" /><figcaption>Alexander Undoing the Gordian | Source: Knot 1st-art-gallery.com</figcaption></figure><p>No one decides to build a fragile system. No executive convenes a meeting to discuss how best to make operations inscrutable, unreliable, brittle. No engineer sets out to create software that no one can maintain or discern later on. No organization deliberately designs processes so convoluted that they guarantee failure.</p><p>Yet fragile, incomprehensible, unmaintainable, and failure-prone systems are everywhere. They are the norm. Systems that excel and are resilient are exceptional. So how come? Fragile systems didnâ€™t arrive through dramatic decisions or catastrophic errors. They evolved. Their fragility and brittleness were accumulated gradually, through a thousand small, locally rational choices that collectively created something unmanageable. Therein lies the importance of Systems Thinking. To understand the aggregate dynamics of individual choices.</p><p><a href=""https://openlearning.mit.edu/news/ask-mit-professor-what-system-thinking-and-why-it-important"">Systems Thinking </a>is the best defense we have against the complexity run amuck. Complexity is an emergent property of systems. It evolves not from reckless decision-making but from responding sensibly to immediate, localized needs. A new feature here, a workaround there, an exception to handle an edge case, a patch to fix an urgent problem. Each addition seems small. Each solves a real problem. Each is approved and implemented with good intentions.</p><p>But complexity isnâ€™t an additive attribute. It multiplies.</p><figure><img alt=""A diagram that demonstrates that the complexity of a system (connections) grows exponentially with the number of components in a system"" src=""https://cdn-images-1.medium.com/max/776/0*2MfYpvTLBmGK5aYk.jpeg"" /><figcaption>Source: Author</figcaption></figure><p>Take the image above. In the abstract it does a good job of demonstrating complexity. A system of 3 nodes has just 3 unique connections. A system with 1 more node, doubles the number of connections to 6. Double that system and the number of connections grows exponentially. Somewhere along the way the system crosses a threshold where the system becomes so fraught with complexity that it creates genuine risk. Like the emergence of complexity, the probability also increases exponentially.</p><p><strong>How Complexity Accumulates</strong></p><p>The first step in managing complexity is understanding how it sneaks into systems despite everyoneâ€™s best efforts to keep thingsÂ simple.</p><p>Feature Creep</p><p>Every user wants one more feature. Every stakeholder has a use case that isnâ€™t quite covered. Every competitive analysis reveals something rivals offer that you donâ€™t. The pressure to add is constant and multifaceted.</p><p>Individual feature requests seem reasonable. A customer needs the system to handle a specific edge case. A sales prospect will sign if you just add this one capability. An internal team needs special handling for their workflow.</p><p>Saying yes to each request improves the system for someone. But each addition increases the surface area of the system, manifesting in more code to maintain, more interactions to test, more documentation to write, more training to conduct, as well as an expanded Optimal Design Domain. The complexity grows with the factorial of features.</p><p>Procedural Layering</p><p>Organizations respond to problems <a href=""https://irb.northwestern.edu/compliance-education/corrective-and-preventive-action-capa-plans.html"">by adding procedures</a>. A mistake occurs; a new approval process is implemented. An audit finds gaps; a new compliance check is required. A risk materializes; a new control is instituted.</p><p>Each procedure makes sense in isolation. We should approve major purchases. We should verify compliance. We should control risk. But procedures accumulate faster than theyâ€™re removed. Organizations rarely ask, â€œWhat procedure can we eliminate now that weâ€™re adding thisÂ one?â€</p><p>The result is sedimentary layers of process, each representing a response to some past problem, many now obsolete but all still in force because no one has authority or incentive to removeÂ them.</p><p><a href=""https://www.tocinstitute.org/theory-of-constraints.html"">The Theory of Constraints</a> (ToC) identifies this as policy constraintsâ€Šâ€”â€Šrules and procedures that become bottlenecks themselves. <a href=""https://www.tocinstitute.org/eliyahu-goldratt.html"">Goldratt </a>(the creator of ToC) observed that organizations often implement policies to optimize local efficiency but never revisit them when conditions change, creating system-level dysfunction.</p><p>Informal Complexity, WorkÂ Arounds</p><p>When formal systems donâ€™t serve user needs well, people create workarounds. They copy data manually between systems. They use spreadsheets to track what the official system should track but doesnâ€™t. They develop informal communication channels because formal ones are tooÂ slow.</p><p>In my experience as an engineer, systems designer, and consultant, this is actually the norm of how business gets done. People are very entrepreneurial by nature. Thereâ€™s never a formalized meeting to figure out how business operations ought to be completed, they just figure out what works, and that becomes the best practice. This is great for getting things done, but it also means that there is usually lots of low-hanging fruit to optimize these systems, usually at lowÂ cost.</p><p>Workarounds are innovations at the edges. They represent localized problem-solving that keeps work flowing. But theyâ€™re also a key driver of hidden complexity. The formal system looks simple on paper, but the actual operating system includes dozens of undocumented workarounds that only certain people knowÂ about.</p><p>When those people leave, their workarounds break. When systems change, workarounds that depended on specific quirks stop working. When new people join, they donâ€™t know the workarounds exist and make errors because the formal system doesnâ€™t match operational reality.</p><p>Technical Debt and The Infrastructure Accumulation</p><p>In software, technical debt is explicit and easy to detect. Shortcuts are taken to complete a project faster, leaving behind code that should be refactored later. Often, â€œlaterâ€ never comes, and the debt accumulates.</p><p>But technical debt exists in all systems, not just software. In manufacturing, we regularly see equipment thatâ€™s been patched repeatedly, or modules added onto instead of replaced. In organizational design we even see it in the form of reporting lines being added without rethinking the fundamental design. Itâ€™s the sales training program thatâ€™s been updated piecemeal for every new product instead of redesigned.</p><p>Each piece of debt makes future changes harder. The code becomes harder to modify, because the internal logic is already fractal. The manufacturing equipment becomes more fragile. The organization becomes more difficult to reorganize. The training becomes less effective. The system becomes rigid precisely when it needs to be adaptive.</p><p><strong>Complexity Increases Faster Than Our Ability to Understand It</strong></p><p>A systemâ€™s complexity doesnâ€™t scale linearly with system size. A system twice as large isnâ€™t twice as complex. Itâ€™s often four times, eight times, or exponentially moreÂ complex.</p><p>Interaction Effects</p><p>An interaction is the reciprocal cause-and-effect relationship between two components within a system. This is important to understand how systems behave as a whole. Systems are not merely the sum of their parts. It means that as systems grow their complexity is boosted by the mutual and cyclical relationships., highlighting that influences are mutual and cyclical, not linear. A system with 10 components has 45 potential interaction pairs, but a system with 100 components hasÂ 4,950.</p><p>Most interactions donâ€™t matter most of the time. But under stress, under unusual conditions, or when specific combinations occur, obscure interactions become critical. And the more interactions exist, the more likely that some will create failure modes no one anticipated. This is the principle of <a href=""https://uxdesign.cc/bad-design-is-like-a-virus-design-defects-and-latent-failures-1e0ab4be7e52"">resident pathogens</a>.</p><p>You cannot understand a system by understanding its parts in isolation. The interactions between parts often dominate behavior. As those interactions multiply, understanding the whole becomes exponentially harder.</p><p>Emergent Behaviors</p><p>Complex systems exhibit emergent behaviors. Emergent behaviors are system-level properties that donâ€™t exist in any individual component. Traffic jams emerge from individual driver decisions. Market crashes emerge from individual trading behaviors. Organizational dysfunction emerges from individual departmental optimizations. Flocking patterns of birds and Schools of Fish emerge as a result of many constituents which donâ€™t, in and of themselves, possess these emergent properties.</p><figure><img alt=""A flock of birds during a yellow sunset"" src=""https://cdn-images-1.medium.com/max/1024/0*wIMNA47WVg3BzCiR.jpg"" /><figcaption>A Flock of birds as emergent behavior found in nature | Source: Arstechnica.com</figcaption></figure><p>Often spawned by localized decision making, these emergent properties are often negative (at least from the system designerâ€™s perspective), namely because they are unintended and unexpected consequences. And theyâ€™re nearly impossible to predict because they emerge from the interaction of many factors, not from any singleÂ cause.</p><p><em>A side note/rant:</em></p><p><em>This is why Iâ€™m bearish when it comes to the future of gene editing technology. Gene editing methods like </em><a href=""https://news.stanford.edu/stories/2024/06/stanford-explainer-crispr-gene-editing-and-beyond""><em>CRISPR </em></a><em>uses correlations and probability to edit genes for some desired effect, eye color. But (1) these may correlate with other factors not accounted for (</em><a href=""https://www.the-scientist.com/how-are-earwax-and-body-odor-linked-72476""><em>like the relationship between earwax and body odor</em></a><em>) and (2) the collective editing of multiple genes can have greater unintended [read: emergent] consequences.</em></p><p>Cognitive Limits</p><p>Human working memory can hold roughly seven chunks of information. When a system has hundreds or thousands of interacting components, no individual can hold the complete system in their head. Understanding becomes distributed across many people, each of whom has a partialÂ view.</p><p>This fragmentation of understanding is itself a risk. No one sees the whole. Decisions are made based on local knowledge that doesnâ€™t account for global effects. Changes are implemented without understanding full implications. The system becomes too complex for anyone to fully reasonÂ about.</p><p><strong>Real-World Complexity Failures</strong></p><p>Healthcare.gov LaunchÂ (2013)</p><p>The Affordable Care Actâ€™s federal insurance exchange launched in October 2013 and immediately collapsed. The website couldnâ€™t handle traffic. Applications failed. Users couldnâ€™t complete enrollment.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*bbTMPytERu-EIAQH.png"" /><figcaption>The now famous error screen of HealthCare.gov | Source: Medium.com</figcaption></figure><p>The failure wasnâ€™t a single bug or bad decision. It was systemic complexity. Multiple contractors built different pieces. Systems needed to integrate with existing federal databases. State exchanges needed to interface with the federal system. Security requirements added layers. Compliance rules created conditional logic. <a href=""https://medium.com/dataseries/small-is-beautiful-the-launch-failure-of-healthcare-gov-5e60f20eb967"">Edge cases demanded special handling</a>.</p><p>Each component worked (more or less) in isolation. But integrating them revealed cascade failures, timing issues, and interaction effects no one had anticipated. The system was too complex for anyone to fully understand, and that complexity created fragility.</p><p>Knight Capital Trading LossÂ (2012)</p><p><a href=""https://www.henricodolfing.ch/en/case-study-4-the-440-million-software-error-at-knight-capital/"">Knight Capital</a> deployed new trading software to seven of eight servers. The eighth still ran old code. When trading began, the old code executed differently than the new code. Orders from the mixed system created erratic behavior that cost $440 million in 45Â minutes.</p><p>The complexity wasnâ€™t in the trading logic itself but was in the deployment process, version control, and fail-safes (or lack thereof). Each element seemed manageable. But the interaction of partial deployment, legacy code, and automated trading created a failure mode that destroyed theÂ company.</p><p>Boeing 787 Development Delays</p><p><a href=""https://www.cnn.com/2023/02/24/business/boeing-787-dreamliner-halt"">Boeingâ€™s 787 Dreamliner was years late </a>and billions over budget, largely due to complexity in managing a global supply chain with unprecedented outsourcing. Boeing delegated entire aircraft sections to suppliers, who delegated to sub-suppliers.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/780/0*K3iiLYAXxETopbEa"" /><figcaption>A Boeing Dreamliner Production Line | Source: Seattletimes.com</figcaption></figure><p>The complexity wasnâ€™t within the airplane design. It was organizational and logistical. Coordinating work across dozens of companies, ensuring interface compatibility, managing schedule dependencies, and integrating testing added complexity that multiplied with every additional partner and interface.</p><p><strong>Detecting Accumulated Complexity</strong></p><p>Complexity accumulates invisibly until it causes problems. How do you detect it before it becomes critical?</p><p>Warning Signs:</p><ol><li><strong>Lengthening cycle times</strong>: Changes that used to take days now take weeks. This often signals that complexity has increased friction.</li><li><strong>Rising error rates</strong>: More defects, more support tickets, more exceptions. Complexity creates more failureÂ modes.</li><li><strong>Knowledge silos</strong>: Only certain people can work on certain systems because theyâ€™re too complex for newcomers to learnÂ quickly.</li><li><strong>Fear of change</strong>: Teams resist modifications because theyâ€™re not confident about side effects. â€œIf itâ€™s working, donâ€™t touch itâ€ becomes theÂ mantra.</li><li><strong>Escalating maintenance costs</strong>: More time spent fixing and patching, less time spent building new capability.</li><li><strong>Integration problems</strong>: New features break existing functionality in unexpected ways.</li><li><strong>Documentation drift</strong>: Formal documentation no longer matches actual operation because the system has evolved through undocumented changes.</li></ol><p>These symptoms donâ€™t prove complexity directly, but they correlate strongly with systems that have accumulated too much ofÂ it.</p><p>Quantitative Measures:</p><p>For software systems, metrics like <a href=""https://www.geeksforgeeks.org/dsa/cyclomatic-complexity/"">cyclomatic complexity</a>, coupling metrics, and dependency graphs provide numerical complexity measures and diagnostic tools. But even non-software systems can be measured:</p><ul><li><strong>Decision tree depth</strong>: How many conditional branches exist in aÂ process?</li><li><strong>Role count</strong>: How many different roles touch a workflow?</li><li><strong>Approval layers</strong>: How many sign-offs does a decisionÂ require?</li><li><strong>Exception frequency</strong>: How often do standard processes require exceptions?</li><li><strong>Handoff count</strong>: How many times does work transfer between people orÂ systems?</li></ul><p>High values donâ€™t automatically mean thereâ€™s a problem. Complexity is sometimes necessary. But they indicate where to look for opportunities to simplify and where to triage when something breaks.</p><p><strong>Pruning Unnecessary Complexity</strong></p><p>Once youâ€™ve detected complexity, how do you reduce it without breaking things thatÂ work?</p><p>1. Dependency Mapping</p><p>You canâ€™t simplify what you donâ€™t understand. Create visual maps of dependencies:</p><ul><li>What depends onÂ what?</li><li>What components interact?</li><li>What can be changed independently?</li><li>Where are the tight couplings?</li></ul><figure><img alt=""An example of a dependency map"" src=""https://cdn-images-1.medium.com/max/1024/1*ZidzvGzhvpLFVASu2l9qtQ.png"" /><figcaption>An example of a dependency map | Source: dependency-map.com</figcaption></figure><p>Tools exist for software (dependency analyzers, architecture visualization). For organizational systems, this might be process maps, responsibility matrices (RACI), or workflow diagrams.</p><p>2. The 80/20Â Analysis</p><p>Most systems exhibit <a href=""https://www.datacamp.com/tutorial/pareto-distribution"">Pareto distributions</a>: 20% of features deliver 80% of value, 20% of code contains 80% of bugs, 20% of procedures handle 80% ofÂ cases.</p><figure><img alt=""An Example of a Pareto Distribution"" src=""https://cdn-images-1.medium.com/max/1024/0*hKMuHOxk7ZxHLk1Z"" /><figcaption>An Example of a Pareto Distribution | Source: scirp.org</figcaption></figure><p>Identify:</p><ul><li>Which features are rarelyÂ used?</li><li>Which procedures handle edgeÂ cases?</li><li>Which code paths are seldom executed?</li></ul><p>These low-value, high-maintenance components are prime candidates for elimination. Removing them reduces surface area without significantly reducing capability.</p><p><a href=""https://www.lightsondata.com/why-focus-reduce-variation/"">Demingâ€™s focus on variation reduction</a> is relevant here. Simplification reduces sources of variation, making systems more stable and predictable. Eliminating rarely-used features eliminates rare but costly failureÂ modes.</p><p>3. Complexity Budgets</p><p>Treat complexity as a constrained resource, like memory or budget. Every addition must fit within the budget, which means something else might need to beÂ removed.</p><p>This forces explicit trade-offs: â€œTo add this feature, we need to remove three existing ones. Which should go?â€ The question surfaces costs that are otherwise hidden.</p><p><a href=""https://medium.com/@ankitsingh1583/how-netflix-uber-and-amazon-manage-their-microservices-at-scale-97b028da2134"">Netflix famously has a policy limiting microservices complexity</a>: teams can add new services, but the total count must stay within bounds, forcing consolidation and simplification as a regular practice.</p><p>4. Simplification Audits</p><p>Regularly review systems specifically to identify simplification opportunities:</p><ul><li>Which procedures exist because of problems that no longerÂ occur?</li><li>Which features could be consolidated?</li><li>Which integrations could be eliminated?</li><li>Which exceptions could be standardized?</li></ul><p>The <a href=""https://www.tocinstitute.org/theory-of-constraints.html"">Theory of Constraints</a> teaches to focus improvement efforts on constraints. But TOC also recognizes that non-constraint resources shouldnâ€™t be optimized to full capacity. Similarly, not every part of a system needs maximum capability. Some parts can and should be simplified, even if it means slightly reduced local performance, if it improves overall system manageability.</p><p>5. Modular Decomposition</p><p>Break complex systems into loosely <a href=""https://aisel.aisnet.org/ecis2022_rp/52/"">coupled modules </a>with clear interfaces. This doesnâ€™t reduce total complexity, but it contains and partitions it.</p><p>A monolithic system with 1,000 interconnected parts is unmanageable. Ten modules of 100 parts each, with well-defined interfaces between modules, is manageable. You can understand one module deeply without needing to understand allÂ modules.</p><p>This requires discipline in interface design: modules should interact through narrow, well-specified interfaces, not through deep coupling or shared state. When modularity is maintained, complexity within modules stays local and doesnâ€™t ripple system-wide.</p><p>6. Standardization and Platforming</p><p>Reduce variety by standardizing components and building on common platforms. Instead of five different authentication systems, use one. Instead of three different data formats, standardize onÂ one.</p><p>This trades flexibility for simplicity. You canâ€™t optimize each use case perfectly, but you reduce the number of things that need to be understood, maintained, and integrated.</p><p><a href=""https://www.6sigma.us/business-process-management-articles/process-standardization-for-operational-excellence/"">Standardization of processes is foundational for quality</a>. You canâ€™t improve what varies wildly. You canâ€™t maintain whatâ€™s different everywhere. Standardization creates the baseline from which to build, measure, andÂ improve.</p><p><strong>Culture and Resisting Complexity</strong></p><p>Technical approaches help, but the deeper challenge is cultural. Organizations must develop a <a href=""https://www.pmi.org/disciplined-agile/dealing-with-complexity-by-creating-a-bias-for-simplicity"">bias toward simplicity</a>, which cuts against many incentives.</p><p>Incentive Misalignments:</p><ul><li>Product managers are rewarded for feature additions, not featureÂ removals</li><li>Engineers are evaluated on what they build, not what they eliminate</li><li>Processes are added in response to visible problems; removing them is invisible work</li><li>Budgets reward spending, not simplification</li></ul><p>To counter these, organizations needÂ to:</p><ul><li><strong>Celebrate simplification</strong>: Publicize cases where removing features improved theÂ product</li><li><strong>Measure complexity explicitly</strong>: Track metrics like code complexity, process steps, and approvalÂ layers</li><li><strong>Require simplification alongside addition</strong>: New features must be accompanied by the removal of oldÂ features</li><li><strong>Create dedicated simplification initiatives</strong>: Not as ongoing work but as explicit projects with resources</li></ul><p>The Power ofÂ â€œNoâ€</p><p>The most important complexity control is saying no to additions. This is politically difficultâ€Šâ€”â€Ševery addition has a constituency. But the cumulative cost of saying yes too often is a system that collapses under its ownÂ weight.</p><p>Saying no requires:</p><ul><li>Clear criteria for whatâ€™s in scope and whatâ€™sÂ not</li><li>Explicit recognition that the capacity for complexity isÂ limited</li><li>Willingness to disappoint stakeholders in the service of system sustainability</li><li>Authority structures that can enforce boundaries</li></ul><p><a href=""https://www.uwyo.edu/ceps/development/awards/deming.html"">W. Edwards Deming</a>â€™s point about <a href=""https://deming.org/create-constancy-of-purpose/"">constancy of purpose</a> applies here. Without a consistent commitment to simplicity, complexity accumulates as each decision optimizes locally without considering globalÂ effects.</p><p><strong>Living with Necessary Complexity</strong></p><p>Some complexity is essential. Real-world problems are complex; solutions must match that complexity to some degree. The eliminate unnecessary complexity while managing necessary complexity effectively.</p><p><strong>Essential Complexity:</strong></p><ul><li>Business rules that reflect genuine domain complexity</li><li>Integration points that correspond to real organizational boundaries</li><li>Features that deliver significant value to significant user populations</li><li>Redundancy that provides resilience</li></ul><p>This complexity shouldÂ be:</p><ul><li>Explicit: Documented and understood</li><li>Contained: Modularized so it doesnâ€™tÂ leak</li><li>Justified: Regularly validated as still necessary</li></ul><p><strong>Accidental Complexity:</strong></p><ul><li><a href=""https://idenhaus.com/why-workarounds-are-iams-silent-killer/"">Workarounds</a> for systems that should beÂ fixed</li><li>Procedures created in response to one-timeÂ events</li><li>Features that sounded good but are rarelyÂ used</li><li>Integration patterns that evolved organically withoutÂ design</li></ul><p>This complexity should be systematically hunted and eliminated.</p><p>The distinction isnâ€™t always clear, but the question should be constantly asked: is this complexity necessary, or did it just accumulate?</p><p><strong>Conclusion: Complexity as SystemicÂ Debt</strong></p><p>Complexity is like financial debt: sometimes useful, always carrying a cost, and dangerous when it accumulates beyond your ability to serviceÂ it.</p><p>Taking on debt to invest in growth can be smart. But debt that accumulates from routine operations without producing value is insidious. It constrains future options, increases risk, and eventually demands painful restructuring.</p><p>The same is true of complexity. Some complexity enables capability. But much of how complexity propagates is just accumulation, the residue of past decisions that no one has cleaned up. It makes the system fragile, expensive to maintain, and difficult toÂ evolve.</p><p>The challenge is that complexity accumulates gradually and locally while its costs manifest globally and suddenly. Each small addition seems manageable. The cumulative effect is system failure that seems to come from nowhere but was actually built up over years of accretion.</p><p>Managing this requires:</p><ul><li><strong>Vigilance</strong>: Constantly watching for complexity creep</li><li><strong>Discipline</strong>: Resisting additions and forcing eliminations</li><li><strong>Understanding</strong>: Mapping and measuring complexity</li><li><strong>Investment</strong>: Dedicating resources to simplification</li><li><strong>Culture</strong>: Valuing simplicity as much as capability</li></ul><p>Systems should be designed for improvement, not just for operation. This means building systems that can be understood, modified, and simplified. Systems that resist simplification, where every change risks breaking something else, have accumulated too much complexity. I also mentioned this as a key principle to designing <a href=""https://uxdesign.cc/responsible-tech-principles-for-technological-use-and-development-3de9f6d8f49d"">Responsible and Humane Technology</a>.</p><p>The risk isnâ€™t that your system will fail catastrophically tomorrow. Itâ€™s that complexity accumulates silently until one day you discover your system has become so fragile, so opaque, and so expensive to maintain that itâ€™s effectively unmaintainable. By then, you have few options: live with mounting failures or undertake expensive, risky restructuring.</p><p>The solution is prevention: treat complexity as debt, recognize when youâ€™re taking it on, ensure itâ€™s justified, and regularly pay it down. Build systems that can be simplified, not just systems that work. Create cultures that celebrate subtraction as much as addition.</p><blockquote>â€œPerfection is achieved, not when there is nothing left to add, but when there is nothing left to take awayâ€â€Šâ€”â€Š<a href=""https://www.goodreads.com/author/show/1020792.Antoine_de_Saint_Exup_ry"">Antoine de Saint-ExupÃ©ry</a></blockquote><p>Because in the long run, the systems that survive arenâ€™t the ones with the most features, the most procedures, or the most components. Theyâ€™re the ones that remain understandable, maintainable, and adaptable. They are the ones that resisted the inexorable accumulation of unnecessary complexity.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e222182a67f5"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/how-complexity-accumulates-e222182a67f5"">How complexity accumulates</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/being-an-ai-native-designer-isnt-what-you-think-it-is-dae1d03e68f5?source=rss----138adf9c44c---4,1771936389,Being an AI-native designer isnâ€™t what you think it is,"Being an AI-native designer isnâ€™t what you think it is

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/being-an-ai-native-designer-isnt-what-you-think-it-is-dae1d03e68f5?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/2600/1*706xYB7lTNLPbjN39gK65w.jpeg"" width=""5192"" /></a></p><p class=""medium-feed-snippet"">What 28 design leaders have said AI-native design really is</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/being-an-ai-native-designer-isnt-what-you-think-it-is-dae1d03e68f5?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
