source,domain,url,created_utc,title,text
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/desktop-wallpaper-calendars-february-2026/,1769850000,"Short Month, Big Ideas (February 2026 Wallpapers Edition)","Short Month, Big Ideas (February 2026 Wallpapers Edition)

Letâ€™s make the most of the shortest month of the year with a new collection of desktop wallpapers that are sure to bring a smile to your face â€” and maybe spark your creativity, too. All of them were designed with love by the community for the community and can be downloaded for free. Happy February!"
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/practical-use-ai-coding-tools-responsible-developer/,1769778000,Practical Use Of AI Coding Tools For The Responsible Developer,"Practical Use Of AI Coding Tools For The Responsible Developer

AI coding tools like agents can be valuable allies in everyday development work. They help handle time-consuming grunt work, guide you through large legacy codebases, and offer low-risk ways to implement features in previously unfamiliar programming languages. Here are practical, easy-to-apply techniques to help you use these tools to improve your workflow."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/unstacking-css-stacking-contexts/,1769508000,Unstacking CSS Stacking Contexts,"Unstacking CSS Stacking Contexts

In CSS, we can create â€œstacking contextsâ€ where elements are visually placed one on top of the next in a three-dimensional sense that creates the perception of depth. Stacking contexts are incredibly useful, but theyâ€™re also widely misunderstood and often mistakenly created, leading to a slew of layout issues that can be tricky to solve."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/beyond-generative-rise-agentic-ai-user-centric-design/,1769086800,Beyond Generative: The Rise Of Agentic AI And User-Centric Design,"Beyond Generative: The Rise Of Agentic AI And User-Centric Design

Developing effective agentic AI requires a new research playbook. When systems plan, decide, and act on our behalf, UX moves beyond usability testing into the realm of trust, consent, and accountability. Victor Yocco outlines the research methods needed to design agentic AI systems responsibly."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/rethinking-pixel-perfect-web-design/,1768903200,Rethinking â€œPixel Perfectâ€ Web Design,"Rethinking â€œPixel Perfectâ€ Web Design

Amit Sheen takes a hard look at the â€œPixel Perfectâ€ legacy concept, explaining why itâ€™s failing us and redefining what â€œperfectionâ€ actually looks like in a multi-device, fluid world."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/smashing-animations-part-8-css-relative-colour/,1768384800,Smashing Animations Part 8: Theming Animations Using CSS Relative Colour,"Smashing Animations Part 8: Theming Animations Using CSS Relative Colour

CSS relative colour values are now widely supported. In this article, pioneering author and web designer [Andy Clarke](https://stuffandnonsense.co.uk/) shares practical techniques for using them to theme and animate SVG graphics."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/ux-product-designer-career-paths/,1768212000,UX And Product Designerâ€™s Career Paths In 2026,"UX And Product Designerâ€™s Career Paths In 2026

How to shape your career path for 2026, with decision trees for designers and a UX skills self-assessment matrix. The only limits for tomorrow are the doubts we have today. Brought to you by <a href=""https://smart-interface-design-patterns.com/"">Smart Interface Design Patterns</a>, a **friendly video course on UX** and design patterns by Vitaly."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/penpot-experimenting-mcp-servers-ai-powered-design-workflows/,1767859200,Penpot Is Experimenting With MCP Servers For AI-Powered Design Workflows,"Penpot Is Experimenting With MCP Servers For AI-Powered Design Workflows

[Penpot](https://penpot.app/?utm_source=SmashingMagazine&amp;utm_medium=Article&amp;utm_campaign=MCPserver) is experimenting with MCP (Model Context Protocol) servers, which could lead to designers and developers being able to perform tasks in Penpot using AI thatâ€™s able to understand and interact with Penpot design files. Daniel Schwarz explains how [Penpot MCP](https://github.com/penpot/penpot-mcp) servers work, what they could mean for creating and managing designs in Penpot, and what you can do to help shape their development."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/pivoting-career-without-starting-from-scratch/,1767780000,PivotingÂ Your Career Without Starting From Scratch,"PivotingÂ Your Career Without Starting From Scratch

Most developers spend their days fixing bugs, shipping features, and jumping into the next sprint without even thinking about it. After a while, you begin to ask yourself, â€œIs this still what I want to be doing?â€ This article looks at how you can move into a new direction in your career without starting from scratch, and how the skills you already use, like problem-solving, communication, and empathy, can open new doors."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/12/desktop-wallpaper-calendars-january-2026/,1767171600,Countdown To New Adventures (January 2026 Wallpapers Edition),"Countdown To New Adventures (January 2026 Wallpapers Edition)

Whether 2026 has already begun as youâ€™re reading this or youâ€™re still waiting for the big countdown to start, how about some new wallpapers to get your desktop ready for the new year? Weâ€™ve got you covered."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/12/how-design-for-with-deaf-people/,1767088800,How To Design For (And With) Deaf People,"How To Design For (And With) Deaf People

Practical UX guidelines to keep in mind for 466 million people who experience hearing loss. More design patterns in <a href=""https://smart-interface-design-patterns.com/"">Smart Interface Design Patterns</a>, a **friendly video course on UX** and design patterns by Vitaly."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/12/giving-users-voice-virtual-personas/,1766484000,Giving Users A Voice Through Virtual Personas,"Giving Users A Voice Through Virtual Personas

Turn scattered user research into AI-powered personas that give anyone consolidated multi-perspective feedback from a single question."
rss,uxdesign.cc,https://uxdesign.cc/phd-researchers-are-the-missing-capability-in-ux-and-ucd-teams-a1dca5f8b262?source=rss----138adf9c44c---4,1770037371,PhD researchers are the missing capability in UX and UCD teams,"PhD researchers are the missing capability in UX and UCD teams

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/phd-researchers-are-the-missing-capability-in-ux-and-ucd-teams-a1dca5f8b262?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/1870/1*KZ1GdyQcZI6Xyhse7hfRJw.png"" width=""1870"" /></a></p><p class=""medium-feed-snippet"">What organisations gain by hiring PhD researchers in user research and service design</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/phd-researchers-are-the-missing-capability-in-ux-and-ucd-teams-a1dca5f8b262?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/an-interview-is-not-a-dog-show-7cf99729cbda?source=rss----138adf9c44c---4,1770034392,An interview is not a dog show,"An interview is not a dog show

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/an-interview-is-not-a-dog-show-7cf99729cbda?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/606/1*JFWfuzMSsYF0Oy8-GIOFlQ.jpeg"" width=""606"" /></a></p><p class=""medium-feed-snippet"">How to impress the interviewer without trying (or feeling like a poodle), and get rid of the &#x201c;this job or the meat grinder&#x201d; mindset.</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/an-interview-is-not-a-dog-show-7cf99729cbda?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/can-ai-do-it-vibe-prototyping-orchestrated-user-interface-oui-c9cc862828a0?source=rss----138adf9c44c---4,1770034091,"Can AI do it, vibe prototyping, Orchestrated User Interface (OUI)","Can AI do it, vibe prototyping, Orchestrated User Interface (OUI)

<h4>Weekly curated resources for designersâ€Šâ€”â€Šthinkers andÂ makers.</h4><figure><a href=""https://uxdesign.cc/when-design-stops-asking-why-and-starts-asking-can-ai-do-it-625c9a5d9c68""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*lgUTYyUycXdM1eKp.png"" /></a></figure><p>â€œHey team, can we use AI to helpÂ here?</p><p>The question dropped into the Slack channel before the user research summary. Before the problem was clearly defined. Before anyone asked if users actually needed thisÂ feature.</p><p>Your product manager already generated three interface options in ChatGPT. Now theyâ€™re asking which one to build. Not whether to build. Not why to build.Â Which.</p><p>And when you slow the conversation down to ask those questions, youâ€™re about to discover that strategic thinking now reads as bottleneck behavior.â€</p><p><a href=""https://uxdesign.cc/when-design-stops-asking-why-and-starts-asking-can-ai-do-it-625c9a5d9c68""><strong>When design stops asking why and starts asking: can AI do it?</strong></a><strong> â†’<br /></strong>ByÂ <a href=""https://medium.com/u/bdc2b1d48910"">Dolphia</a></p><figure><a href=""https://sidebar.io/""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*FJRppAQTzGuG1pjp.png"" /></a></figure><p><a href=""https://sidebar.io/""><strong>The Sidebar Newsletter: 5 design links a day, nothing else</strong></a><strong>Â â†’</strong></p><h3>Editor picks</h3><ul><li><a href=""https://uxdesign.cc/vibe-prototyping-is-a-double-edged-sword-0e092435c07c?sk=704aa2ea873a209df321321cb91c4bd5""><strong>Vibe prototyping is a double-edged sword</strong></a><strong> â†’</strong><br />Itâ€™s so fun we forget what prototypes are for.<br />By <a href=""https://medium.com/u/9373adfc5a12"">EdÂ Orozco</a></li><li><a href=""https://uxdesign.cc/going-analog-in-2026-64a007180d4c?sk=c5830af4c9d7a6d56238dccb0e23cf0d""><strong>Going analog in 2026</strong></a><strong> â†’</strong><br />Can UX coexist with analog living?<br />By <a href=""https://medium.com/u/8ab653ea27a6"">DaleyÂ Wilhelm</a></li><li><a href=""https://uxdesign.cc/beyond-the-waterfall-state-why-missions-need-a-different-decision-making-architecture-d81fadb93106""><strong>Beyond the waterfall state</strong></a><strong> â†’</strong><br />Why missions need a different decision-making architecture.<br />By <a href=""https://medium.com/u/e1ffebbf296c"">JackÂ Strachan</a></li></ul><p><em>The UX Collective is an independent design publication that elevates unheard design voices and helps designers think more critically about theirÂ work.</em></p><figure><a href=""https://visualrambling.space/?ref=sidebar""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*eb6aGu1noq2cNXmw.png"" /></a></figure><p><a href=""https://visualrambling.space/?ref=sidebar""><strong>Visualrambling: just a place to ramble visually</strong></a><strong>Â â†’</strong></p><h3>Make meÂ think</h3><ul><li><a href=""https://randsinrepose.com/archives/sometimes-your-job-is-to-stay-the-hell-out-of-the-way/?ref=sidebar""><strong>Sometimes your job is to stay the hell out of the way</strong></a><strong> â†’</strong><br />â€I have tried and completely failed to build a Wolf-like role within two different companies. I used different approaches and different framing in each attempt, but each was a failure. Existing Wolves were, at best, distracted from their work and, at worst, left the company because they felt like Iâ€™d forced them into management.â€</li><li><a href=""https://blog.jim-nielsen.com/2026/cta-hierarchy/?ref=sidebar""><strong>CTA hierarchy in the wild</strong></a><strong> â†’</strong><br />â€œHyperlinks are subversive. Big Tech must protect themselves and their interests. But now it seems like everywhere I go, software is increasingly designed againstÂ me..â€</li><li><a href=""https://blog.avas.space/privacy-values/?ref=sidebar""><strong>Privacy is a value we can lose</strong></a><strong> â†’</strong><br />â€œSometimes, I think about the fact that society at large could just stop caring about data protection and privacy, and there goes everything that I worked towards and am passionate about. Humbling.â€</li></ul><h3>Little gems thisÂ week</h3><figure><a href=""https://uxdesign.cc/giraffe-muppet-or-human-9c5457d44a50""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*knG0atFJoy-gJeOq.png"" /></a></figure><p><a href=""https://uxdesign.cc/giraffe-muppet-or-human-9c5457d44a50""><strong>Giraffe, muppet, or human?</strong></a><strong> â†’<br /></strong>By <a href=""https://medium.com/u/3247d777c8cd"">Catherine Chu</a></p><figure><a href=""https://uxdesign.cc/the-rise-of-the-orchestrated-user-interface-oui-ac4202d1777d?sk=0302744b725be20c75a32e981891c29b""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*dAXCUJQKvfIK1Zpw.png"" /></a></figure><p><a href=""https://uxdesign.cc/the-rise-of-the-orchestrated-user-interface-oui-ac4202d1777d?sk=0302744b725be20c75a32e981891c29b""><strong>The rise of the Orchestrated User Interface (OUI)</strong></a><strong> â†’<br /></strong>By <a href=""https://medium.com/u/d3f644d932db"">DanielÂ Ruston</a></p><figure><a href=""https://uxdesign.cc/how-wrong-becomes-normal-381f5a33f8b7?sk=484fcba7a8adadf511de7d1671747328""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*XKCVDNo0lQywXyAH.png"" /></a></figure><p><a href=""https://uxdesign.cc/how-wrong-becomes-normal-381f5a33f8b7?sk=484fcba7a8adadf511de7d1671747328""><strong>The quiet normalization of dark patterns</strong></a> â†’<br />By <a href=""https://medium.com/u/46ebaf4ad998"">ElvisÂ Hsiao</a></p><h3>Tools and resources</h3><ul><li><a href=""https://uxdesign.cc/spatial-vibe-coding-prototyping-immersive-reality-with-ai-c2b99fd4cd84""><strong>AR/VR vibe coding</strong></a><strong> â†’</strong><br />Prototyping immersive reality with AI.<br />By <a href=""https://medium.com/u/150d70184ae5"">Albertmauri</a></li><li><a href=""https://uxdesign.cc/are-we-doing-ux-for-ai-the-right-way-aea01e14138e""><strong>Are we doing UX for AI the right way?</strong></a><strong> â†’</strong><br />How chatbot-first thinking makes products harder for users.<br />By <a href=""https://medium.com/u/98d6c804f89f"">Katya Korovkina</a></li><li><a href=""https://medium.com/design-bootcamp/designing-ar-that-listens-a-longer-tale-from-the-camera-front-lines-6f183f44227b""><strong>Designing AR that listens</strong></a><strong> â†’</strong><br />A longer tale from the camera front lines.<br />By <a href=""https://medium.com/u/aa68934f1a81"">Ivan Dzmitryievich</a></li></ul><h3>Support the newsletter</h3><p>If you find our content helpful, hereâ€™s how you can supportÂ us:</p><ul><li>Forward this email to a friend and invite them to <a href=""https://newsletter.uxdesign.cc/"">subscribe</a></li><li><a href=""https://uxdesigncc.medium.com/sponsor-the-ux-collective-newsletter-bf141c6284f"">Sponsor anÂ edition</a></li></ul><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c9cc862828a0"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/can-ai-do-it-vibe-prototyping-orchestrated-user-interface-oui-c9cc862828a0"">Can AI do it, vibe prototyping, Orchestrated User Interface (OUI)</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/gemini-vs-chatgpt-5-cloud-dancing-with-data-color-schemes-c32658205a65?source=rss----138adf9c44c---4,1769949085,Gemini vs ChatGPT-5: cloud dancing with data color schemes.,"Gemini vs ChatGPT-5: cloud dancing with data color schemes.

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/gemini-vs-chatgpt-5-cloud-dancing-with-data-color-schemes-c32658205a65?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/1952/1*Xj5r0mGdlP1V8_nc2jYhbw.png"" width=""1952"" /></a></p><p class=""medium-feed-snippet"">Applying the 2026 Pantone Color of the Year to data visualization.</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/gemini-vs-chatgpt-5-cloud-dancing-with-data-color-schemes-c32658205a65?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/lost-for-words-why-text-in-ai-images-still-goes-wrong-b5232c39bd11?source=rss----138adf9c44c---4,1769949004,Lost for words: why text in AI images still goes wrong,"Lost for words: why text in AI images still goes wrong

<h4>AI can conjure photorealistic faces and dreamy landscapes in seconds, but ask it to write â€œHappy Birthdayâ€ on a cake and things get weird fast. The culprit is how these models learn to read, and the solution is trickier than itÂ seems.</h4><p>Imagine youâ€™ve just prompted your chosen AI image generator to create a stunning fashion advertisement for a major platform. The colours are gorgeous, the composition is beautifully balanced, and the lighting would make any photographer jealous. Thereâ€™s just one small problem: the text reads â€œSHOP NUG.â€ Close, but notÂ quite.</p><figure><img alt=""Four identical fashion ads with increasingly garbled AI-generated text, showing errors like â€˜SHUP NOY,â€™ â€˜DISGVER YUR SFYLE,â€™ and â€˜SUMMILER COLLECIONâ€™ instead of proper words."" src=""https://cdn-images-1.medium.com/max/1024/1*JpH8f3y4pOZpYh5Mlbg1iQ.png"" /><figcaption>Image byÂ author</figcaption></figure><p>If this sounds familiar, youâ€™re certainly not alone. Despite remarkable leaps in AI-generated imagery over the past few years, text rendering remains the Achillesâ€™ heel of even the most sophisticated models. And hereâ€™s the kicker: whilst generation has been steadily improving, <em>editing</em> that garbled text after the fact is proving to be an even thornier challenge.</p><p>So, whatÂ gives?</p><h3><strong>The fundamental mismatch</strong></h3><p>The truth is, AI image generators donâ€™t actually â€œreadâ€ text. At all. When you ask <a href=""https://openai.com/index/dall-e-3/"">DALL-E</a> or <a href=""https://www.midjourney.com/"">Midjourney</a> to include words in an image, the model isnâ€™t processing language. Itâ€™s simply pattern-matching shapes it has seen before in trainingÂ data.</p><p>Traditional text-to-image models like Stable Diffusion perceive text as a collection of pixels and visual elements to composite into the scene, not as meaning-conveying strings of characters. The model has seen millions of images containing billboards, book covers, and street signs, but it was never explicitly taught what those squiggles <em>mean</em> or how they follow strict rules of spelling andÂ grammar.</p><p>This creates a fundamental mismatch. Text is precise and unforgiving: one wrong letter changes everything (â€œSTOPâ€ versus â€œST0Pâ€, anyone?). Images, by contrast, are fluid and interpretive. A face can be slightly asymmetrical and still look human. A landscape can have unusual lighting and still feel real. But misspell a single word? The illusion shatters.</p><h3><strong>How diffusion actually works (in broadÂ strokes)</strong></h3><p>To understand why text is such a headache, it helps to peek behind the curtain at how these models generate images in the firstÂ place.</p><p>Most modern AI image generators use something called <a href=""https://stable-diffusion-art.com/how-stable-diffusion-work/"">diffusion models</a>. The basic idea is surprisingly elegant. During training, the model learns to gradually add noise to images until they become pure static, then reverses this process by removing noise step-by-step to recover coherent pictures.</p><p>When you type a prompt, it gets processed through a text encoder (typically <a href=""https://openai.com/index/clip/"">CLIP</a>) that converts your words into numerical representations. These embeddings then guide the denoising process, steering the random noise towards something resembling your description.</p><figure><img alt=""Diagram showing AI image generation workflow in five steps: 1) Prompt (text input), 2) Encode (AI processes text), 3) Denoise (removes visual noise shown as dots), 4) Refine (improves image composition), 5) Output (final image). Illustrated with simple icons connected by arrows flowing left to right."" src=""https://cdn-images-1.medium.com/max/1024/1*Q2BzL8FhuLaDKns0UbFJ5g.png"" /><figcaption>Image byÂ author</figcaption></figure><p>The problem? Fine details like individual letters are treated as low-priority during the early denoising steps. Errors get â€œbaked inâ€ early and become <a href=""https://dev.to/tracywhodoesnot/why-ai-struggles-with-text-in-image-generation-n69"">incredibly difficult to correct later in the process</a>. The model is essentially guessing letter shapes with no feedback loop to verify whether its guess makes any linguistic sense.</p><p>Thereâ€™s also the tokenisation issue. When CLIP processes text, it splits words into tokens, and sometimes those splits break apart the very concepts itâ€™s trying to understand. â€œDeep focus,â€ for instance, gets tokenised into separate pieces, losing the photographic meaning entirely. The same fragmentation happens with the text <em>in</em> your images, making coherent words harder to reconstruct.</p><h3><strong>Generation is getting betterâ€Šâ€”â€Šediting, not soÂ much</strong></h3><p>Now for some good news: text generation in AI images has improved dramatically. Newer models are making genuine strides, and the gap between â€œabsolutely wrongâ€ and â€œactually usableâ€ has narrowed considerably over the past year orÂ so.</p><p>The improvement isnâ€™t accidental. Companies have specifically invested in solving this problem, recognising that text accuracy is a dealbreaker for commercial applications. After all, nobody wants to hand a client an otherwise perfect AI-generated mockup with â€œBSET PRCIESâ€ plastered acrossÂ it.</p><p><a href=""https://ideogram.ai/"">Ideogram</a>, a platform built specifically with typography in mind, is said to be achieving now roughly <a href=""https://pxz.ai/blog/ideogram-vs-midjourney-2026"">95% accuracy on text prompts</a>, a remarkable leap compared to Midjourneyâ€™s approximately 40%. The company claims its latest model reduces text error rates by <a href=""https://the-decoder.com/ideogram-1-0-outshines-midjourney-and-dall-e-3-with-impressive-text-rendering/"">nearly half compared to DALL-E 3</a>. For designers who need readable words on posters, logos, and social media graphics, this could be genuinely transformative.</p><p>But does it really stand up to scrutiny? In the first example below, I asked it to generate a bookshelf with popular titles on product design, with the text clearly visible. The results showed significant errors (â€œDonâ€™t Make Me Thinkâ€ became â€œDONâ€™T MAKE ME M THINK THINK,â€) and other titles were similarly garbled. Next, I provided a specific list of books and authors. The outcome? Still riddled with text errors, suggesting that Ideogramâ€™s touted 95% accuracy claim falls short in actual use cases that require heavy text generation. It may fare better with simpler, single-text prompts, but complex scenarios remain problematic.</p><figure><img alt=""Two AI-generated images of design book bookshelves side by side. Left image shows books with text errors including â€˜DONâ€™T MAKE ME M THINK THINKâ€™ and garbled versions of titles like â€˜The Design of Everyday Things,â€™ â€˜About Face,â€™ â€˜Lean UX,â€™ and â€˜Atomic Design.â€™ Right image shows even more severely garbled text with â€˜DO ONâ€™ OK ME THIK,â€™ â€˜ROOK LE,â€™ â€˜DEUNNEWE UMBI BOULE,â€™ and â€˜LEZCEDRâ€™ demonstrating AIâ€™s failure to render accurate book spine text."" src=""https://cdn-images-1.medium.com/max/1024/1*_ieWpXulhkEfhTa5bjnVPw.png"" /><figcaption>Image byÂ author</figcaption></figure><p><a href=""https://www.recraft.ai/blog/comparing-popular-and-high-performing-text-to-image-models-and-providers"">Recraft V3</a>, another newcomer, has also positioned itself as a leader in accurate text rendering, claiming â€œflawless results with every prompt.â€ A bold assertion, but the benchmarks suggest itâ€™s not farÂ off.</p><p>So if generation is improving, why is editing still such aÂ mess?</p><h3><strong>The short answer: context preservation</strong></h3><p>When generating an image from scratch, the model has complete creative freedom. It can place text wherever it likes, choose fonts that play nicely with its capabilities, and build the entire composition around what it knows it can renderÂ well.</p><p>Editing is a different beast entirely. When you ask an AI to fix garbled text on an existing image (through inpainting or other techniques) the model must preserve everything <em>around</em> the text whilst only modifying the specific region youâ€™ve highlighted. This is where things get complicated.</p><p><a href=""https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1614608/full"">Research on latent diffusion models for inpainting</a> shows that when applied to large or irregular regions, these models â€œtend to generate blurry or contextually ambiguous reconstructions.â€ The model fills in details based on surrounding context and learned patterns, which simply isnâ€™t precise enough for the exact letter shapes that text requires.</p><p>In practical terms? The AI might successfully remove your gibberish text, but when it tries to paint new letters into that space, it struggles to match the exact style, lighting, and perspective of the original image. The result often looks patched rather than seamless.</p><h3><strong>Why youâ€™re not imagining things</strong></h3><p>If editing feels harder than generating, the data backs you up. A <a href=""https://www.mdpi.com/2076-3417/15/5/2274"">2025 benchmark study published in MDPI</a> evaluated multiple AI models across text accuracy metrics and found that all major platforms (including DALL-E 3, Ideogram, and Stable Diffusion) face â€œsignificant challenges with text accuracyâ€ across domains like code, chemical diagrams, and multi-line text.</p><p>The scores are telling. For code-related text, Stable Diffusion scored just 1.25 out of 5 for text accuracy. Even Ideogram, the supposed text champion, managed only 1.75 in the same category. When it comes to complex, structured text requiring precise formatting, all models struggle.</p><p>Platform comparisons in 2025 reflect this reality. <a href=""https://apatero.com/blog/image-to-image-ai-transformation-comparison-2025"">Image-to-image tools vary significantly</a>, with Midjourney notably lacking â€œtrue inpainting and transformation controls.â€ For users who do heavy editing work, local Stable Diffusion with ControlNet remains the go-to option. However, that requires technical expertise most casual users might simply notÂ have.</p><h3><strong>The workarounds (forÂ now)</strong></h3><p>Until AI cracks this particular nut, whatâ€™s a designer to do? A few strategies have emerged, each with their own trade-offs.</p><p><strong>Use text-first generators.</strong> If you know youâ€™ll need accurate typography, start with <a href=""https://ideogram.ai/"">Ideogram</a> or similar text-focused tools rather than trying to force words into Midjourneyâ€™s artistic outputs. Itâ€™s a case of picking the right tool for the job; you wouldnâ€™t edit a feature film in PowerPoint.</p><figure><img alt=""Dual monitor setup showing AI workflow: left screen displays AI-generated coffee shop interior without text, right screen shows the same image in Photoshop with â€˜Daily Brew Co.â€™ text overlay being added. Design tool logos (Photoshop, Figma, Canva, Affinity Designer) surround the monitors on the right side."" src=""https://cdn-images-1.medium.com/max/1024/1*_KEoHpKYJ9mBavQCGm33XA.png"" /><figcaption>Image byÂ author</figcaption></figure><p><strong>Generate first, add text later.</strong> Many professionals now create their AI imagery text-free, then overlay typography using traditional design tools like Photoshop, Canva, or Figma. Itâ€™s an extra step, but the results are far more reliable. This approach also gives you complete control over font selection, kerning, and placement; things AI still handles clumsily atÂ best.</p><p><strong>Try dedicated fix-it tools.</strong> Platforms like <a href=""https://www.makeuseof.com/how-to-fix-gibberish-text-in-ai-generated-images/"">Storia Labâ€™s Textify</a> or Canvaâ€™s Grab Text tool can identify gibberish text and attempt to replace it. Results vary (patience is required) but for simple corrections, they can save considerable headaches. Just donâ€™t expect miracles with complex multi-line text or heavily stylised typography.</p><p><strong>Embrace the hybrid workflow.</strong> As one industry comparison put it, â€œmany professionals use both platforms,â€ generating artistic foundations in Midjourney, then handling typography in Ideogram. Itâ€™s not elegant, and it requires subscriptions to multiple services, but it works. Think of it as assembling a toolkit rather than searching for a single magicÂ wand.</p><p><strong>Keep expectations realistic.</strong> Perhaps the most important strategy is simply managing client and stakeholder expectations. AI-generated imagery is powerful, but it has clear limitations. Flagging potential text issues upfront saves awkward conversations down theÂ line.</p><h3><strong>What this means for designers and UX professionals</strong></h3><p>Beyond the technical challenges, thereâ€™s a broader conversation here about workflow and expectations. As AI tools become more embedded in creative processes, understanding their limitations isnâ€™t just nice-to-have knowledge; itâ€™s essential professional competence.</p><p>For UX writers and content designers, this has particular relevance. If youâ€™re working with teams that use AI-generated imagery, you need to factor text limitations into your content strategy. That punchy headline might look great in a mockup, but will it survive the AI rendering process intact? Sometimes the answer is simply: maybeÂ not.</p><p>And letâ€™s be honest, thereâ€™s something almost poetic about AI mastering the creation of human faces whilst fumbling with human writing. Itâ€™s a reminder that these tools, however impressive, arenâ€™t magic. Theyâ€™re sophisticated pattern-matching systems with specific strengths and equally specific blindÂ spots.</p><h3><strong>Whatâ€™s on theÂ horizon?</strong></h3><p>The research community hasnâ€™t given up. Several promising approaches are in development:</p><ul><li><strong>Hybrid AI systems</strong> that generate a base image through one model, then overlay text using a separate, specialist module designed specifically for accurate placement. Think of it as mimicking how human designers work: creating the visual first, then adding captions as a finalÂ step.</li><li><strong>Better training data.</strong> Most current datasets lack properly labelled, structured text within images. Training on annotated text data could help models understand not just where text appears, but what it actually says and how words are properlyÂ formed.</li><li><strong>Vector-based rendering.</strong> Some teams are exploring ways to separate text from raster imagery entirely, treating typography as a discrete, editable layer rather than baking it into pixels. This would fundamentally change how text is handled, moving from pattern-matching to something closer to actual typesetting.</li><li><strong>OCR feedback loops.</strong> Imagine a model that generates text, runs optical character recognition to check its own work, and iterates until the spelling is correct. This kind of self-correction mechanism could dramatically reduce errors, though it would add computational overhead.</li></ul><h3><strong>Spelling itÂ out</strong></h3><p>AI image generation has come a long way in a remarkably short time. Photorealistic faces, impossible architecture, dreamlike landscapesâ€Šâ€”â€Šall conjured from a few words typed into a prompt box. But text, that most human of visual elements, remains stubbornly resistant.</p><p>The core issue isnâ€™t laziness or neglect. Itâ€™s architectural. Diffusion models were designed to see images holistically, not to parse the precise, rule-bound structures that make written language work. Generation is improving because researchers are training dedicated models for the task. Editing lags behind because preserving context whilst fixing details is a fundamentally harderÂ problem.</p><p>For now, the practical advice is straightforward: plan for text limitations, use the right tool for the job, and donâ€™t be afraid to bring in traditional design software for the finishing touches. AI is brilliant at many things. Fixing spelling errors in images just isnâ€™t one of themâ€¦Â yet.</p><blockquote><strong><em>Thanks for reading!Â ğŸ“–</em></strong></blockquote><blockquote><em>If you liked this post, </em><a href=""https://medium.com/@doracee""><em>follow me on Medium</em></a><em> forÂ more!</em></blockquote><h3><strong>References &amp;Â Credits:</strong></h3><ul><li><a href=""https://www.mdpi.com/2076-3417/15/5/2274"">(2025). Challenges in Generating Accurate Text in Images: A Benchmark for Text-to-Image Models on Specialized Content</a></li><li><a href=""https://stable-diffusion-art.com/how-stable-diffusion-work/"">Stable Diffusion Art. (2024). How Does Stable Diffusion Work?</a></li><li><a href=""https://dev.to/tracywhodoesnot/why-ai-struggles-with-text-in-image-generation-n69"">Why AI Still Struggles with Text in Image Generation</a></li><li><a href=""https://the-decoder.com/ideogram-1-0-outshines-midjourney-and-dall-e-3-with-impressive-text-rendering/"">THE DECODER. (2024). Ideogram 1.0 outshines Midjourney and DALL-E 3 with impressive text rendering</a></li><li><a href=""https://pxz.ai/blog/ideogram-vs-midjourney-2026"">(2026). Ideogram vs Midjourney: 50+ HoursÂ Testing</a></li><li><a href=""https://www.recraft.ai/blog/comparing-popular-and-high-performing-text-to-image-models-and-providers"">(2025). Comparing Text to Image Models and Providers</a></li><li><a href=""https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1614608/full"">Frontiers in AI. (2025). High-resolution image inpainting using a probabilistic framework</a></li><li><a href=""https://apatero.com/blog/image-to-image-ai-transformation-comparison-2025"">(2025). Image-to-Image AI Comparison 2025</a></li><li><a href=""https://www.makeuseof.com/how-to-fix-gibberish-text-in-ai-generated-images/"">(2024). I Tested 4 Tools to Fix Gibberish Text in AI-Generated Images</a></li></ul><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b5232c39bd11"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/lost-for-words-why-text-in-ai-images-still-goes-wrong-b5232c39bd11"">Lost for words: why text in AI images still goes wrong</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/data-tables-need-to-be-accessible-too-6a13533a6fde?source=rss----138adf9c44c---4,1769948938,"Data tables need to be accessible, too","Data tables need to be accessible, too

<h4><em>Why data tables are an accessibility blind-spot (and how WCAGÂ helps)</em></h4><p>Data tables are used in many different products and contexts. They are used in dashboards, product comparisons, or even internal finance reports. They help users make decisions, spot data patterns, and take action whereÂ needed.</p><p><strong>But when it comes to accessibility, data tables are often skipped.</strong> Theyâ€™re assumed to be handled by default HTML or put off until the development stage.</p><p>Visually, the tables look easy to read with clear columns and rows, as well as distinct headers. But for many users (like those using screen readers or keyboard navigation), these tables can become confusing or even unusable.</p><figure><img alt=""Example data tables from IBM Carbon and Salesforce Lightning design systems"" src=""https://cdn-images-1.medium.com/max/1024/1*V2pdiWu3IGVeuqcTJI_yRw.png"" /><figcaption><em>Many open-source data tables are available via </em><a href=""https://react.carbondesignsystem.com/?path=/docs/components-datatable-basic--overview""><em>IBM Carbon</em></a><em> and </em><a href=""https://sds-site-docs-1fea39e7763a.herokuapp.com/index.html?path=/story/components-data-table--base""><em>Salesforce Lightning</em></a></figcaption></figure><p><strong>Just to be clear; WCAG doesnâ€™t prohibit data tables, it just requires that the tableâ€™s information, relationships, and functionality are available to all users.</strong> Not only that, you donâ€™t need complex solutions to have both data tables and WCAG-compliant designs.</p><p>Letâ€™s break down why data tables are frequently overlooked, what WCAG requires of them, and how designers and product teams can create accessible dataÂ tables.</p><h3>Why are tables an accessibility blind-spot?</h3><p>To be fair, data tables sit in an awkward spot between design and development. They feel too technical to be a pure design problem, but also feel too visual to be treated as just code. As a result, accessibility issues slip through theÂ cracks.</p><p>But what specific issues come from neither design or dev taking full ownership ofÂ tables?</p><h4>Tables lack semantic structure</h4><p>Visually, most tables appear easy to scan with aligned column headers and distinguished rows. But when this same structure isnâ€™t communicated properly in the code, assistive technologies lose the context that sighted users get visually.</p><p><strong>Without proper header associations, a screen reader (like Voiceover or NVDA) may announce a value without indicating which column or row it belongsÂ to.</strong></p><p>For instance, letâ€™s say there is a data table depicting the concerts you attended last year. A sighted user can easily understand when and where each concert was, but a screen reader announces â€œ12 Februaryâ€ without the associated concert it corresponds to.</p><figure><img alt=""Data table example with header cells named Date, Event, Venue in the top row"" src=""https://cdn-images-1.medium.com/max/1024/1*m6eIsbk0X6ngwuwkpiQeAQ.png"" /><figcaption><em>Data table example with header cells in the top row via </em><a href=""https://www.w3.org/WAI/tutorials/tables/one-header/#table-with-header-cells-in-the-top-row-only""><em>W3 WAI tutorials</em></a></figcaption></figure><pre>&lt;table&gt;<br />  &lt;tr&gt;<br />    &lt;th&gt;Date&lt;/th&gt;<br />    &lt;th&gt;Event&lt;/th&gt;<br />    &lt;th&gt;Venue&lt;/th&gt;<br />  &lt;/tr&gt;<br />  &lt;tr&gt;<br />    &lt;td&gt;12 February&lt;/td&gt;<br />    &lt;td&gt;Waltz with Strauss&lt;/td&gt;<br />    &lt;td&gt;Main Hall&lt;/td&gt;<br />  &lt;/tr&gt;<br />  [â€¦]<br />&lt;/table&gt;</pre><h4>Design Systems with information gaps</h4><p><strong>Many Design Systems include a table component, but sometimes they donâ€™t clarify how the component should be used in the wild.</strong> The guidance leaves open questions about accessibility, like:</p><ul><li>When should a table be used instead of aÂ list?</li><li>How should sortingÂ behave?</li><li>What happens when data is empty, truncated, orÂ loading?</li></ul><p>Without explicit standards, tables used by different teams behave differently, and accessibility becomes inconsistent.</p><h4>Added interactions without accessibility</h4><p>Modern tables are typically dynamic with sortable columns, selectable rows, and inline actions. But these interaction patterns are layered on top of the table without clear guidance on keyboard behavior, focus order, or screen reader announcements.</p><p><strong>When accessibility is addressed late in development, teams might add impromptu ARIA attributes instead of designing inclusively from the start.</strong> This results in a table that â€œfunctions,â€ but feels unpredictable or exhausting to navigate with only a keyboard.</p><figure><img alt=""Data table with batch actions as an example from IBM Carbon Storybook"" src=""https://cdn-images-1.medium.com/max/1024/1*gxIqfbG4PyAO04wG_Sk3WA.png"" /><figcaption><em>Data table with batch actions via </em><a href=""https://react.carbondesignsystem.com/?path=/story/components-datatable-batch-actions--default""><em>IBM Carbon Storybook</em></a></figcaption></figure><h3>Anatomy of an accessible dataÂ table</h3><h4>Overall structure</h4><ul><li>Rows and columns reflect the dataâ€™s relationships (not just visual alignment)</li><li>Headers clearly describe the content they correspond to</li><li>Reading order matches how the data is meant to be understood</li></ul><figure><img alt=""Tables create inherent visual structure using columns and row"" src=""https://cdn-images-1.medium.com/max/1024/1*Okr49IcRO2jl9hwhoNUSdA.png"" /><figcaption><em>Tables create inherent visual structure using columns and row; via </em><a href=""https://docs.google.com/spreadsheets/u/0/""><em>GoogleÂ Sheets</em></a></figcaption></figure><h4>Data relationships</h4><ul><li>Individual values connect to both their column and their row headers (even in multi-dimensional tables)</li><li>Context is communicated through programmatic associations (not only by visual means like spacing or alignment)</li><li>Users navigate cell by cell and understand the data as aÂ whole</li></ul><h4>Predictable navigation</h4><ul><li>Filters, column headers, and actions have clearÂ labels</li><li>Keyboard focus is always visible on the dataÂ table</li><li>Focus (tab) order aligns with the visual and logical layout of theÂ table</li></ul><figure><img alt=""Dellâ€™s Design System includes proper keyboard interactions for their table component"" src=""https://cdn-images-1.medium.com/max/1024/1*CEFgqCp5UFOJYITDRfLxJg.gif"" /><figcaption><a href=""https://react.delldesignsystem.com/2.21.0/index.html?path=/story/components-table--bulk-actions""><em>Dellâ€™s Design System</em></a><em> includes proper keyboard interactions for their table component</em></figcaption></figure><h4>Proper interactions</h4><ul><li>Interactive tables have clear affordances, accessible labels, and visibleÂ states</li><li>Users know when a column is sortable and which direction itâ€™s sortedÂ in</li><li>Users know what happens when they activate a control with a keyboard orÂ mouse</li></ul><blockquote><strong>Note</strong><em>: This anatomy allows assistive technologies to answer basic questions like: What column am I in? What does this value represent? Without it, users are forced toÂ guess.</em></blockquote><h3>WCAG criteria that matter most for dataÂ tables</h3><p>WCAG doesnâ€™t treat data tables as a special case. Thereâ€™s no guideline that says â€œmake tables accessible.â€ Instead, WCAG is more concerned with whether users can perceive relationships, operate functionality, and understand content within theÂ table.</p><p><strong>WCAG provides a framework, but itâ€™s up to designers and teams to meet the expectations.</strong></p><h4><a href=""https://www.w3.org/WAI/WCAG21/Understanding/info-and-relationships.html#brief"">SC 1.3.1 Info and Relationships (LevelÂ A)</a></h4><p>This criteria asks that all users can understand how headers relate to data cells, regardless of how the table looks visually. When header relationships are missing, assistive technologies canâ€™t provide meaningful context.</p><p><strong>Example techniques toÂ pass:</strong></p><ul><li>Use table markup to present tabular information (<a href=""https://www.w3.org/WAI/WCAG21/Techniques/html/H51"">Technique H51</a>)</li><li>Use caption elements to associate the data tableâ€™s title or heading with the table (<a href=""https://www.w3.org/WAI/WCAG21/Techniques/html/H39"">Technique H39</a>)</li><li>Use the scope attribute to associate header cells with cells in data tables (<a href=""https://www.w3.org/WAI/WCAG21/Techniques/html/H63"">Technique H63</a>)</li><li>Use id and headers attributes to associate cells with header cells in data tables (<a href=""https://www.w3.org/WAI/WCAG21/Techniques/html/H43"">Technique H43</a>)</li></ul><a href=""https://medium.com/media/338e43f81349ebc2a12841deaf0dd258/href"">https://medium.com/media/338e43f81349ebc2a12841deaf0dd258/href</a><h4><a href=""https://www.w3.org/WAI/WCAG21/Understanding/keyboard"">SC 2.1.1 Keyboard (LevelÂ A)</a></h4><p>Any interaction included in a table must be operable using only a keyboard. This includes not only navigating the table itself, but also using features like filtering, pagination, and row-level actions.</p><p><strong>Example techniques toÂ pass:</strong></p><ul><li>Ensure keyboard control for all functionality (<a href=""https://www.w3.org/WAI/WCAG21/Techniques/general/G202"">Technique G202</a>)</li><li>Provide keyboard-triggered event handlers (<a href=""https://www.w3.org/WAI/WCAG21/Techniques/general/G90"">Technique G90</a>)</li></ul><h4><a href=""https://www.w3.org/WAI/WCAG21/Understanding/focus-order#brief"">SC 2.4.3 Focus Order (LevelÂ A)</a></h4><p>Users navigating with a keyboard need to know where they are at all times. WCAG requires visible focus indicators and clear communication of stateÂ changes.</p><p><strong>Example techniques toÂ pass:</strong></p><ul><li>Place the interactive elements in an order that follows sequences and relationships within the content (<a href=""https://www.w3.org/WAI/WCAG21/Techniques/general/G59"">Technique G59</a>)</li><li>Make the DOM order match the visual order (<a href=""https://www.w3.org/WAI/WCAG21/Techniques/css/C27"">Technique C27</a>)</li></ul><h4><a href=""https://www.w3.org/WAI/WCAG21/Understanding/headings-and-labels"">SC 2.4.6 Headings and Labels (LevelÂ AA)</a></h4><p>WCAG requires that controls and content have clear, descriptive labels. In data tables, this applies to the tableâ€™s title, column headers, and its interactive controls.</p><p><strong>Example techniques toÂ pass:</strong></p><ul><li>Provide descriptive headings (<a href=""https://www.w3.org/WAI/WCAG21/Techniques/general/G130"">Technique G130</a>)</li><li>Provide descriptive labels (<a href=""https://www.w3.org/WAI/WCAG21/Techniques/general/G131"">Technique G131</a>)</li></ul><a href=""https://medium.com/media/64748825f9b314ce382b355646219ca9/href"">https://medium.com/media/64748825f9b314ce382b355646219ca9/href</a><h4><a href=""https://www.w3.org/WAI/WCAG21/Understanding/name-role-value#brief"">SC 4.1.2 Name, Role, Value (LevelÂ A)</a></h4><p>WCAG requires that content work reliably with assistive technologies now and in the future. For data tables, this means relying on semantic structure and predictable behavior rather than visually driven solutions.</p><p>Example techniques toÂ pass:</p><ul><li>Use aria-label to provide an accessible name where a visible label cannot be used (<a href=""https://www.w3.org/WAI/WCAG21/Techniques/aria/ARIA14"">Technique ARIA14</a>)</li><li>Use aria-labelledby to provide a name for user interface controls (<a href=""https://www.w3.org/WAI/WCAG21/Techniques/aria/ARIA16"">Technique ARIA16</a>)</li></ul><h3>What designers should do before dev touchesÂ code</h3><p>Accessible data tables start with design decisions. Designers are responsible for preventing accessibility issues long before a developer ever starts coding. This includes defining design decisions like table structure, interactions, and data complexity.</p><h4>Use real structure, not justÂ layout</h4><p>Before thinking about visual styling, designers should answer the following:</p><ul><li>What are theÂ rows?</li><li>What are theÂ columns?</li><li>What relationships matterÂ most?</li></ul><p>If these answers arenâ€™t obvious in the design, they wonâ€™t be obvious to assistive technologies. You need clear column headers, meaningful row labels, and consistent data structure.</p><h4>Have intentional interactions</h4><p>Not every table needs sorting, filtering, or inline actions. Each interaction adds cognitive load and accessibility complexity. When interactions are necessary, designers shouldÂ define:</p><ul><li>Which columns areÂ sortable</li><li>What happens when sorting isÂ applied</li><li>How users move between table content andÂ controls</li><li>If actions apply to a row, a cell, or the entireÂ table</li></ul><figure><img alt=""Atlassianâ€™s Design System includes interactions when it makes sense with the data relationships"" src=""https://cdn-images-1.medium.com/max/1024/1*pplXXuqgTPxTXbCCSO0EqA.png"" /><figcaption><a href=""https://atlassian.design/components/dynamic-table/examples#uncontrolled""><em>Atlassianâ€™s Design System</em></a><em> includes interactions, like sorting, when it makes sense with the data relationships</em></figcaption></figure><h4>Design for keyboardÂ users</h4><p>Using a keyboard is a primary input method for many users. Designers need to ensure that tables can be navigated in a logical, predictable order byÂ asking:</p><ul><li>Can every interactive element be reached without a mouse/ trackpad?</li><li>Does keyboard focus move in a way that matches the tableâ€™s structure?</li><li>Are hover-only actions also doable via keyboard?</li></ul><h4>Consider various screenÂ sizes</h4><p>Designers need to explicitly define how table structure and relationships are preserved across breakpoints. As the screen size shrinks, itâ€™s simple to collapse and hide content, but designers should document:</p><ul><li>How labels remain associated with theirÂ values</li><li>How users can navigate between theÂ data</li><li>How accessible interactions are maintained</li></ul><figure><img alt=""Lightning Design Systemâ€™s data table responding to mobile screen size"" src=""https://cdn-images-1.medium.com/max/1024/1*8s-b_ImlOvrMbOwmyYkvvg.png"" /><figcaption>Data tables need to respond when presented on mobile devices; via <a href=""https://sds-site-docs-1fea39e7763a.herokuapp.com/index.html?path=/story/components-data-table--with-sorting"">Lightning DesignÂ System</a></figcaption></figure><h4>Document table expectations in DesignÂ Systems</h4><p>Data tables scale and remain consistent when theyâ€™re a shared standard. Documenting how tables behave is just as important as documenting how theyÂ look.</p><p>Helpful documentation includes:</p><ul><li>When to use a table (and when to other components like aÂ list)</li><li>Required semantics with structure, headers, andÂ labels</li><li>Keyboard and interaction behavior (like focus and pagination)</li><li>Interaction states and edge cases (like empty, loading, or errorÂ states)</li><li>Any known accessibility considerations or constraints</li></ul><p>Data tables are treated as containers for information, but theyâ€™re some of the most complex elements users interact with. Without incorporating accessibility into tables, users lose context, navigation becomes frustrating, and critical data becomes hard to understand.</p><p><strong>Complying with WCAG criteria helps all users no matter their ability.</strong> Accessible tables are easier to scan, more predictable to navigate, and clearly communicate meaning. They benefit screen reader users, keyboard users, and anyone working with dense data. When tables are designed with the same care as any other interface, accessibility follows naturally.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6a13533a6fde"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/data-tables-need-to-be-accessible-too-6a13533a6fde"">Data tables need to be accessible, too</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/beyond-the-double-diamond-integrating-leavitts-diamond-into-ux-e581b6d0a799?source=rss----138adf9c44c---4,1769862107,Beyond the Double Diamond: Integrating Leavittâ€™s Diamond into UX,"Beyond the Double Diamond: Integrating Leavittâ€™s Diamond into UX

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/beyond-the-double-diamond-integrating-leavitts-diamond-into-ux-e581b6d0a799?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/1200/0*YQon5D-87jYUit6M.png"" width=""1200"" /></a></p><p class=""medium-feed-snippet"">Moving from &#x201c;feature factories&#x201d; to systems architecture</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/beyond-the-double-diamond-integrating-leavitts-diamond-into-ux-e581b6d0a799?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/when-design-stops-asking-why-and-starts-asking-can-ai-do-it-625c9a5d9c68?source=rss----138adf9c44c---4,1769862102,When design stops asking why and starts asking â€œcan AI do it?â€,"When design stops asking why and starts asking â€œcan AI do it?â€

<h3>When design stops asking why and starts asking, â€œCan AI doÂ it?â€</h3><h4>This advice is why designers are quietly losing strategic influence. We optimized AI interfaces for confidence. Organizations learned that confidence replaces judgment.</h4><figure><img alt=""Illustration of a fictional team chat where a product leader pushes for an AI-generated solution to a business problem before design investigation."" src=""https://cdn-images-1.medium.com/max/1024/1*oypi566Pl6vxkzJGk69RQw.png"" /><figcaption>Image Credit:Â Author.</figcaption></figure><p>The question dropped into the Slack channel before the user research summary. Before the problem was clearly defined. Before anyone asked if users actually needed thisÂ feature.</p><p>Your product manager already generated three interface options in ChatGPT. Now theyâ€™re asking which one to build. Not <em>whether</em> to build. Not <em>why</em> to build.Â <em>Which.</em></p><p>And when you slow the conversation down to ask those questions, youâ€™re about to discover that strategic thinking now reads as bottleneck behavior.</p><p>This isnâ€™t an accident. We designed the system that taught teams to trust AI outputs over design judgment.</p><p><a href=""https://www.figma.com/blog/figma-2025-ai-report-perspectives/"">Figmaâ€™s 2025 AI Report</a> surveyed 2,500 designers and developers and found that 78% agree AI significantly enhances work efficiency. Only 32% say they can actually rely on theÂ output.</p><figure><img alt=""Copy of Figmaâ€™s 2025 AI Report"" src=""https://cdn-images-1.medium.com/max/1024/1*IFEBoa0VZjExIekRqsQWkQ.png"" /><figcaption><a href=""https://www.figma.com/reports/ai-2025/"">Copy of Figmaâ€™s 2025 AIÂ Report.</a></figcaption></figure><p>That gap isnâ€™t a quality problem. Itâ€™s a powerÂ shift.</p><p>22% designers now use AI to create first drafts of interfaces. 33% use it to generate design assets. The time from concept to visible prototype collapsed from days toÂ minutes.</p><p>But something shifted that nobody warned you about: <strong>â€œCan AI do this?â€ started showing up at the beginning of product discussions.</strong> Not after user needs are validated. Not after strategic intent is clarified. It arrivesÂ first.</p><p>And when you slow teams down to ask strategic questions, youâ€™re increasingly seen as friction rather than guidance.</p><p>We built this pattern. Now itâ€™s being used againstÂ us.</p><h3>We taught teams that polish means correctness</h3><p>For years, UX designers optimized AI interfaces to hide uncertainty. <a href=""https://medium.com/core-ai/why-ai-sounds-confident-even-when-its-wrong-4c0683a6f61e"">Chatbots</a> that sounded certain even when guessing. Loading states that implied thoughtfulness without revealing computational doubt. Systems that presented single recommendations with high visual confidence rather than surfacing alternatives with calibrated uncertainty.</p><p>We did this because <a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC3240751/"">research told us</a> users trust confident systems. Smooth experiences read as competent. Hesitation feels like failure. As Erika Hall noted in her <a href=""https://picovoice.ai/blog/conversation-design-tips-from-Hall/"">conversation with John Maeda</a>, â€œthe level of visual polish can lead designers and decision-makers to think that the concepts underneath are stronger than theyÂ are.â€</p><p>The concepts underneath are what matter. But we wrapped them in interfaces that signaled completion rather than exploration.</p><p>Those design decisions didnâ€™t stay contained. They taught organizations how to relate to AI outputs: <strong>polish became synonymous with correctness</strong>. Generation became synonymous with judgment. â€œAI created thisâ€ became implicit validation rather than a signal requiring verification.</p><p>We optimized for adoption. We got replacement.</p><h3>The data exposes the disconnect</h3><p>Figmaâ€™s survey of 2,500 designers and developers reveals the mechanic driving designâ€™s strategic erosion:</p><ul><li><strong>78% agree AI significantly enhances work efficiency</strong></li><li><strong>Only 32% say they can rely on AIÂ output</strong></li><li><strong>68% of developers report AI improves their workÂ quality</strong></li><li><strong>Only 40% of designers say theÂ same</strong></li></ul><p>The gap isnâ€™t about AI capability. Itâ€™s about what organizations value. When engineers can generate functional code, theyâ€™re delivering tangible output. When designers generate questions about whether this serves users, theyâ€™re deliveringâ€¦ friction.</p><figure><img alt=""Copy from Figmaâ€™s AI Report."" src=""https://cdn-images-1.medium.com/max/428/1*RnkBrXncSK__OSsyVRdAow.png"" /><figcaption><a href=""https://www.figma.com/reports/ai-2025/"">https://www.figma.com/reports/ai-2025/</a></figcaption></figure><p><a href=""https://medium.com/@msjuliabraimova/the-rise-of-ai-in-ux-ui-design-how-artificial-intelligence-is-reshaping-digital-experiences-7639bee841c7"">Julie Zhuo observed</a>: â€œAI is redefining how we prototype. What once took days can now be done in hours, allowing designers to iterate and test more rapidly.â€ True. But iteration without interrogation isnâ€™t strategic designâ€Šâ€”â€Šitâ€™s production atÂ scale.</p><p><a href=""https://banotes.org/administrative-thinkers/bounded-rationality-satisficing-decision-making-simon/"">Herbert Simonâ€™s research on bounded rationality</a> explains why this matters: under time pressure and cognitive constraints, people accept solutions that appear â€œgood enoughâ€ rather than optimal. AI doesnâ€™t create this behaviorâ€Šâ€”â€Šit accelerates it by making â€œgood enoughâ€ arrive so quickly that deeper evaluation feels wasteful.</p><p>When creation is cheap, <strong>teams learn to mistake speed for strategy</strong>.</p><h3>What actually changed: Decision orderÂ flipped</h3><p>Product teams used to follow a sequence: understand user problems â†’ clarify strategic intent â†’ explore solutions â†’ generate artifacts. This ordering wasnâ€™t arbitraryâ€Šâ€”â€Šit created space for judgment.</p><p>AI disrupted that sequence not by replacing designers but by making execution nearly instantaneous. When something can be generated in seconds, the act of creation no longer signals commitment. It signals possibility.</p><p>But teams donâ€™t always treat it thatÂ way.</p><p>Once a generated interface existsâ€Šâ€”â€Ševen provisionallyâ€Šâ€”â€Šit reshapes the conversation. The artifact becomes gravitational. Feedback clusters around it. Critique becomes incremental. The deeper question (<em>why this at all</em>) arrives late, if it arrives atÂ all.</p><p><a href=""https://designintech.report/2024/03/12/design-against-ai-2024/"">John Maedaâ€™s 2024 Design in Tech Report</a> distinguishes between â€œmakersâ€ (designers and developers who create) and â€œtalkersâ€ (product managers who drive revenue). AI made it easier for makers to make. But it made it <em>better</em> for talkers to talkâ€Šâ€”â€Šbecause AI outputs give them tangible artifacts to discuss without waiting for design judgment.</p><figure><img alt=""Abstract architectural forms in teal, gold, and black showing stacked geometric shapes representing organizational hierarchy transformation and power redistribution between different roles"" src=""https://cdn-images-1.medium.com/max/1024/1*EivrjbF7gUjc8D0uhCv67Q.png"" /><figcaption>The organizational power shift: When AI made artifacts instant, strategic influence moved from makers to talkers. Generated With Midjourney.</figcaption></figure><p>When stakeholders see â€œworkingâ€ prototypes in first meetings, the implicit question becomes: <em>What do we need designers for?</em></p><p>The answer used to be: <strong>Strategic thinking. User advocacy. The discipline to ask why beforeÂ how.</strong></p><p>But when generation speed becomes the primary value signal, those skills read as obstruction.</p><h3>The confidence problem weÂ created</h3><p>Most AI design tools are optimized for decisiveness. They generate singular recommendations with confident presentation. This makes sense from a UX perspectiveâ€Šâ€”â€Šconfidence reads as competence, smoothness reads as quality. As <a href=""https://www.studioubique.com/ai-ux-design/"">design best practices emphasize</a>, AI interfaces should â€œset honest expectationsâ€ and â€œshow confidence.â€</p><p>But between honest expectations and confident presentation, most products chose confidence.</p><p>This creates cascading problems. When AI presents one interface design with high visual polish, teams treat it as <em>the</em> answer rather than <em>an</em> exploration. Alternative approaches arenâ€™t surfaced. Edge cases arenâ€™t flagged. Uncertainty is smoothedÂ over.</p><p><a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC3240751/"">Research on automation bias</a> has demonstrated this dynamic for decades: people are significantly more likely to accept system output even when it conflicts with their own judgment, particularly when that output is presented confidently. The effect strengthens as systems appear more capable and authoritative.</p><p><a href=""https://interactions.acm.org/archive/view/july-august-2019/toward-human-centered-ai"">Human-centered AI research</a> has long advocated that systems should â€œsupport human judgment rather than replace itâ€â€Šâ€”â€Šsurfacing uncertainty, presenting multiple options, enabling override. But velocity pressures push teams toward tools that minimize friction, not maximize judgment.</p><p><strong>We designed the experience that trained organizations to trust confident outputs over strategic questioning.</strong></p><h3>What stopped gettingÂ asked</h3><p>As generation becomes easier, certain questions surface less often in product discussions:</p><p><strong>Why this approach instead of alternatives?</strong> When AI produces one polished solution quickly, exploring other directions feels wasteful. The existence of a â€œworkingâ€ prototype creates psychological commitment before strategic evaluation happens.</p><p><strong>What assumptions are embedded in this output?</strong> AI training data encodes countless decisions about what â€œgood designâ€ looks like, what user problems matter, what solutions are appropriate. These assumptions remain invisible unless teams actively interrogate them.</p><p><strong>Who does this work well forâ€Šâ€”â€Šand who does it exclude?</strong> Rapid generation optimizes for median cases in training data. Edge cases, accessibility considerations, users who donâ€™t match demographic norms get systematically overlooked.</p><p>As <a href=""https://www.muledesign.com/"">Mike Monteiro and Erika Hall have argued</a> for years, designâ€™s ethical responsibility is interrogating these questions <em>before</em> building. But when â€œCan AI do this?â€ shows up first, those questions get framed as barriers to velocity rather than essential judgment.</p><h3>The strategic ground designers areÂ losing</h3><p><strong>1 in 3 Figma users shipped an AI-powered product in 2025</strong>â€Šâ€”â€Ša 50% increase from 2024. Only 9% cited revenue growth as the primary goal. Instead, <strong>35% said â€˜experiment with AIâ€™ and 41% said â€˜enhance customer experienceâ€™</strong>â€Šâ€”â€Šgoals that struggle to define measurable success.</p><figure><img alt=""Fractured architectural platform with city lights glowing on top while ocean waves erode the breaking edges, representing diminishing strategic territory and crumbling foundation beneath ongoing work"" src=""https://cdn-images-1.medium.com/max/1024/1*dfkLKul12MDKLvxYSyVw3A.png"" /><figcaption>The strategic erosion: Design work continues on the surface while foundational influence fragments beneath. Generated With Midjourney.</figcaption></figure><p>Translation: teams are building because they can, not because theyâ€™ve clarified what problem requiresÂ solving.</p><p>This isnâ€™t speculation. <a href=""https://www.nngroup.com/articles/state-of-ux-2026/"">Nielsen Norman Groupâ€™s State of UX in 2026</a> names the existential tension directly: â€œAvailable roles will increasingly demand breadth and judgment, not just artifactsâ€¦ The practitioners who thrive will be adaptable generalists who treat UX as strategic problem solving, rather than focusing on producing deliverables.â€</p><p>But when artifacts arrive instantly via AI, organizations donâ€™t value judgment that questions whether those artifacts should exist. They value judgment that makes those artifacts shipÂ faster.</p><p><a href=""https://maven.com/centercentre/uxai"">Jared Spool notes</a>: â€œAI gives us an unprecedented ability to anticipate user needs. The challenge is balancing automation with human empathy in design.â€ The challenge isnâ€™t technical. Itâ€™s organizational. When teams measure progress in shipped features rather than solved problems, empathy reads as slowdown.</p><p><strong>68% of developers say AI improves work quality. Only 40% of designers agree.</strong> The gap reveals whoâ€™s winning the value argument. Engineers deliver code. Designers deliver questions. In velocity-obsessed cultures, questions donâ€™tÂ ship.</p><h3>What separates successful teams from everyoneÂ else</h3><p>Figmaâ€™s data on successful versus unsuccessful AI product teams reveals the pattern: <strong>60% of successful teams explored multiple design or technical approaches</strong>, compared to only <strong>39% of unsuccessful ones</strong>.</p><p>The differentiator wasnâ€™t AI capability. It was judgment discipline.</p><blockquote>Successful teams donâ€™t slow down generation. They slow down after generation.</blockquote><p>They treat AI output as a starting point requiring validation, not a conclusion requiring execution. They assume speed increases the risk of unexamined assumptions, not decreases it.</p><p>This aligns with <a href=""https://www.muledesign.com/"">principles Erika Hall has advocated</a> for years: â€œ80% of your job should be talking to peopleâ€¦ The concepts underneath are the most important part.â€ But talking takes time. In cultures optimized for generation speed, time feels expensive.</p><p><a href=""https://designbetterpodcast.com/p/brad-frost"">Brad Frostâ€™s work on design systems</a> emphasizes that sustainable systems require â€œhuman relationshipsâ€ and collaborative judgment, not just component libraries. But when AI can generate components in seconds, the relationship part getsÂ skipped.</p><h3>The uncomfortable truth about designâ€™s complicity</h3><p>Hereâ€™s what makes this particularly painful: <strong>UX designers built thisÂ problem</strong>.</p><p>We spent years optimizing AI interfaces for confident presentation over calibrated uncertainty. We designed chatbots to sound certain even when guessing. We created loading states that implied thoughtfulness without revealing doubt. We built systems that hid alternatives behind single recommendations.</p><p>We did this because users wanted smooth experiences. Because confident systems get adopted. Because our job was removing friction.</p><p>Those design decisions didnâ€™t stay contained. They taught organizations that <strong>AI outputs deserve trust by default</strong>. That polish equals correctness. That questioning generated work slows teamsÂ down.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*Alv38Wim_CfOCyWKzwudig.png"" /><figcaption>We optimized for smooth experiences. We didnâ€™t see what we were hiding underneath. Generated with Midjourney.</figcaption></figure><p>As <a href=""https://johnmaeda.medium.com/autodesigners-on-autopilot-88c5b07609b9"">John Maedaâ€™s 2025 Design in Tech Report</a> notes: â€œComputational thinking is invaluableâ€¦ Work transformation is coming FAST.â€ We focused on making tools smooth. We didnâ€™t prepare for how smooth tools would reshape what organizations value about designers.</p><h3>What happensÂ next</h3><p>The real risk isnâ€™t that AI will design for us. AI already designs <em>with</em> us, and that collaboration produces real value when guided by strategic judgment.</p><p>The risk is that organizations will stop rewarding the judgmentÂ part.</p><p>When â€œAI can do this fasterâ€ becomes sufficient reason to build something, design stops being about solving meaningful problems and becomes about demonstrating AI capability. User needs become secondary to technical possibility. Strategic clarity becomes friction to be eliminated.</p><p>As noted, teams cite â€˜experiment with AIâ€™ and â€˜enhance customer experienceâ€™ as goals rather than measurable outcomes. Theyâ€™re building because they can, iterating because itâ€™s fast, shipping because velocity signals progressâ€Šâ€”â€Šall without the designers who used to ask whether any of this serves anyone beyond the teamâ€™s desire to ship AI features.</p><h3>The choice that hasnâ€™t disappeared yet</h3><p>Design has always been about judgment. Not just making things usable or attractive, but deciding what deserves to exist in the firstÂ place.</p><p>AI changes how quickly we arrive at form. It doesnâ€™t change the need forÂ intent.</p><p>When creation is cheap, judgment becomes the most valuable part of the process. When generation is instant, the ability to say â€œthis solves the wrong problemâ€ becomes rare enough to be strategically important. When teams can build anything, knowing what <em>not</em> to build becomes the differentiator.</p><figure><img alt=""An image of Speed vs Quality from article Finding the balance: Speed vs Quality."" src=""https://cdn-images-1.medium.com/max/1024/1*XaAPOgfsnuGydQH2fiNwqg.png"" /><figcaption>Article Source: <a href=""https://medium.com/design-bootcamp/finding-the-balance-speed-vs-quality-1955d42da6bf"">Finding the balance: Speed vsÂ Quality</a></figcaption></figure><p>But Nielsen Norman Group is clear: â€œAdaptability, strategy, and discernment are the skills that will serve us best in the futureâ€¦ If youâ€™re just slapping together components from a design system, youâ€™re already replaceable by AI. What isnâ€™t easy to automate? Curated taste, research-informed contextual understanding, critical thinking, and careful judgment.â€</p><p>The question isnâ€™t whether designers can prompt AIÂ tools.</p><p>The question is whether organizations will still value designers who slow down to ask <em>why</em>â€Šâ€”â€Ševen when AI has already answeredÂ <em>how</em>.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=625c9a5d9c68"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/when-design-stops-asking-why-and-starts-asking-can-ai-do-it-625c9a5d9c68"">When design stops asking why and starts asking â€œcan AI do it?â€</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/going-analog-in-2026-64a007180d4c?source=rss----138adf9c44c---4,1769862084,Going analog in 2026,"Going analog in 2026

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/going-analog-in-2026-64a007180d4c?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/1600/1*L6qGRoRcrICrf4eKfwiWxw.png"" width=""1600"" /></a></p><p class=""medium-feed-snippet"">Can UX coexist with analog living?</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/going-analog-in-2026-64a007180d4c?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/thinking-clearly-while-everything-speeds-up-af5399ac0f7f?source=rss----138adf9c44c---4,1769775212,Thinking clearly while everything speeds up,"Thinking clearly while everything speeds up

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/thinking-clearly-while-everything-speeds-up-af5399ac0f7f?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/2600/1*9ODCVnUP3kLQO6hYu_ePiQ.jpeg"" width=""5712"" /></a></p><p class=""medium-feed-snippet"">It&#x2019;s a crazy time to be alive, let alone be a UX designer.</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/thinking-clearly-while-everything-speeds-up-af5399ac0f7f?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
