source,domain,url,created_utc,title,text
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/designing-streak-system-ux-psychology/,1771426800,Designing A Streak System: The UX And Psychology Of Streaks,"Designing A Streak System: The UX And Psychology Of Streaks

What makes streaks so powerful and addictive? To design them well, you need to understand how they align with human psychology. Victor Ayomipo breaks down the UX and design principles behind effective streak systems."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/building-empathy-centred-ux-framework-mental-health-apps/,1770994800,Building Digital Trust: An Empathy-Centred UX Framework For Mental Health Apps,"Building Digital Trust: An Empathy-Centred UX Framework For Mental Health Apps

Designing for mental health means designing for vulnerability. Empathy-Centred UX becomes not a “nice to have” but a fundamental design requirement. Here’s a practical framework for building trust-first mental health products."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/designing-agentic-ai-practical-ux-patterns/,1770814800,"Designing For Agentic AI: Practical UX Patterns For Control, Consent, And Accountability","Designing For Agentic AI: Practical UX Patterns For Control, Consent, And Accountability

Autonomy is an output of a technical system. Trustworthiness is an output of a design process. Here are concrete design patterns, operational frameworks, and organizational practices for building agentic systems that are not only powerful but also transparent, controllable, and trustworthy."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/css-scope-alternative-naming-conventions/,1770278400,CSS <code>@scope</code>: An Alternative To Naming Conventions And Heavy Abstractions,"CSS <code>@scope</code>: An Alternative To Naming Conventions And Heavy Abstractions

Prescriptive class name conventions are no longer enough to keep CSS maintainable in a world of increasingly complex interfaces. Can the new `@scope` rule finally give developers the confidence to write CSS that can keep up with modern front ends?"
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/combobox-vs-multiselect-vs-listbox/,1770112800,Combobox vs. Multiselect vs. Listbox: How To Choose The Right One,"Combobox vs. Multiselect vs. Listbox: How To Choose The Right One

Combobox vs. Multi-Select vs. Listbox vs. Dual Listbox? How they are different, what purpose they serve, and how to choose the right one. Brought to you by <a href=""https://ai-design-patterns.com"">Design Patterns For AI Interfaces</a>, **friendly video courses on UX** and design patterns by Vitaly."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/desktop-wallpaper-calendars-february-2026/,1769850000,"Short Month, Big Ideas (February 2026 Wallpapers Edition)","Short Month, Big Ideas (February 2026 Wallpapers Edition)

Let’s make the most of the shortest month of the year with a new collection of desktop wallpapers that are sure to bring a smile to your face — and maybe spark your creativity, too. All of them were designed with love by the community for the community and can be downloaded for free. Happy February!"
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/practical-use-ai-coding-tools-responsible-developer/,1769778000,Practical Use Of AI Coding Tools For The Responsible Developer,"Practical Use Of AI Coding Tools For The Responsible Developer

AI coding tools like agents can be valuable allies in everyday development work. They help handle time-consuming grunt work, guide you through large legacy codebases, and offer low-risk ways to implement features in previously unfamiliar programming languages. Here are practical, easy-to-apply techniques to help you use these tools to improve your workflow."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/unstacking-css-stacking-contexts/,1769508000,Unstacking CSS Stacking Contexts,"Unstacking CSS Stacking Contexts

In CSS, we can create “stacking contexts” where elements are visually placed one on top of the next in a three-dimensional sense that creates the perception of depth. Stacking contexts are incredibly useful, but they’re also widely misunderstood and often mistakenly created, leading to a slew of layout issues that can be tricky to solve."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/beyond-generative-rise-agentic-ai-user-centric-design/,1769086800,Beyond Generative: The Rise Of Agentic AI And User-Centric Design,"Beyond Generative: The Rise Of Agentic AI And User-Centric Design

Developing effective agentic AI requires a new research playbook. When systems plan, decide, and act on our behalf, UX moves beyond usability testing into the realm of trust, consent, and accountability. Victor Yocco outlines the research methods needed to design agentic AI systems responsibly."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/rethinking-pixel-perfect-web-design/,1768903200,Rethinking “Pixel Perfect” Web Design,"Rethinking “Pixel Perfect” Web Design

Amit Sheen takes a hard look at the “Pixel Perfect” legacy concept, explaining why it’s failing us and redefining what “perfection” actually looks like in a multi-device, fluid world."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/smashing-animations-part-8-css-relative-colour/,1768384800,Smashing Animations Part 8: Theming Animations Using CSS Relative Colour,"Smashing Animations Part 8: Theming Animations Using CSS Relative Colour

CSS relative colour values are now widely supported. In this article, pioneering author and web designer [Andy Clarke](https://stuffandnonsense.co.uk/) shares practical techniques for using them to theme and animate SVG graphics."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/ux-product-designer-career-paths/,1768212000,UX And Product Designer’s Career Paths In 2026,"UX And Product Designer’s Career Paths In 2026

How to shape your career path for 2026, with decision trees for designers and a UX skills self-assessment matrix. The only limits for tomorrow are the doubts we have today. Brought to you by <a href=""https://smart-interface-design-patterns.com/"">Smart Interface Design Patterns</a>, a **friendly video course on UX** and design patterns by Vitaly."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/penpot-experimenting-mcp-servers-ai-powered-design-workflows/,1767859200,Penpot Is Experimenting With MCP Servers For AI-Powered Design Workflows,"Penpot Is Experimenting With MCP Servers For AI-Powered Design Workflows

[Penpot](https://penpot.app/?utm_source=SmashingMagazine&amp;utm_medium=Article&amp;utm_campaign=MCPserver) is experimenting with MCP (Model Context Protocol) servers, which could lead to designers and developers being able to perform tasks in Penpot using AI that’s able to understand and interact with Penpot design files. Daniel Schwarz explains how [Penpot MCP](https://github.com/penpot/penpot-mcp) servers work, what they could mean for creating and managing designs in Penpot, and what you can do to help shape their development."
rss,uxdesign.cc,https://uxdesign.cc/the-craft-of-the-instruction-f0da34c445cb?source=rss----138adf9c44c---4,1771681999,The craft of the instruction,"The craft of the instruction

<h4><strong>Writing AI prompts isn’t just a new technical skill — it’s how we can make our own thinking visible</strong></h4><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*bRZ9l9RUCOKKTpZD.png"" /><figcaption><em>In this collage: Baby photo in front of Frank Lloyd Wright’s Fallingwater in Mill Run, Pennsylvania; “Towers on String — Variant Dispersed” by Haegue Yang, 2013; example of a writing instruction with corresponding output.</em> <em>Image credit: Personal photograph (Fallingwater, 2005); Haegue Yang, “Towers on String — Variant Dispersed,” 2013, Henry Art Museum; original writing instruction document by author.</em></figcaption></figure><p>I have a confession. The article I wrote last month — was written with Claude. This one too. Not by Claude. With Claude. And the difference between those two words is what I want to unpack here.</p><p>Both pieces were written using instructions. Instructions I created. Instructions I am continuing to hone — instructions that required me to study my own old essays, identifying what I do when I write. The sentence rhythms. The way I move between timescales. The zooming in and out from concept to detail. The instructions tell Claude how I would like ideas composed. I pull together concepts and experiences from my lived expertise to formulate a point of view — in this case, on this new AI technology we are all metabolizing into our lives and work. For this piece, we’ve gone back-and-forth eleven times to get to this published draft. I rework most sentences but not all.</p><p>This is a powerful and highly personalizable tool. Not the output. The instruction.</p><p>/</p><p>Christopher Alexander published <em>A Pattern Language</em> in 1977. Two hundred and fifty-three patterns — from the scale of entire regions down to the placement of a doorknob — each one describing a recurring problem in the built environment and offering the core of a solution. Pattern 159: Light on Two Sides of Every Room. Pattern 88: Street Cafe. Pattern 252: Pools of Light. The patterns are specific, tested, structural. They give you a system.</p><p>But Alexander was never interested in just the system. He was interested in what he called “the quality without a name” — something alive, whole, experienced — that emerges when patterns are followed well but can never be reduced to logic alone. He tried word after word to capture it. None were sufficient. The quality resists naming. It can only be felt.</p><p>I studied such phenomena at the University of Pennsylvania. Architecture history and theory. I spent countless hours thinking about the relationship between ideas on form and proportion, the structures created from those ideas, and the life that happens inside of them. My final thesis examined how concepts of nature in urban theory shape the way we read and design cities. Three canonical texts on urbanism, three entirely different readings of the same city, because each began with a different understanding of what was “natural.” The foundational mental models we carry into a problem structure everything that follows.</p><p>Attunement to that tension has stayed with me. The pattern is the structure. The pattern came from distilling ideas and experiences. But what makes a place — what makes any made thing — feel alive is something the pattern enables but does not contain.</p><p>What I am uncovering — and what has been genuinely surprising — twenty years later and in an entirely different medium, is that the same tension lives inside the instructions we write for AI.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*ut-R4ivcHKM9dNChdr_b1Q.png"" /><figcaption><em>In this collage: Instructions from “A Pattern Language” — Pattern 128, Indoor Sunlight; photograph of Fallingwater; covers of Collage City (Rowe &amp; Koetter), Architecture of the City (Rossi), S,M,L,XL (Koolhaas).</em> <em>Image credit: Christopher Alexander, A Pattern Language, Oxford University Press, 1977 (pattern excerpt); personal photograph (Fallingwater, 2005); book covers from Collage City, The Architecture of the City, and S,M,L,XL used for educational reference.</em></figcaption></figure><p>/</p><p>I used to write a lot. I have stacks of essays no one will ever read. But they sharpened something — the way I pull ideas together, the way I form very long sentences, the way I know in my intuition, not my intellect, when something lands.</p><p>Then life happened. Architecture for eight-ish years, a pivot into product design for the past ten-ish, two kids. Most days I vacillate between corporate theater, recreating K-pop demon hunter hairstyles, long conversations with engineers to grok the technical enough to translate it into the experiential, and putting nutritious meals on the table that go largely underappreciated. Between all this life, I still had ideas I wanted to better formulate with words. I craved that time at a carrel stacked with texts in a library — putting concepts together into an essay, hard-coding what I was feeling into a cohesive perspective. But the hours for that type of work had disappeared. The practice of writing had largely gone quiet.</p><p>So I ran an experiment. I took writing I loved — my own writing, from when I had the time to be careful with it — and I fed it to Claude. Not to generate new text. To analyze. This was my exact prompt:</p><blockquote>“I want you to look at these articles I wrote and analyze the writing mechanics I used in them to create instructions for this project space. I will write several articles in this project and I want them to sound like me and leverage some of the things I did in these articles and apply them to articles in the future.”</blockquote><p>What Claude identified was a penchant for varying sentence lengths. Shocking, right? Moving back and forth in time — from cave paintings at Lascaux, 40,000 BC, to Haegue Yang, a South Korean sculptor working in installation today. The “/” section breaks I use instead of headers. The way I embed lists in prose rather than bullets. The em dashes that let me think sideways mid-sentence — which I get called out for at work as a negative, but get to trademark as an aesthetic signature here.</p><p>A pattern language for <em>my</em> writing.</p><p>I don’t use that phrase flippantly. Because what I found myself holding was exactly what Alexander described: a collection of recurring solutions to recurring problems of expression, each one specific enough to be actionable but open enough to generate infinite variation. Not a template. Not a formula. A language — in the Alexandrian sense — that I can use to build something that feels like mine even when I am not building it alone.</p><p>This move — from intuitive practice to explicit instruction — is something designers in the field are increasingly grappling with. In a thoughtful<a href=""https://uxdesign.cc/a-ux-designer-guide-to-prompt-f045cd839489""> UX Collective piece on prompting as design craft</a>, Paz Perez frames prompting not as a trick for getting AI to comply, but as the new medium through which designers articulate intent: “Effective communication, whether with people or machines, depends on context.” She’s describing something deeper than prompt hygiene. She’s describing the need to externalize what we usually carry internally — the instinct, the judgment, the taste.</p><p>/</p><p>This changed something. Not because Claude now “writes like me” — that’s too simplified. It changed something because I now understand my own craft in a way I didn’t before. I can see the moves I make. I can name them.</p><p>Alexander wanted patterns to be legible to ordinary people — not just architects. When he interviewed for a position as head of the architecture department at Cambridge and was asked who his first hire would be, he said a carpenter. The panel pressed: which <em>named architect</em>? He persisted. Who after the carpenter? “A mason.” He didn’t get the job.</p><p>The point was radical to academics: the people who make things should understand the patterns they’re working with. The knowledge shouldn’t live with experts alone. It should be owned by the people who construct and inhabit the spaces.</p><p>That’s what happened when I extracted my writing patterns. The knowledge stopped being locked inside intuitive practice and became something I could hold, examine, refine. The instructions aren’t just for Claude. They’re for me — a mirror held up to accumulated craft.</p><p>The philosopher Michael Polanyi captured this asymmetry in a phrase — “We can know more than we can tell.” His point was that most sophisticated human skill is tacit — embedded in the body, in habit, in intuition — and therefore almost impossible to fully articulate. What I find remarkable about writing instructions for AI is that it forces exactly this kind of articulation. The tacit becomes explicit, not because the AI demands it, but because without that explicitness, the AI produces something competent and hollow. You have to tell it what you know.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*-bFFsDcP6F0XSxBTYfz6sg.png"" /><figcaption><em>Collage includes: K-pop inspired hairstyle recreation; the Rosetta Stone; an illustration from Wired Magazine by Koma Zhang.</em> <em>Image credit: Personal photograph (hairstyle); The Rosetta Stone, © The Trustees of the British Museum; illustration by</em><a href=""https://www.instagram.com/komaciel/""><em> Koma Zhang</em></a><em> for Wired Magazine.</em></figcaption></figure><p>/</p><p>My writing process these days is messy, but <em>it is happening</em>. I am always the first one up, before the kids are fully conscious, and I talk. Speech input — Wispr Flow into a Google Doc on my phone — putting ideas down while making breakfast or packing lunches. It gives my writing a spoken quality I actually prefer to what comes out when I type carefully. Then I bring those fragments to the Claude project space where my instructions are housed — along with other writings as context and memory — and we go back and forth. Claude generates a draft. I edit, weave concepts in and out, rewrite in a Google Doc, send it back. And so on. The AI holds the scaffolding; I do the building.</p><p>I want to be clear: this is not content generation. Not slop. I hope you are experiencing the difference.</p><p>Language is a technology. This is easy to forget because we are born into it, but it is one of the oldest and most sophisticated tools humans have ever developed — and some people have a mastery of it that I do not. My spelling is terrible. My punctuation, left to my own devices, is creative at best. But I know how I want something to feel. I know how I want it to land. And a large language model is not magic. It is human language, compressed. Billions of words written by people, distilled through neural networks into mathematical weights and probabilities — patterns of how language follows language, learned at a scale no individual could achieve. When I give it instructions drawn from my own writing, I am not outsourcing my voice. I am using the largest pattern-recognition engine ever built to help me stay faithful to patterns I already own but cannot always execute alone.</p><p>That’s what the instructions capture. Not rules. Sensibility. The pattern is the structure. What emerges — if the pattern is well crafted — is the quality without a name.</p><p>/</p><p>I was listening to the<a href=""https://www.youtube.com/watch?v=_1KQRPtgiM0&amp;list=PLB9gMmtMLXxsa8C0PzHFL2tJFh7FrKrYD""> Hard Fork podcast</a> recently and they covered romance novel writers using AI. New York Times reporter Alexandra Alter talked about how the explosion of AI-assisted books has made one thing obvious: you need very specific instructions if you want anything convincing for the genre. There are tells. Phrases the models default to when you don’t guide them. AI is still bad at human emotion, at nuance, at the art of the slow burn. Apparently “he said her name like a ragged prayer” appears in book after book — a phrase the model’s weights favor heavily, surfacing again and again across novels by different AI-assisted authors. One prolific romance writer turned instructor, Carol Hart, tells her students to always add the LLM instruction: <em>make it slow and agonizing. Do not rush to the finish.</em></p><p>This made me laugh, but also made sense. The unguided, uninstructed output has a specific lack of texture. You can feel it. Competent and smooth and saying nothing of depth. Falling back on the same tricks. No quality without a name. It’s the architectural equivalent of a building that follows code but has no life in it — technically correct, humanly dead. Alexander spent his career fighting exactly this. The pattern exists to prevent it. The instruction exists to prevent it.</p><p>The craft is in the specificity. In the personal. In the particular.</p><p>Katy Neale, a UX designer who writes about the intersection of AI and practice, recently articulated this from the product design side. In<a href=""https://medium.com/@katyneale/prompt-engineering-is-the-new-ux-skill-b3a58cb00e14""> her piece on Medium</a>, she argues that prompt engineering is less a technical skill than a design skill — that it requires the same kind of user-contextual thinking designers already do, just applied to the AI system itself: “We’re not just learning a new tool — we’re discovering a fundamental skill that will define how humans interact with technology for decades to come.” She’s right. But I want to push it a little further: we’re not just communicating with the machine. We’re learning to articulate ourselves.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*4JRJlDprL3QfPB0zl0SuZA.png"" /><figcaption>Collage includes: <em>A few example Topics with their corresponding Descriptions and Instructions from testing on a fictional shipping company called “ShipIT.”</em> <em>Image credit: Original documentation created by the author for AI testing purposes.</em></figcaption></figure><p>/</p><p>As you might imagine, I am finding deep resonance between this personal writing practice and my day job. I lead the design of an AI agent for customer service representatives. It helps reps resolve cases by generating step-by-step service plans grounded in a company’s actual policies, knowledge base, and case data. The quality of those plans depends almost entirely on one thing: the quality of the instructions.</p><p>In the system we’re working within, you write <em>topics</em> — categories that tell the agent what kind of case it’s looking at — and <em>instructions</em> — the specific guidance the agent should follow. Add a skill and those intructions can take actions. A topic might be “Return Request.” An instruction might be: “Make sure the customer has provided the required information, including a valid return date, customer name, and receipt or order number.” Another: “If the customer provides the order number, then search the return in the Order Management System using the order number and return date.” Simple language. Clear conditional logic. One task per instruction. If this, then that. Always, must, verify.</p><p>What we’re asking enterprise customers to do is exactly what Alexander proposed: build a pattern language. Each topic is a category. Each instruction is a pattern — a recurring problem paired with its solution, expressed clearly enough that anyone — or any reasoning engine — can apply it, but specific enough that what emerges is grounded in that company’s actual reality.</p><p>What I didn’t expect was what happens to companies in the process of writing those instructions. The act of articulating how your business works — clearly enough for a reasoning engine to follow — is a form of forced introspection. Organizations discover that processes they assumed were well-defined actually live in someone’s head, or differ team to team, or were never documented at all. You try to tell an agent how to handle a return and realize there <em>is</em> no documented policy for the edge case. You write an instruction for escalation and discover that different offices escalate differently and nobody has reconciled the approaches.</p><p>This connects to something knowledge management researchers have been tracking for decades: what Polanyi called tacit knowledge is precisely what organizations lose when experienced employees leave, and precisely what AI instruction-writing forces them to surface. Writing instructions isn’t just configuration. It is, as one research framework puts it, <em>externalization</em> — converting what has lived in practice into something that can be shared, reviewed, and refined. The same way extracting my writing patterns made my craft visible to me, writing AI instructions makes a company’s operational logic visible to itself.</p><p>And the testing is where the craft deepens. The word “artisan” has started circulating in conversations about developers working with AI agents — and it resonates. You write instructions, test them against hundreds of simulated interactions, review where the agent fails or drifts, trace its reasoning, refine, test again. It’s iterative and nondeterministic — you cannot predict the output the way you can with traditional software, because these are reasoning systems, not rule engines. Success requires a disposition toward experimentation, toward relationship with the system rather than control of it.</p><p>And like Alexander’s patterns, the best instructions generate something that exceeds them. A dynamic service plan that evolves in real time as new information arrives — creating resolution steps on the fly, checking whether an action can be automated, falling back to human guidance when it can’t. The instructions provide the structure. But the quality without a name — that lives in the encounter itself. How the customer experiences the company. How they feel treated. How what started as a support interaction begins to feel like relationship.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*SuFqHJNASOGRrhdWcsRcVQ.png"" /><figcaption>Collage includes: <em>Diagram showing the flow of information from a voice call through an AI reasoning engine, through protocol identification, to the AI agent generating a service plan for the representative — with Ryoji Ikeda’s “data-verse 1” (2019) layered underneath.</em> <em>Image credit: Original process diagram by the author; Ryoji Ikeda, “data-verse 1,” 2019. Image of artwork used for illustrative/editorial purposes.</em></figcaption></figure><p>/</p><p>B. Joseph Pine II — the same thinker who gave us <em>The Experience Economy</em> — argues in his most recent work that we are entering what he calls the Transformation Economy. Goods, services, even memorable experiences are no longer enough. Customers want to change. They want to become healthier, more knowledgeable, more capable. They want to reach their aspirations. The highest form of economic value, Pine argues, is guiding that transformation.</p><p>I’ve been sitting with this idea because it sharpens everything I’ve been describing. If Pine is right — and I think he is — then the bar for enterprise instructions rises dramatically. It’s no longer enough to resolve a case or close a ticket. The instructions need to create structure for personalized experiences that move beyond traditional support. The interaction between a brand and its customer — at the point of contact, in the moment of need — becomes an opportunity not just for resolution but for growth.</p><p>This is where I think instruction-writing becomes intellectual property.</p><p>Just as I understand the mechanics of how I write — because I know how I want it to land — companies can understand how they want their brand to be experienced and begin to codify that into instructions. We used to have knowledge articles and training manuals. Static documents. One-size-fits-all. Now we have instructions that generate dynamic, adaptive experiences — addressing a specific person, a specific problem, in a specific moment. The interaction is repeatable in structure but unique in execution.</p><p>The companies that invest in the quality and depth of their instructions will differentiate themselves. Their instructions will reflect how deeply they understand their own customers, their own processes, their own values. That understanding — encoded into patterns clear enough for AI to follow but rich enough to feel alive — becomes a form of brand. Not brand as logo or tagline, but brand as experience. The felt quality of interacting with an organization that knows itself and knows what it wants for the people it serves.</p><p>The instruction set will be the IP. The craft of writing it will be the competitive advantage.</p><p>/</p><p>There’s a flip side. The explosion of content — writing, agents, messages, recommendations — is real. More people will use these tools, more output will flood every channel, and the question of how we digest it all doesn’t have an answer yet.</p><p>But I feel good about being able to formulate my thoughts in a way that feels like me. It’s using a technology built from human language to amplify the intelligence I already have. To carry ideas I would have lost in the noise of daily life. To maintain a practice of thinking that the full weight of living had nearly crowded out.</p><p>Alexander’s first book was called <em>Notes on the Synthesis of Form</em> — rigid, scientific, prescriptive. By the time he wrote <em>The Timeless Way of Building</em>, he’d moved toward something softer, more alive. He’d realized that the pattern was necessary but never sufficient. That the life in a building — or a sentence, or an AI-aided support interaction — comes from somewhere the pattern points to but cannot reach.</p><p>The craft of the instruction is the same. It’s the pattern you build so that something beyond the pattern can emerge. Know your own patterns. Name them. Refine them. Own them.</p><p>The craft is always yours. The instruction helps you remember.</p><p><em>This article was written with Claude using writing style instructions developed by analyzing previous work. I also drew on</em><a href=""https://uxdesign.cc/a-ux-designer-guide-to-prompt-f045cd839489""><em> Paz Perez’s guide to prompting for UX designers</em></a><em> published in UX Collective,</em><a href=""https://medium.com/@katyneale/prompt-engineering-is-the-new-ux-skill-b3a58cb00e14""><em> Katy Neale’s writing on prompt engineering as a UX skill</em></a><em>, and the</em><a href=""https://www.youtube.com/watch?v=_1KQRPtgiM0&amp;list=PLB9gMmtMLXxsa8C0PzHFL2tJFh7FrKrYD""><em> Hard Fork podcast episode on AI-assisted romance writing</em></a><em>. Christopher Alexander’s pattern language framework is drawn from A Pattern Language (Oxford University Press, 1977) and The Timeless Way of Building (Oxford University Press, 1979). Michael Polanyi’s concept of tacit knowledge is from The Tacit Dimension (Doubleday, 1966). B. Joseph Pine II’s transformation economy framing draws from his ongoing work following The Experience Economy (Harvard Business Review Press, 1999).</em></p><p><em>Graphic collage inspiration: </em><a href=""https://laurasullivancassidy.com/""><em>Laura Cassidy</em></a><em> from </em><a href=""https://grieversball.substack.com/""><em>Grievers Ball.</em></a></p><p><em>Very open to feedback and hearing about your own instruction-writing practice — personal or enterprise.</em></p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f0da34c445cb"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/the-craft-of-the-instruction-f0da34c445cb"">The craft of the instruction</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/field-study-prototypes-over-mockups-8581f20102ff?source=rss----138adf9c44c---4,1771589634,Field study: prototypes over mockups,"Field study: prototypes over mockups

<h4>A practical guide to designing with code in 2026</h4><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*KRmA-I3ubGIeZ53xNAH8lA.png"" /></figure><p>Someone drops a link in a thread — not a deck, not a Figma file — something you can click through, interact with. The conversation shifts from opinions to behavior. This keeps happening <a href=""https://dust.tt/"">at Dust</a>. We’ve been experimenting with making prototypes our default design artifact. The question driving us:</p><blockquote><strong>What should designers produce to help teams decide faster while raising the quality bar and reducing implementation waste?</strong></blockquote><p>We don’t have a final answer yet. This article is a field report: what we’ve tried so far, what seems to work, and what still feels unresolved.</p><h3>Running a design project at Dust</h3><p>I’ll describe the workflow in the order it usually happens. It’s not a rigid process — more a default path we’ve been trying.</p><h4>From “thinking” to “making it real”</h4><p>After the initial analysis and quick sketchbook phase, when I need to give the idea shape and pressure-test it, I don’t open Figma.</p><h4>Create a playground</h4><p>I open my development environment pull the latest version of our repo, and create a branch. Then I ask an agent to scaffold a new prototype, and I describe what I’m trying to make.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*DRbolDACpMR1V_In70LESw.png"" /></figure><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*DOcoZx7cEzBj2DpKU2GhqQ.png"" /><figcaption>Our playground environment. Nothing fancy. Each entry is a story.</figcaption></figure><h4>Iterate on behavior, not pictures</h4><p>At this point I mostly care about trying the idea and seeing whether the interaction holds. I’ll build small flows, prototype the transitions, and sanity-check the parts that static screens often hide (state changes, error cases, motion, empty states, keyboard/navigation/accessibility basics).</p><p>I can very easily use realistic fake data. We’ve pre-generated a bunch of it (people profiles, pictures, names, emails, lists of files and folders, entire conversations…). If I need new ones, it’s a matter of seconds to generate them.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*tON3pEqQar-RiXwxitNaIg.png"" /></figure><p>Everything runs in the browser, so I’m always looking at the experience at 1:1 scale — at user level. What I see is what user gets.</p><h4>Use the design system by default, bend it when needed</h4><p>During that phase, I don’t optimize for visual polish. I start with behavior and structure; the styling can catch up once the flow holds. The agent naturally builds on our design system (Sparkle), so the prototype reuses our components and tokens from the start.</p><p>If I need a new component or a tweak, I’ll do it on the branch — and only later decide what should become “real” design-system work vs. what was just local exploration.</p><a href=""https://medium.com/media/56ec6445356d38c3fe402d4ed71afcff/href"">https://medium.com/media/56ec6445356d38c3fe402d4ed71afcff/href</a><h4>Share early</h4><p>Sharing is simple: I push the branch to GitHub and open a PR. My prototype auto-deploys, I can share an easy-to-open URL. GitHub is a natural environment for engineers, so feedback happens where the work lives.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*DrhjiRCYmzZAGm4-Ph7BWw.png"" /><figcaption>Automatically generated deploys of storybook and playground.</figcaption></figure><h4>Converge: reduce the diff</h4><p>As iteration loops tightens and I get closer to something final, I start paying attention to:</p><ul><li>introducing just the right new component (and no more)</li><li>making the smallest necessary change in Sparkle</li><li>cleaning up accidental modifications</li></ul><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*YJRrxMU_oCIH4gt-B8WfTQ.png"" /></figure><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*zDHU4GLC-2xydHqT0D8OfA.png"" /><figcaption>Component generated, added to the design system, documented in storybook.</figcaption></figure><h4>Handoff through the PR</h4><p>Engineering handoff is straightforward: it happens through the PR.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*mJY1WVDqYtld8WpZcJLHBA.png"" /><figcaption>Design system updates and prototypes.</figcaption></figure><p>I can point to what changed, what needs refining in the design system, and what is “prototype-only”. Engineers can reuse components directly, and use the prototype code as reference — because it runs on the same stack, with the same components.</p><h3>Shortcomings</h3><p>Before going into the setup, a few honest trade-offs we’ve run into:</p><ul><li><strong>It’s still a sandbox.</strong> Unless we simulate it, it won’t naturally reproduce real latency, loading states, and backend weirdness.</li><li><strong>Feedback isn’t perfectly smooth</strong>. Vercel’s preview comments can help, but it’s been a bit flaky for us — screenshots + Slack are still the default.</li><li><strong>You occasionally pay the “code tax.”</strong> Most of the time this is faster than mockups, but occasionally you lose 30 minutes to a silly environment or tooling issue. Quite often, the prototype reveals the brief itself is still fuzzy — or that what we asked for isn’t feasible yet.</li><li><strong>It can invite premature polish</strong>. Because it looks real quickly, it’s easy to get dragged into nitpicks before the core interaction is settled.</li><li><strong>Hand-off clarity</strong>. It’s not always clear with engineers what they should directly re-use and what is prototyping.</li></ul><h3>The Setup</h3><p>Dust runs on React, in a monorepo. Our design system (Sparkle) lives there too. Sparkle gives us two complementary places to work:</p><ul><li><a href=""https://storybook.js.org/""><strong>Storybook</strong></a><strong> </strong>for building and documenting components in isolation (variants, interactions, visual regression). It’s our component catalog.</li><li><strong>Playground</strong> is just a small Vite app nested inside Sparkle. It is a fast development server, it starts quickly, refreshes instantly as you edit, and stays lightweight.</li></ul><p><strong>The key point</strong>: both environments consume the same Sparkle source and Tailwind styles, so prototypes reuse real components and tokens by default.</p><h4>Vercel for easy sharing</h4><p>Vercel is a cloud platform that automatically deploys a GitHub branch as a shareable, live URL. For us, if the branch name contains sparkle, Vercel auto-deploys both Storybook and the Playground and posts preview links in the PR — so sharing a runnable prototype is basically instant.</p><p>One thing we haven’t tried yet: bringing real AI interactions into prototypes — streaming responses, agent behavior, conversational flows. Since Dust is an AI product, that’s a meaningful gap in our current setup. Vercel’s serverless functions could be a way to close it.</p><h4>Coding tools</h4><p>The beauty of this setup is that almost any AI coding tool will work. <strong>Claude Code</strong>, <strong>Antigravity</strong>, <strong>Codex</strong> . My tool of choice is often <a href=""https://cursor.com/agents""><strong>Cursor</strong></a><strong> </strong>(for prototyping), but I move between them freely.</p><p>Notable gap: <strong>Lovable</strong>, which I heavily use and love for personal projects, isn’t a good match here — yet. For good simplicity reasons, it doesn’t handle repos as complex as ours or support the kind of professional-grade git collaboration we need.</p><h4>What about Figma?</h4><p>Don’t believe the catchy “RIP Figma” headlines. Figma hasn’t gone anywhere for us. We still use it upstream for mapping journeys, sketching flows, and running brainstorms; downstream for managing visual assets and illustrations.</p><p>It’s taken a back seat in the middle: the step where you translate an idea into something concrete enough to evaluate. It helps that we never treated Figma as the source of truth for product or design system — <em>truth is in the code</em>. That’s a huge potential efficiency win when you flip to prototyping first.</p><p>A note on Figma Make: we haven’t been convinced by it compared to our own setup. Make is designed to generate code from mockups — which makes sense if mockups are your starting point. But if you flip to prototyping first, it doesn’t play well with an existing codebase.</p><h3>The question underneath: should designers work in code?</h3><p>Behind all the talking is the old debate of technical literacy in design. Should designers code? To what extent? Here’s my current take.</p><h4>Belief #1 — Technical literacy is a design skill</h4><p>I’m an industrial designer by training. In industrial design, understanding your materials is not a “nice to have”. If you don’t know how plastic bends, how aluminum behaves, or what manufacturing constraints do to a shape, you design objects that look right in 3D but fail in the hand.</p><p>Digital products are built with a material too: code. Technical literacy does not necessarily mean coding, but it means understanding how a digital system behaves and why.</p><h4>Belief #2 — Designers working in code can improve quality and speed</h4><p>Having designers work in code can be a real accelerator: tighter feedback loops, fewer handoffs, and less drift between what we design and what ships.</p><p>The catch is that this approach doesn’t scale by default. As organizations grow, the product surface expands, the codebase gets more complex, and the friction adds up. Recruiting designers who can both design and contribute significantly to production code — at scale — is hard to rely on as a core strategy.</p><h4>Belief #3 — “Designers coding” cost-benefit has flipped</h4><p>AI is changing the game on three axes:</p><ol><li><strong>It’s easier.</strong> The “need to know” is shrinking and the cost of learning is going down fast.</li><li><strong>It’s faster.</strong> What used to take hours takes minutes. Once you have a runnable baseline, you can explore variations, states, and interactions at a speed that makes static mockups feel like overhead.</li><li><strong>It’s more powerful.</strong> The amount you can build with limited ability is remarkable — full interaction flows, working components, realistic prototypes.</li></ol><p>I won’t pretend this requires zero knowledge. It doesn’t. But the cost-benefit has flipped: what you can do, and how fast you can do it, makes it hard to justify not putting in the effort to acquire the minimal skills required.</p><h3>What does the future look like?</h3><p>The next step for us is bridging the gap between prototypes and implementation. Today, designers rarely make changes directly in production code . Running a local instance of Dust is heavy, and pushing to a test environment is too slow for tight iteration.</p><p>This is changing extremely fast. More on this soon — but if you’re curious, here’s <a href=""https://dust.tt/blog/engineering-at-the-speed-of-ai"">a technical preview of what we’re working on</a>.</p><h3>Closing</h3><p>I keep coming back to a longer historical loop. Design as a distinct practice really took shape when mass production separated roles that used to live in a single person: the craftsperson who could think, make, and sell the object. Once manufacturing scaled beyond the workshop and those roles split, you needed something that could travel between people and disciplines.</p><p>That something wasn’t just a pretty shape. It was a <em>dessein</em>: a drawing, a plan, a set of intentions constrained by technical reality. In other words: a model of the object, expressed in a format compatible with production.</p><p>Digital product design inherited that logic. Static mockups, specs, flows — they’re all versions of the same move: translating intent into a plan that others can implement.</p><p>But when coding (and prototyping) becomes more accessible we’re not only getting faster at producing plans. <strong>We’re regaining the ability to model directly</strong>.</p><p>More like clay than drafting: you shape, you test, you feel, you adjust — with an instantaneous feedback loop. The artifact is no longer a description of the thing. It starts to become the thing, or at least a runnable slice of it.</p><p>Design is moving from planning experiences to modeling them — closer to reality, earlier, and with tighter feedback loops.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8581f20102ff"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/field-study-prototypes-over-mockups-8581f20102ff"">Field study: prototypes over mockups</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/are-we-performing-ourselves-into-exhaustion-0e41da7d7652?source=rss----138adf9c44c---4,1771436232,Are we performing ourselves into exhaustion?,"Are we performing ourselves into exhaustion?

<h4>On self-surveillance, professional performance, and the cost of forgetting that we built the cage.</h4><figure><img alt=""A translucent mask held in hands with a blurred figure in the background, suggesting the separation between performance and identity."" src=""https://cdn-images-1.medium.com/max/1024/1*_gP_nYArWNMnQl8l2rAojQ.jpeg"" /><figcaption>Is this one me? (All conceptual images in this article were generated by the author using AI.)</figcaption></figure><p>As designers, we live performance twice over: as users who perform constantly, and as the creators of the systems that encourage that performance. This article is about the first, but it inevitably speaks to the second.</p><p>I spent forty minutes choosing a profile picture. Forty minutes taking selfies, testing angles, adjusting light, cropping, thinking about how I wanted to be seen. My professional dignity is somehow correlated to how well I can camouflage my aging. For people I’ve never met. Who may never see it.</p><p>While testing angles, I thought: When did this become normal? Then I did something even worse. Or more practical. Or both — I can’t tell the difference anymore.</p><p>I used an AI tool to “enhance” the photo. Adjust the lighting. Add a studio background. Give it that more “professional” look.</p><blockquote>I literally asked an algorithm to tell me what my face should look like.</blockquote><p>And the most depressing part wasn’t doing it. It was that it worked. The photo was “better.” More presentable. More performative.</p><p>I learned this from an influencer with millions of followers. He shows it with data: posts featuring his face generate 40% more engagement. More likes, more comments, more shares. Translation: more visibility, more clients, more revenue. Except you don’t have time to produce 30 different photos a week. So he uses AI to generate headshots of himself in any context. Very practical.</p><p>If you don’t do it, you’re losing reach. If you’re not doing it now, your competitors already are.</p><p>The logic is impeccable.</p><p>And I tested it. Because the data was clear. And anyone who seriously studies metrics has been doing this for months.</p><p>We went from practical to practical. And we distorted ourselves along the way.</p><h3>The promise that worked</h3><p>To be fair: personal branding was born as a promise of freedom.<br />Before, you depended on gatekeepers. Editors deciding whether your writing deserved to exist. HR deciding whether your résumé was “adequate.” Agencies deciding whether you were “relevant enough.” Your competence only mattered if someone with power gave you permission to show it.</p><p>The internet changed that.</p><p>Designers sharing their process on Behance and landing clients on the other side of the world. Writers publishing directly, without a publisher, building real audiences. That brilliant but shy professional who never shone at in-person networking is finally able to let the work speak for itself.<br />And it worked.</p><p>You wouldn’t be reading this if that weren’t true. A substantial part of those without a platform finally gained a voice. Many voices can now be heard, and that is a humanitarian gain. Opportunities that simply wouldn’t have existed 15 years ago became possible.</p><p>Personal branding was empowerment: controlling your own narrative and deciding who you are, instead of accepting the box they put you in.</p><p>But then the market noticed it was working.</p><blockquote>And did what markets do best:<br />systematized, optimized, monetized, and turned it into an obligation.</blockquote><figure><img alt=""Hand adjusting a translucent mask against a soft-focused background, depicting the continuous refinement of professional performance."" src=""https://cdn-images-1.medium.com/max/1024/1*PFmLv0kt5vVpF3k0pPh7_A.jpeg"" /><figcaption>Almost natural. Just one more adjustment.</figcaption></figure><h3>When empowerment became compulsory performance</h3><p>“Show your work” became “perform expertise 24/7.”</p><p>“Be authentic” became “authenticity as a brand strategy — here are 10 templates.”</p><p>“Build an audience” became “monetize every second of attention or you’re wasting your potential.”</p><p>And “follow these steps” became the promise that you might finally be seen in an ocean of people competing for attention.</p><p>What was optional became mandatory to survive.</p><p>And again we bought in. Because it works. Those who perform better grow faster. Those who aren’t seen aren’t remembered. Those without a personal brand have no market.</p><p>So you and I performed. And we performed well.</p><h4>We became the product</h4><p>We became a Coca-Cola that needs a logo, a tagline, an editorial calendar, a conversion funnel, alignment with causes, and constant optimization.<br />And now, an increasingly common strategy: turn on the camera. Reels. Stories. Short videos. For some, text is no longer enough. Show your face in motion. Speak directly. Rehearse spontaneity twelve times until it looks natural.</p><p>That uncomfortable task of selling yourself publicly. Of talking alone to a camera. Of feigning naturalness while repeating the same hook three times until it sounds “authentic.”</p><p>The logic is clear: the algorithm prioritizes video, “people trust people.” Showing your face has become synonymous with credibility.</p><p>So they record. Edit. Re-record. Post. And hope those fifteen seconds of filmed discomfort generate the numbers that justify the effort.</p><blockquote>They called it “personal branding.” As if you were a product. We objectified ourselves.</blockquote><p>And what does refusing mean? Becoming invisible in a market where visibility has become a condition of existence.</p><p>We show the quality of our work and what makes us unique — but through so many layers of optimization and strategic performance that the line between substance and packaging begins to disappear. We sell edited versions of ourselves — myself included — for the market to consume.</p><h4>It’s a silent escalation</h4><p>If only this were just a problem of photos.</p><p>It has migrated into everyday communication. That email with eight people copied? You know exactly what happens. Heightened attention. Every word calculated. You’re not writing to communicate — you’re performing competence for an audience you imagine judging every comma.</p><p>And we find ourselves wondering: How will so-and-so read this? What if someone takes it the wrong way? Does this sentence sound confident or arrogant? Is this tone collaborative or passive?</p><p>We edit.</p><p>Re-edit.</p><p>Save the draft.</p><p>Come back.</p><p>Change one word.</p><p>Send.</p><p>And breathe.</p><p>One more theater of efficiency.</p><figure><img alt=""A sharp, in-focus mask in the foreground with a blurred human figure behind it, showing the mask becoming clearer than the person."" src=""https://cdn-images-1.medium.com/max/1024/1*aTGFWrN1Hsbecodms47WEg.jpeg"" /><figcaption>I definitely look better this way.</figcaption></figure><h3>The mask that became a face</h3><p>At work, the performance is even more explicit. We have to put on the mask of “I’m the most incredible person you have.” Nothing more than structural survival.</p><p>If you don’t show up, you’re not remembered when promotions come around. Visible performance matters more than silent work. The clarity of your reports ends up defining your perceived reliability.</p><p>So we send weekly reports that are 70% real work, 30% personal marketing. “Key achievements this week,” we write, as if we’d won the space race. That presentation where you rehearsed your “spontaneous contribution” multiple times to impress the people who matter. That calculated moment of waiting a few minutes before responding on Slack, fast enough to seem available, but not so instant that you look desperate or like you have nothing real to do.</p><h4>And it works. Of course it works.</h4><p>Managers love employees who are clear, proactive, communicative. Organizational efficiency improves. Engagement goes up. Everyone wins.</p><p><strong>Does it, though? What’s the cost?</strong></p><p>The cost is invisible but real. Time spent editing messages instead of thinking through the problem that actually needs solving. Mental energy wasted anticipating imaginary judgments. The constant anxiety of being misunderstood or forgotten.</p><p>Professional decisions made based on “what will look good,” until you forget what actually matters.</p><p>Because beneath all this performative efficiency, there’s exhaustion. A permanent sense of inadequacy. You never sent all the updates you should have. Never participated in all the strategic meetings. Never read all the messages or the endless email threads. Never took enough initiative in company-wide meetings. Never had enough visibility to be remembered when the promotion came.</p><p>And the more you try, the more you confirm that <strong>you’re not enough</strong>.</p><figure><img alt=""Two hands pressed against a translucent barrier from inside, palms open, depicting confinement in a self-built prison."" src=""https://cdn-images-1.medium.com/max/1024/1*c0DUYalI8FDA_Lcx8JcVGA.jpeg"" /><figcaption>What was the way out again?</figcaption></figure><h3>Voluntary prisoners</h3><p>We built our own prison. There is no captor. No explicit coercion. But there are invisible bars, internalized, real. Bars made of constant visibility, of public metrics, of social validation as the currency of professional survival.<br />And we built them. Post by post.</p><p>It’s a practical necessity. Your career and market relevance depend on it. You perform without even noticing. Until you lose the ability to distinguish what you want from what you need to demonstrate.</p><p>I say this as someone who has already built a platform. Portfolio, clients, margin. What about those trying to enter? There’s no “before performing.” You start already performing. The prison has floors. Some think about leaving. Others, about getting in. That doesn’t invalidate the critique. It just situates who has the margin to make it.</p><h4>Nothing worse than becoming a voluntary enforcer</h4><p>That colleague who delivers exceptional work but doesn’t post their process? “They need to learn to communicate better.”</p><p>That designer who doesn’t update their portfolio every month?<br />“Doesn’t know how to sell themselves.”</p><p>You’ve internalized the logic so thoroughly that you now police it for free.<br />And when someone criticizes the system, we feel indignation. Almost a personal attack.</p><p>“Well, I grew because of it. You’re being naive; this is just how the world works now.”</p><p>Someone pointed at the prison, and we explained why the bars are necessary.</p><figure><img alt=""A heart shape drawn on a foggy glass barrier with hands visible below, representing affection directed toward one’s own confinement."" src=""https://cdn-images-1.medium.com/max/1024/1*mCfzlDWITl7GOUI6JwHReQ.jpeg"" /><figcaption>Privileged to be here.</figcaption></figure><h3>And we’re still grateful</h3><p>The results are real. That client from networking, that promotion, they happened. Visibility works. For those who need to build material security, this isn’t superficial.</p><p>The problem arises when we lose the ability to distinguish: am I doing this because I chose to, or because I can no longer imagine any other way of existing professionally?</p><p>And when we achieve something?</p><p><strong>Genuine gratitude, or a performance of contentment?</strong></p><p>I don’t even know anymore.</p><p>So we post about gratitude. About the “privilege of doing what you love.” About lessons from the journey. About how our team is “the most incredible family in the world,” how the company culture “transforms lives.”</p><p>#grateful.</p><h4>The moment identity disappears</h4><p>But there comes a moment, sooner or later, when that thought surfaces.<br />“That opinion I posted… do I actually believe it, or was it because it would generate engagement?”</p><p>“That project I chose… did it interest me, or did it just make better content?”</p><p>“That moment of introspection…</p><p>Did I need to think?</p><p>Or was I already performing introspection?”</p><p>And maybe we no longer know. We’ve performed for so long that we don’t know who we’d be without it.</p><figure><img alt=""A blurred interface or document visible through translucent glass with a human figure observing it, symbolizing designers viewing their own creations."" src=""https://cdn-images-1.medium.com/max/1024/1*tnPAz5xqu5ndwIUuZIv8Ug.jpeg"" /><figcaption>Ready for personal use.</figcaption></figure><h3>And who designs all this?</h3><p>Us. Designers, UX, product managers.</p><p>I’m not saying we do it in bad faith. Most genuinely believe they’re “improving the experience.” But let’s be honest about what that means:<br />Public validation metrics — views, likes, shares — so you know in real time exactly how well you’re performing.</p><ul><li><strong>The “seen” receipt <br /></strong>It obligates you to respond immediately, or perform an excuse.</li><li><strong>“Typing…”<br /></strong>It makes you rewrite your message knowing they can see you typing.</li><li><strong>Who viewed your profile<br /></strong>Sold as a premium feature. Monetizing paranoia.</li><li><strong>Your profile is 73% complete<br /></strong>Amanufactured sense of being unfinished.</li><li><strong>Seen by 47 people<br /></strong>Paranoia about who saw it and said nothing.</li><li><strong>Active now</strong> <br />Logging in as a public declaration of presence.</li></ul><p>Interfaces designed to manipulate decisions. Features that create behavioral dependency. Architectures that hijack your time.</p><p>All of this has a name. <a href=""https://www.deceptive.design/""><strong>Dark patterns</strong></a>. <strong>Addictive design</strong>. <strong>Attention capture</strong></p><p>Every discomfort you feel was engineered.</p><h4>And it worked. Better than we imagined.</h4><p>We built systems so efficient that they now face litigation. <a href=""https://www.npr.org/2026/01/27/nx-s1-5684196/social-media-kids-addiction-mental-health-trial"">Meta and YouTube for addictive design</a>. <a href=""https://www.ftc.gov/news-events/news/press-releases/2025/09/ftc-secures-historic-25-billion-settlement-against-amazon"">Amazon fined for impossible cancellation flows</a>. <a href=""https://www.digital-fairness-act.com/"">The European Union classified 97% of the most popular App Store applications as containing dark patterns</a> and began regulating the sector.<br />But when you’re on the product team, the metrics make sense. Engagement up 40%. Retention improved. We celebrate. Bonuses. A success case for the portfolio.</p><p><strong>We are, unfortunately, optimizing addiction.</strong></p><figure><img alt=""A hand against a barrier with intersecting lines or wires overlaid, representing interconnected surveillance and tracking systems."" src=""https://cdn-images-1.medium.com/max/1024/1*s7-C6NFcoyM8HzULNmyTyA.jpeg"" /><figcaption>I have bad news.</figcaption></figure><h3>The inevitable convergence</h3><p>This constant performance, this voluntary self-optimization, this prison we built… The corporate market is systematizing all of it. Developing tools that don’t just observe your performance — they help you perform better. With data, insights, algorithmic empathy.</p><blockquote>And the worst part: we’ll accept it. Because refusing means falling behind.</blockquote><p>The pieces already exist. <a href=""https://www.bbc.com/news/business-68067022"">Amazon uses tracking systems — wristbands and scanners that monitor warehouse workers’ productivity every second</a>, generating “idle time” metrics. Microsoft Viva analyzes your communication patterns and productivity. Humanyze monitors tone of voice and body language in interactions. Insurance companies offer discounts in exchange for health data.</p><p>In isolation, each seems reasonable. But when they talk to each other?<br />You stay late working on an urgent proposal. Your laptop logs nighttime productivity. Your smartwatch logs insufficient sleep. Your calendar shows six meetings in one day.</p><p>The system infers: “Problematic time management. Procrastination during working hours. Burnout risk.”</p><p>You probably won’t be consulted. The system will build a behavioral profile you’ve never seen, but that your manager can access. That interface is likely already being built by talented designers and engineers, perhaps even well-intentioned ones, who believe they’re solving real problems.</p><h4>And how will it be sold?</h4><p>“Empowerment through data.”</p><p>“Enhanced self-awareness.”</p><p>“Personalized support.”</p><p>The messaging will be empathetic, careful, almost moving.<br />But you didn’t define what “healthy” looks like. They did. You didn’t choose to be monitored. You just accepted it, because refusing would mean “not caring about your own development.”</p><p>It’s the same logic as the profile picture.</p><p>Every step is perfectly rational. Supremely practical.</p><p><strong>And we keep distorting ourselves.</strong></p><figure><img alt=""A person’s hands held together in contemplation, clearly visible without barriers, suggesting awareness and presence outside of performance."" src=""https://cdn-images-1.medium.com/max/1024/1*Y5ddbSG-7Q71-zTZ8h_vrA.jpeg"" /><figcaption>Here I am.</figcaption></figure><h3>There any redemption?</h3><p>I don’t believe the solution is deleting your account or retreating to the woods. And you probably won’t stop performing — I won’t. The game exists. Refusing to play is a privilege most people don’t have.<br />But there is a difference between performing with the awareness that it’s theater, and believing that the theater is you.</p><p>If you need to edit the photo to get in the game, that’s fine. A diplomat wears a suit, a doctor wears a white coat; you wear a profile picture. We put on masks because the world demands it. This is ritual, contingent on the moment we’re living in. It is neither eternal nor natural.</p><p>The problem was never the mask. It was forgetting you were wearing one.<br />I write this knowing that this very text participates in the same structure, it will be shared, and it will be performed. Awareness of the theater doesn’t take me off the stage. It only reminds me that I’m on it too.</p><p>But what makes us sick is not the performance itself, it’s the sustained lie. The manager who posts about human-centered culture while expecting replies at 11pm. The designer who celebrates inclusivity but never questions the product’s exclusionary metrics. The professional who performs gratitude while feeling nothing but exhaustion.</p><p>Awareness of the theater is what prevents moral dissonance. When you know it’s performance, you preserve mental energy for what truly matters. Your family. Your values. Your actual competence.</p><figure><img alt=""A person with hands in reflective pose, with a blurred screen or interface visible in the background, depicting conscious work and agency."" src=""https://cdn-images-1.medium.com/max/1024/1*uAr7q8SqhJ6CiUlqhZZeVA.jpeg"" /><figcaption>What if I changed this?</figcaption></figure><h4>And us, as designers?</h4><p>For us, creators of systems, designers, UX, product managers, there is always a gap.</p><p>A gap between “what the metric demands” and “what I know is right.”</p><p>A gap between “what will perform better in the A/B test” and “what won’t leave the user feeling manipulated afterward.”</p><p>The gap isn’t always navigable. There are moments when the system leaves no room, when the choice is between compromising the design or compromising the paycheck.</p><h4>But the gap exists.</h4><p>And culture is never static. It is always transforming. What holds value today becomes irrelevant tomorrow. What seems inevitable today, five years from now, is embarrassing.</p><p>Remember when smoking on planes was normal? When doctors recommended cigarettes? When asbestos was the “material of the future”? When tobacco advertising sponsored Formula 1? When children’s advertising had no limits? When drunk driving was socially tolerated? When no one wore a seatbelt?</p><p>I know, the damage of the attention economy is different. It’s diffuse, psychological, subjective. You can’t prove in court that the infinite scroll “caused” your anxiety the way cigarettes caused cancer.</p><p>So the change will be slower. More fragmented. It won’t be “we banned dark patterns.” It will be “some companies realize that trust is worth more than engagement, and that becomes a competitive advantage.”</p><p>Change doesn’t come overnight. It comes from accumulated small acts of resistance. From designers who, when they can, choose not to add that manipulative feature. From PMs who advocate for satisfaction metrics, not just time-on-platform. From companies that realize — late, but realize — that trust is worth more than predatory engagement.</p><h4>The market transforms.</h4><p>Fines, lawsuits, and regulations have already begun to appear. Companies are changing because it’s become too costly to stay the same. The market is extremely fluid and hates losing money.</p><h4>Perception transforms.</h4><p>Users already know they’re being manipulated. Teenagers already say “I know this is addictive, but I can’t stop.” Parents are already looking for basic phones for their children. Entire generations growing up distrustful of likes. Books like Jonathan Haidt’s The Anxious Generation and Johann Hari’s Stolen Focus are worldwide bestsellers.</p><p>None of this comes close to solving everything. But it creates space.<br />It creates conditions for products to function without depending on addiction. For professionals to build a presence without constant theatricality. For the process — not just the polished result — to hold recognized value.</p><p>And when designers, creators, and builders of systems begin to see our own complicity, the tools we create begin to change too.</p><p>Because we realize that systems that consume people eventually consume us as well.</p><p>Redemption, if it exists, is not individual. It is not “I saved myself, good luck to the rest.” It is collective, far slower than it should be, and full of setbacks.</p><p>But it is possible.</p><p>Small cracks in the system. Minimal gestures, repeated until they become culture. We stay in the game, but play it a little differently. A little more consciously.</p><p>And when we have the choice, we can choose to lie a little less.</p><p>Maybe it won’t change the world. But it keeps a piece of you intact.</p><p>And that, in itself, is already a beginning.</p><h4>To continue the conversation</h4><p>Thank you for following me through this reflection. This text is part of an ongoing inquiry into design, digital behavior, and identity, one that spills into the practice at my studio and the tools I build. If these ideas resonated, I’d love to continue the conversation.</p><p><strong>Where to find me</strong>: Substack · LinkedIn · <a href=""mailto:pedro@reinostudio.com"">pedro@reinostudio.com</a><br /><strong>Projects</strong>: Reino Studio · Talk to Amia</p><h3>References and further reading</h3><h4>I. The structure of self-surveillance</h4><p><a href=""https://www.amazon.com.br/Discipline-Punish-Prison-Michel-Foucault/dp/014013722X""><strong>FOUCAULT, Michel. Discipline and Punish: The Birth of the Prison (</strong>1975)</a><br />The concept of panopticism: when the prisoner doesn’t know if they’re being watched, they begin to watch themselves permanently. The warden becomes unnecessary because they have been internalized.</p><p><a href=""https://en.wikipedia.org/wiki/Byung-Chul_Han""><strong>HAN, Byung-Chul</strong></a><strong>. </strong><a href=""https://www.amazon.com.br/Burnout-Society-Byung-Chul-Han/dp/0804795096""><strong>The Burnout Society</strong> (2015)</a><br />The achievement-subject is not oppressed by another — they oppress themselves with greater efficiency than any external system could manage.</p><h4>II. Performance, theater, and identity</h4><p><a href=""https://www.amazon.com/Presentation-Self-Everyday-Life/dp/0385094027""><strong>GOFFMAN, Erving. The presentation of self in everyday life</strong> (1959)</a> Goffman proposes that all social interaction is performance, with front stage and back stage. He wrote this decades before social media — and described with precision exactly what they would do once they eliminated the backstage.</p><p><a href=""https://www.amazon.com/Society-Spectacle-Guy-DEBORD/dp/0934868077/ref=sr_1_1?crid=3QQH81X8H5F6&amp;dib=eyJ2IjoiMSJ9.ipVuVSR-KZ75gnFYEPz4oy8dlinEN1agXzPeL93S7w61QxfcdJ0LG8UdT4_vraJJq5kTaUQiuAf62R8IdpBAVynDw4TXpGZePXWifSEzqsGm5aE0rkOSmMLDglVFIrBrN7QfFiNYXSAMs-IbQscX28kah6_JwZ7vuQfFsx3FgFTrO6mEayQQ0YXkhhNk5xdbMoSGDS48iCRFC4hxIqyyfX92VeQTuSv2S6jc4jkCGoc.LG_PSMvRD0Zk-cRLIntj_mJCrZs9n6K2cIxnFd5-8b4&amp;dib_tag=se&amp;keywords=The+Society+of+the+Spectacle+%281967%29&amp;qid=1771373734&amp;s=books&amp;sprefix=the+society+of+the+spectacle+1967+%2Cstripbooks%2C196&amp;sr=1-1""><strong>DEBORD, Guy. The Society of the Spectacle (1967)</strong></a> <br />The spectacle is not a set of images: it is a social relation between people mediated by images.</p><h4>III. The design of attention and its mechanisms</h4><p><a href=""https://pt.wikipedia.org/wiki/Shoshana_Zuboff""><strong>ZUBOFF, Shoshana</strong></a><strong>. </strong><a href=""https://pt.wikipedia.org/wiki/Shoshana_Zuboff""><strong>The Age of Surveillance Capitalism</strong> (2019)</a> <br />How platforms transformed human behavior into raw material for prediction and modification. The economic logic behind every feature designed to create anxiety.</p><p><a href=""https://www.deceptive.design/""><strong>BRIGNULL, Harry. Deceptive Design</strong> </a><br />The researcher who named and catalogued dark patterns in 2010. The site maintains a living archive of deceptive patterns identified in real products.</p><p><a href=""https://www.edpb.europa.eu/system/files/2023-02/edpb_03-2022_guidelines_on_deceptive_design_patterns_in_social_media_platform_interfaces_v2_en_0.pdf""><strong>EUROPEAN DATA PROTECTION BOARD. Guidelines 03/2022 on Deceptive Design Patterns in Social Media Platforms</strong></a><strong><br /></strong>The European regulatory document that classified manipulation techniques across major platforms.</p><h4>IV. The cost of metrics</h4><p><a href=""https://www.amazon.com.br/Tyranny-Metrics-Jerry-Z-Muller/dp/0691174954""><strong>MULLER, Jerry Z. The Tyranny of Metrics </strong>(2018)</a><br />When the metric becomes the goal, behavior reorganizes itself around appearing — not being.</p><p><a href=""https://www.amazon.com.br/Weariness-Self-Diagnosing-Depression-Contemporary/dp/0773536256""><strong>EHRENBERG, Alain. The Weariness of the Self</strong> (2010)</a><br />The contemporary anguish is not that of someone who has failed — it is that of someone who never finishes constructing themselves.</p><h4>V. For further investigation</h4><p><a href=""https://uxdesign.cc/the-snake-that-eats-its-tail-3656b31fd0f9""><strong>MURRAY, Rachel M. The snake that eats its tail</strong></a><strong><br /></strong>UX Collective. The tech ecosystem as a self-feeding feedback loop: attention generates data, data refines design, design captures more attention.</p><p><a href=""https://uxdesign.cc/the-snake-that-eats-its-tail-3656b31fd0f9""><strong>FABRIZIA. The design of shallow thinking</strong></a><strong><br /></strong>UX Collective, 2025. How the internet’s design choices have reconfigured not just what we consume, but the cognitive architecture with which we process anything at all.</p><p><a href=""https://uxdesign.cc/are-we-creating-brain-rot-dad9e947ba5c""><strong>WILHELM, Daley. Are we designing for brain rot?</strong></a><strong><br /></strong>UX Collective, November 2025. The consequences of creating products designed to form compulsive habits — and the designer’s responsibility in that process.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=0e41da7d7652"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/are-we-performing-ourselves-into-exhaustion-0e41da7d7652"">Are we performing ourselves into exhaustion?</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/openai-ads-as-content-708666ddab2b?source=rss----138adf9c44c---4,1771436145,OpenAI: from ads to content,"OpenAI: from ads to content

<h4>The future of digital advertising should feel less like a distraction and more like recommendations from a friend.</h4><figure><img alt=""A delicate watercolor and graphite sketch of a smartphone screen showing a chat interface where one specific message bubble is glowing with a warm, golden light, symbolizing a high-utility, relevant answer"" src=""https://cdn-images-1.medium.com/max/1024/1*3gimrvTxVY4VhPIvGKaU5A.png"" /><figcaption><em>High relevance ads could enhance the quality of the content.</em></figcaption></figure><p>I’m a <strong>product designer</strong> and for the last four years I have been working in digital advertising for tech. I wouldn’t have imagined building part of my career in advertising, but here I am, and I have found myself honestly enjoying the challenge. Through this experience, I have learned that <strong>healthy and fair advertising</strong> is actually a good thing for the industry. It helps brands reach people, and it helps people find what they are looking for. When ads are well-executed, they are great content. There have been amazing pieces of advertisement across history, and when you pair great ads with great products, magic happens.</p><p>A few days ago, ChatGPT started <a href=""https://openai.com/index/testing-ads-in-chatgpt/"">testing ads</a> within its conversations. The first screenshots show a safe and expected approach: a user starts a conversation around a specific vertical, the model gives a standard AI answer, and then it shows an ad related to the conversation keywords. This replicates the model used in other media platforms by capturing signals and showing inventory that matches them. It’s a safe starting point, but it feels simplistic considering the possibilities of AI. There should be a more sophisticated way to do this. We need a model that treats ads not as an interruption, but as <strong>high-utility content</strong> that actually improves the experience.</p><figure><img alt=""A screenshot of a ChatGPT interface showing a ‘Potluck power tips’ response followed by a sponsored ad from ‘Heirloom Groceries’ for an Enchilada Kit, featuring price, stock status, and a small product image."" src=""https://cdn-images-1.medium.com/max/1024/1*KHxGNphfpnapvof7EIbXog.jpeg"" /><figcaption><em>OpenAI’s initial test displays ads as sponsored links following a conversational response.</em></figcaption></figure><p>The immediate reaction from many is that ads in ChatGPT will harm the user experience. I think that if the strategy is well executed, it could <strong>improve the quality of recommendations</strong>. My rationale is based on the <strong>data source</strong>. When platforms create an ad ecosystem, they have a real incentive to digest merchant catalogs, local service listings, and travel databases. Once this content is set up for advertisers, it can be used to power general responses, too.</p><p>Currently, when you ask ChatGPT about a product or local service, it likely scrapes multiple websites. This doesn’t guarantee the metadata is right or updated. A <strong>direct integration</strong> with a business’s database ensures freshness and reliability. It is the technical difference between scraping a UI and communicating directly through an <strong>API</strong>. I don’t doubt that advertisers, from global retailers to local barber shops, are waiting for the moment they can upload their catalogs and service lists directly to OpenAI. As <a href=""https://www.roidna.com/blog/advertising-in-the-age-of-llms-what-happens-when-ai-becomes-the-ad-platform/"">industry reports from <strong>ROI·DNA</strong></a> suggest we are entering an era where brand-verified catalogs become a fundamental part of the reasoning layer, allowing the model to ground its advice in high-quality, real-time data rather than just scraped noise.</p><p>Think of it this way: if you ask about running shoes today, the model analyzes your context and scrapes the web. It reads reviews and influencer videos, but it is unlikely to find professional, unbiased benchmarking. Most of these sources already have sponsored rankings, so the content is already biased by <strong>SEO games</strong>. The model might show you an organized summary, but it’s coming from already <strong>biased and probably sponsored sources</strong>. The products might also be out of stock or the technical specs might be outdated.</p><p>In a new model, the AI would bypass this noise by directly integrating with <strong>rich catalogs</strong> of shoes, boutique hotels, or specialized medical services. These databases include real-time price, stock, and local availability. When doing research, the model could cross-reference this first-hand data with community reviews or scientific research. This also <strong>levels the playing field</strong>, as smaller, local brands could be part of that same inventory.</p><p>By leaning into this infrastructure now, OpenAI is pulling ahead of most competitors. They are building a better recommendation experience under the guise of an ad network, establishing the <strong>technical pipes required for commerce </strong>before others even begin. <strong>Google</strong> remains a unique incumbent here. Through its shopping features, it has spent years perfecting an ecosystem of real-time merchant data. While Google may be better positioned to activate these features when they decide to fully commit to AI ads, OpenAI’s moves are defining a new standard for what a helpful recommendation engine in the AI world looks like.</p><p>My take on the recent <a href=""https://www.youtube.com/watch?v=De-_wQpKw0s"">Anthropic ad campaign</a>, which brilliantly mocks the scenario where ads become intruders in the conversation, is that while it is a great piece of marketing, it only addresses the “noisy” version of advertising. In a scenario where ads are treated as high-quality, verified content, this satirical picture no longer fits. More importantly, LLMs that fail to integrate these APIs and catalogs soon risk falling behind in the race to provide the best commercial recommendations because that is a primary use case for AI users today.</p><p>I often think of the analogy of a friend recommending a product. If you tell a friend who is passionate about bikes that you’re looking for a weekend ride, they don’t just shout a brand name at you. They ask about your taste, your budget, and your local terrain. They are a friend first, understanding your context before suggesting a solution. This is the difference between <strong>advertising as noise and advertising as advice.</strong></p><figure><img alt=""A minimalist watercolor study of two friends sitting at a cafe table; one is gesturing while a faint, sketched bicycle appears as a shared thought between them."" src=""https://cdn-images-1.medium.com/max/1024/1*QNSAdwJd_H2s30QaPRdENg.png"" /><figcaption><em>AI recommendations should feel like a conversation between friends.</em></figcaption></figure><p>This analogy brings us to a <strong>“neutrality paradox.”</strong> A good AI analysis should rank options based on what is actually best for the user, rather than who paid the most. Critics will rightly ask: how can the model remain an unbiased advisor if it’s also the salesperson? To get this right, OpenAI must maintain a <strong>strict wall</strong> between the model’s reasoning and the ad auction. The AI should first determine the ideal solution for the user, and only then see if there is an ad that matches that specific intent. If the best suggestion happens to be sponsored content, everyone wins. If the model starts nudging users toward an ad just to hit a revenue target, the trust is gone.</p><p>For this neutrality to feel real, it has to be visible. This means following the rules of digital advertising, like <strong>clear labeling</strong>. If a recommendation is sponsored, the user should know exactly why it’s there and have the power to opt out or mute specific ads entirely. Trust isn’t built by camouflaging the ads, it’s built by giving the user <strong>visibility and control</strong>.</p><p>This new paradigm also has massive implications for <strong>user agency</strong>. Think of a scenario where, when planning a trip, the model surfaces a tiny local coffee shop that just opened in your destination: a business that otherwise could never afford to compete with big brands for global SEO. Perhaps because the user specifically asked for small local businesses. For this to work, OpenAI has to solve the <strong>“pay to play”</strong> barrier. If only the giants can afford to bid, the user’s preference for local shops becomes a dead end. A fair game requires a model where relevance and values can occasionally outrank a large ad budget.</p><figure><img alt=""A warm, glowing watercolor of a small local coffee shop storefront standing out in vibrant color against a muted, grey-sketched city background."" src=""https://cdn-images-1.medium.com/max/1024/1*3ylFl0Cshsq07DBHsYxJFg.png"" /><figcaption><em>Find local businesses that matter to you.</em></figcaption></figure><p>This leads to a shift away from one-way communication. Most ads today are static broadcast. In this new model, an ad becomes a <strong>two-way dialogue</strong>. Instead of a generic banner, the ad can become an interactive experience that answers specific questions about a product’s features. It moves from interruption to <strong>consultation</strong>, surfacing only the parts of a product relevant to your current need. The model becomes more like a great salesperson that answers your questions, one who knows when to say, “I have nothing to offer you right now, but it was great having a conversation with you.”</p><p>This approach also enables better <strong>discovery</strong>. Traditional search is limited by what we know to ask for. In a conversation, an AI can identify a need that the user hasn’t yet put a name to. This is a shift toward a discovery-based experience, similar to what platforms like <strong>Pinterest</strong> have spent years perfecting. As <a href=""https://medium.com/pinterest-engineering/ads-candidate-generation-using-behavioral-sequence-modeling-f9077ee1325d"">Pinterest Engineering notes</a>, ads are most effective when they are designed to inspire and connect users with ideas they genuinely love rather than acting as a simple distraction.</p><p>Of course, this level of personalization raises the stakes for privacy. To be the “friend” in the analogy, the AI needs to understand your context deeply, but that data must remain a private secret. The real opportunity is in <strong>zero-knowledge advertising</strong>, where an advertiser knows their ad was shown to the right person, but never knows who that person is or what they said to the AI. The proof of success will be found in ad performance, not data harvesting.</p><p>Ultimately, the success of this shift depends on one thing: OpenAI’s willingness to be rigorous enough to <strong>decide when not to show an ad</strong>. The business hunger for growth often leads platforms to flood every empty space with sponsored content. If OpenAI shows an ad just to collect the incentive, even when it doesn’t truly help the user, they become just another search engine. If they make it right, they can leverage the power of the ads as content.</p><p>There are broader challenges in the AI landscape that need to be addressed. Like any powerful technology, AI must be governed by priorities like safety and ethics. <strong>Safety concerns must always come first.</strong> It is only when all necessary guardrails are met, and the model identifies a clear and safe shopping intent, that a healthy advertisement ecosystem can exist.</p><p>This new paradigm opens a significant opportunity for ads to evolve from a <strong>tax on our attention</strong> into a <strong>service for our needs</strong>. A well-executed advertisement could be a win for everyone involved. We’re jumping into the <strong>next phase of human history</strong>, and yes, there will be ads. I don’t think anything will stop this change, so the best thing we can do is get it right by ensuring the value of the answer is never traded for the price of the ad. That’s what I’m hoping to happen.</p><p><strong><em>Rodrigo Osornio</em></strong><em> is a San Francisco-based product designer with training in visual arts and industrial design. He is passionate about shaping the future of human-technology interaction.</em></p><p><em>Illustrations created by the author using mixed media. The screenshot of the OpenAI ads test was taken from their announcement.</em></p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=708666ddab2b"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/openai-ads-as-content-708666ddab2b"">OpenAI: from ads to content</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/5-reasons-why-ar-glasses-are-inevitable-69fb46013816?source=rss----138adf9c44c---4,1771417176,5 Reasons why AR glasses are inevitable,"5 Reasons why AR glasses are inevitable

<figure><img alt=""A glasses with floating UI on it. AI-Generated image"" src=""https://cdn-images-1.medium.com/max/1024/1*RDWf03gEFMyyNtIVGJpa2g.jpeg"" /><figcaption>AI-generated image based on the original from this article <a href=""https://www.queppelin.com/ar-glasses-for-navigation/"">https://www.queppelin.com/ar-glasses-for-navigation/</a></figcaption></figure><p>When I talk about AR glasses, I don’t mean bulky headsets. I mean devices that look like regular eyewear capable of whispering information into your ear and displaying 3D content or flat panels overlaid on the real world.</p><p>This article is neither a validation of the technology nor a criticism, it is an objective personal look at what I see coming. Here are five reasons why I believe AR glasses will succeed and become mainstream, along with the consequences designers and users must face.</p><h3>1. Solving the “Kidnapped Body” problem</h3><figure><img alt=""An illustration of a woman looking at the phone. AI-Generated image"" src=""https://cdn-images-1.medium.com/max/1024/1*Kv1QO5wUplFqomrm1QmdGg.jpeg"" /></figure><p>Mobile devices demand a high physical price. They force us to look down, occupy our hands, and monopolize our attention. I call this the “kidnapped body” problem. We pay this price without thinking, but the implications are profound. It severs us from the physical world, blocking out views, people, and genuine connection.</p><p>We have even developed medical terms for this. “<a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC11856789/"">Tech Neck</a>” describes the repetitive stress injury caused by constantly looking down at a screen. AR glasses may succeed because they liberate the body. They return our hands to us and, more importantly, they lift our chins. By allowing us to look up and ahead, they reintegrate us into the world. While they will for sure introduce new distractions, the shift from “head-down” to “heads-up” computing is a fundamental ergonomic correction that users will gravitate toward.</p><h3>2. The new, no-new device</h3><figure><img alt=""A woman smiling grabbing her glasses. AI-Generated image"" src=""https://cdn-images-1.medium.com/max/1024/1*zGXPyqrNH8iLmzgZrqIbKw.jpeg"" /></figure><p>In a gadget-saturated world, introducing a completely new device is risky. However, AR glasses have a secret weapon. They aren’t “new.” They are just glasses.</p><p>Global estimates suggest around <a href=""https://www.overnightglasses.com/eyewear-industry-statistics/#:~:text=Globally%2C%20at%20least%202.2%20billion,for%202023%20was%20$65.6%20billion."">4 billion people already wear glasses</a> for various reasons, with over <a href=""https://www.who.int/news-room/fact-sheets/detail/blindness-and-visual-impairment"">2.2 billion people having a near or distance vision impairment</a> according to the World Health Organization. The form factor is familiar, socially accepted, and worn daily. The friction to adoption is incredibly low compared to a VR headset. Much like the Apple Watch which sold 12 million units in its breakout year by leveraging the centuries-old habit of wearing wristwatches. As long as they look like iconic eyewear rather than tech hardware, they are primed for mass adoption.</p><h3>3. The Spatial Canvas (Escaping the Screen)</h3><figure><img alt=""An illustration of a busy street seen from the perspective of a person with AR glasses. The street is full of floating user interface. AI-Generated image"" src=""https://cdn-images-1.medium.com/max/1024/1*a-BfGo5NIZp_uzQbQHmxCg.jpeg"" /><figcaption>AI-generated image based on the original from this article <a href=""https://www.crunchfish.com/how-does-the-world-look-through-smart-glasses/"">https://www.crunchfish.com/how-does-the-world-look-through-smart-glasses/</a></figcaption></figure><p>There is a multi-million-dollar industry currently trapped behind 6-inch glass rectangles. Tech giants from phone manufacturers to app developers are hitting the limits of what can be done on a 2D flat screen.</p><p>The physical world is a “green field” for these companies, a massive, unclaimed canvas for digital overlay. Whether we like it or not, the push to colonize our surroundings with digital content is the next logical growth engine for the tech sector.</p><p>For product designers, this represents the most significant paradigm shift ever seen. We are moving from designing within a constrained frame to designing without borders. Eventually, we will no longer design apps that “trap” users, we will design apps that enhance the environment. This shift to “Spatial Computing” will unlock entirely new economies and user behaviors that flat screens simply cannot support.</p><h3>4. Strong Use Cases</h3><figure><img alt=""A floating user interface seen from inside a car. AI-Generated image"" src=""https://cdn-images-1.medium.com/max/1024/1*dDpTz4QCVcDT2OR8tmirpA.jpeg"" /><figcaption>AI-generated image based on the original from this article <a href=""https://www.digitaltrends.com/cars/envisics-ar-windshield-technology/"">https://www.digitaltrends.com/cars/envisics-ar-windshield-technology/</a></figcaption></figure><p>The utility of placing digital objects into the physical world is undeniable. While current iterations act as a Heads-Up Display (HUD) rather than fully immersive AR, they are the necessary bridge to the future.</p><p>Apps designed for physical interaction will thrive first:</p><ul><li><strong>Exploration and Learning:</strong> This is perhaps the strongest long-term use case. You could stand in front of a historical monument and instantly see contextual history overlaid on the stone. You could look at a complex car engine and see step-by-step repair labels pointing to the exact parts you need to touch. It turns the entire world into an interactive learning manual.</li><li><strong>Navigation:</strong> Arrows overlaid on the street as you walk, allowing you to converse with friends without checking a phone map.</li><li><strong>Cooking:</strong> Hands-free recipes floating above the counter.</li><li><strong>Safety:</strong> The ability to livestream to emergency services instantly during dangerous situations could be a significant deterrent to crime.</li><li><strong>Accessibility:</strong> For users with disabilities, these glasses can provide real-time context, enhance visual contrast, or narrate the environment instantly empowering users without requiring active intervention.</li></ul><p>As product designers we can already start exploring how to design for AR glasses in Google’s documentation for instance, in this <a href=""https://design.google/library/transparent-screens"">article</a>, David Allan Reese talks about how to design with daylight, UI components sizes, contrast, space awareness and more. You can also take a look at how Google’s AR UI kit, <a href=""https://developer.android.com/develop/xr/jetpack-xr-sdk/jetpack-compose-glimmer"">Jetpack Compose Glimmer</a>, looks like. On the other hand, Meta has also released its <a href=""https://wearables.developer.meta.com/docs/develop"">documentation </a>about developing and designing for Ray-Ban Meta Glasses.</p><h3>5. Voice Interaction Made Natural</h3><figure><img alt=""An illustration of a woman with an AR glasses touching the right side of them simulating touching the volume. Some sound player UI floating next to the hand. AI-Generated image"" src=""https://cdn-images-1.medium.com/max/1024/1*mEhRbXCbvv-XrjkMYjF2kg.jpeg"" /></figure><p>Historically, conversational interfaces have suffered from poor reputations. They were often frustrating and limited. However, the rise of Large Language Models (LLMs) has fundamentally changed this.</p><p>Conversational AI is becoming natural, instant, and context-aware very fast. When you combine competent AI with the “hands-free” nature of glasses, voice becomes a crucial input system. The social awkwardness of talking to a device will likely fade, much like how wearing wireless earbuds 24/7 went from “rude” to “normal.”</p><h3>The Consequences</h3><p>If this technology succeeds, we must be prepared for two major friction points, particularly from a design perspective:</p><p><strong>1. The Privacy Paradox</strong> Privacy concerns run in two directions. First, what are manufacturers capturing about our daily lives? Second, how do we manage peer-to-peer privacy? The “always-on” camera creates uneasiness in social environments. How will society regulate recording in public? Will hacking the “recording LED” become common? These social contracts will need to be rewritten, and many users may reject the tech simply to opt out of this surveillance state.</p><p><strong>2. Spatial Invasion and New UX Patterns</strong> If the world becomes a digital canvas, who owns the rights to that space? We risk a future of “spatial spam” ads anchored to physical storefronts, landmarks, or even sidewalks. Without strict regulation, the tranquility of the real world could be interrupted by pop-ups and notifications that we cannot swipe away.</p><p>For designers, this raises critical ethical questions. What is the “dark pattern” of spatial design? How do we design a safe mode mechanism for the real world? We may see the rise of aggressive attention-grabbing patterns that intentionally block your view. Designers will have the heavy responsibility of defining the etiquette of this new layer. Will we design respectful interfaces that blend in, or intrusive billboards that demand attention? We may not see the regulations put in place until after the intrusion has already begun.</p><h3>To wrap up</h3><p>AR glasses introduce a new reality, literally. They’ll bring with them as many opportunities as there are threads of concern. For product designers, the challenge is not just to be ahead of the trend, but to deeply understand what this technology adds to or subtracts from human life.</p><p>Personally, I am fascinated by a future enhanced by this technology, a life where reality is augmented to help and delight us. However, we have seen how initial excitement can turn into unforeseen consequences. Social media reminds us of how a tool for connection can evolve into a mental health crisis. By the time we realized the damage, it was already too late for some generations.</p><p>I believe in the power of individual common sense, but I also know that mass adoption shifts behavior in unpredictable ways. I hope this article has sparked your interest and encouraged you to investigate further. We are facing a fascinating technological horizon, and it is up to us, designers and users, to ensure it remains a tool for human enhancement rather than distraction.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=69fb46013816"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/5-reasons-why-ar-glasses-are-inevitable-69fb46013816"">5 Reasons why AR glasses are inevitable</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/bad-model-behaviour-by-design-af0b514a1314?source=rss----138adf9c44c---4,1771417119,Bad (model) behaviour by design,"Bad (model) behaviour by design

<h4>AI doesn’t just reflect human bias. It amplifies it through everyday use, quietly shaping judgement, trust, and decisions.</h4><figure><img alt=""Illustration of a person from behind, holding a pen thoughtfully while reviewing two stacks of CVs on a wooden desk. One stack is labelled “AI” and the other “Colleague.”"" src=""https://cdn-images-1.medium.com/max/1024/1*tIC7DvEr9qL6ZqaiHrcYcw.png"" /><figcaption>Image generated by author</figcaption></figure><p>You’re halfway through a stack of job applications, AI assistant humming along beside you, flagging promising candidates like an eager intern. It disagrees with your gut feeling about one applicant. You pause. Reconsider. A few screens later, it disagrees again. You change your mind once more. By the end of the session, you’ve deferred to its judgment on nearly a third of the decisions where you initially felt confident.</p><p>Now swap the algorithm for a colleague. Same stack, same disagreements. How often do you actually budge? Much less often, as it turns out.</p><p>This isn’t hypothetical hand-wringing. <a href=""https://www.nature.com/articles/s41562-024-02077-2"">Recent research</a> involving 1,401 participants found that <strong>when people disagree with AI, they change their minds 32% of the time. With other humans, that figure drops to 11%.</strong> That’s almost three times more influence from a machine than from another person. Quite something, given we’ve spent millennia learning to be sceptical of each other.</p><p>We like to think of ourselves as rational decision-makers who simply use these tools as helpful aids. The evidence tells a different story. They do more than support our thinking; they reshape it. And far more powerfully than human opinions ever could.</p><p>This raises a question that should keep designers up at night: if users are this susceptible to algorithmic persuasion, what happens when the model itself is biased?</p><p>The answer isn’t just that users make biased choices. The effect compounds. Through repeated interactions, small errors snowball into meaningful distortions. And in most cases, users never notice it happening.</p><figure><img alt=""An infographic showing two stacks of CVs: a tall stack labelled “AI Recommended” and a shorter stack labelled “Your Gut Feeling.” An arrow shows 32% of decisions shifting toward the AI pile. A hand with a magnifying glass reviews the documents."" src=""https://cdn-images-1.medium.com/max/1024/1*yvNAnP9ZZFCbxgWnaz2X4A.png"" /><figcaption>Image by author</figcaption></figure><h3>The amplification machine</h3><p>To understand why AI influence packs such a punch, we need to look at what’s actually happening in the interaction itself. The problem isn’t one-sided. It’s a two-way street, with both the AI and the human feeding into a loop that amplifies bias at every turn.</p><p>On the AI side, these systems don’t simply reflect the patterns in their training data. They tend to exaggerate them. Machine learning models are optimised to detect and reinforce patterns, which means subtle skews in the data often become stronger in the output.</p><p>A hiring algorithm trained on historical data where 70% of successful hires were men doesn’t just reproduce that 70/30 split. It learns to weight male candidates more heavily, nudging the bias further along. The system is simply doing what it was built to do: find patterns and double down on them.</p><p>But that’s only half of the picture.</p><h3>The human half of the equation</h3><p>We don’t treat AI recommendations the same way we treat advice from another person. We tend to see automated systems as more objective, more analytical. Somehow immune to the messy biases that plague human thinking. This perception gets reinforced by how AI is marketed (“trained on the sum of human knowledge”) and by our long-standing trust in technology for high-stakes decisions in healthcare and finance.</p><p><a href=""https://www.nature.com/articles/s41562-024-02077-2"">Research from University College London and MIT</a> shows just how potent this combination can be. Across a series of experiments, participants interpreted facial expressions, motion perception, and other people’s performance. In each case, they interacted with AI systems that had been deliberately programmed with biases similar to those found in many real-world models.</p><p><strong>What happened next wasn’t just a momentary nudge.</strong> The distortion deepened. Through repeated interaction, small initial errors grew into significant skews. Participants who started with relatively minor biases ended up with significantly stronger ones. Largely because the AI amplified existing patterns while participants remained unusually receptive to its guidance.</p><p>Here’s where it gets uncomfortable: most participants had no idea how much the AI was shaping their judgement. When researchers told participants they were interacting with another person (while secretly using a machine), the effect weakened. Simply believing the source was human reduced how deeply the bias took hold.</p><figure><img alt=""Three-phase diagram showing bias persistence. Phase 1: Human and AI robot making decisions together, producing a skewed pattern of outputs. Phase 2: AI is removed, human continues working alone. Phase 3: Human alone still making the same skewed decisions, with a “bias stuck” label and a broken gear icon indicating the pattern has transferred to the human even without the AI present."" src=""https://cdn-images-1.medium.com/max/1024/1*I7183KuMBJ1-HEdXNNmGnQ.png"" /><figcaption>Image by author</figcaption></figure><p>Awareness mattered, but not in the way we might expect. Knowing that the advice came from an algorithm made people <em>more</em> susceptible, not less. <strong>We expect AI to be right, so we lower our guard.</strong> It’s the automation equivalent of assuming the smoke alarm would have gone off if there were a real fire.</p><p>Even more troubling: this effect doesn’t vanish when the AI does. <a href=""https://www.nature.com/articles/s41598-023-42384-8"">A 2023 study</a> had participants assess medical images (specifically, deciding whether skin spots were benign or potentially cancerous) using a deliberately biased algorithm.</p><p>When the system was removed, the bias stuck around. People continued making the same skewed calls on their own, having unconsciously absorbed its patterns into their decision-making. The higher someone’s self-reported trust in automation, the more their independent conclusions mimicked the machine’s errors.</p><h3>The path of least resistance</h3><p>Our vulnerability to AI isn’t just about trust in technology. It’s rooted in something rather unflattering about how our brains handle effort.</p><p><strong>Psychologists call this the <em>cognitive miser</em> effect. </strong>Thinking is expensive (metabolically, emotionally, and in terms of time) so when we’re handed an opportunity to reduce mental effort, we tend to grab it. These systems offer a particularly seductive shortcut: offloading complex judgement to something that appears faster, smarter, and more thorough than we could manage ourselves.</p><p>This helps explain why automation bias shows up so consistently. <a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC3240751/"">Studies show</a> that when people work with automated decision-support systems, they dial back their own monitoring and verification. <strong>The presence of AI doesn’t just influence decisions; it quietly changes how much effort we put into thinking at all.</strong></p><p>We see the recommendation, and it feels complete. The work appears to be done. On to the next thing.</p><figure><img alt=""Split illustration comparing cognitive load. Left side labelled “Without AI: High Cognitive Load” shows an overheating brain filled with gears, surrounded by data, documents, and question marks. Right side labelled “With AI: Reduced Cognitive Effort” shows a calm, glowing brain with a “Recommendation Accepted” checkmark and a small robot below."" src=""https://cdn-images-1.medium.com/max/1024/1*YdaGvpezSIlnOGjNRow9JQ.png"" /><figcaption>Image by author</figcaption></figure><p>But this isn’t purely about conserving effort. We also tend to believe that these tools have analytical abilities that surpass our own. In some domains, that belief is justified. They can crunch enormous datasets, spot patterns at scale, and perform calculations far beyond human capability.</p><p>The problem is that we extend this assumption into contexts where it doesn’t hold.</p><p>The fact that an algorithm can analyse thousands of medical images doesn’t mean its assessment about an individual patient is infallible. Just because a system can process every CV in a database doesn’t mean it grasps what makes a good hire in a specific team. The leap from “impressive at scale” to “trustworthy in specifics” is one we make far too readily.</p><h3>When trust spirals</h3><p><a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC3240751/"">The evidence on automation bias</a> shows that inexperience and low confidence amplify this effect. When people feel uncertain about their own abilities, or when they’re working under time pressure, they’re more likely to defer to the machine.</p><p>This creates a rather unhelpful loop. <strong>The more we rely on AI, the less we exercise our own judgement. The less confident we become, the more we rely on it.</strong> Keep that cycle spinning long enough, and your decision-making instincts start to gather dust.</p><p>Branding matters too. Studies have found that when users are told they’re working with an “expert system” rather than a basic algorithm, they’re more likely to follow its recommendations — even when those recommendations are wrong. How we label AI directly shapes how much trust users place in it.</p><p>There’s also what researchers call <a href=""https://en.wikipedia.org/wiki/Automation_bias"">“learned carelessness”</a>. When these systems perform well over time, users gradually reduce their vigilance. High reliability leads to complacency. If the system has been right 95% of the time, we stop checking as carefully. And when it stumbles on that remaining 5%, we’re rarely prepared.</p><figure><img alt=""Diagram of a downward spiral with four stages: “Defer to AI” at the top, followed by “Practice Own Judgement Less”, then “Confidence in Own Ability Drops”, and finally “Defer to AI Even More.” The spiral tightens and darkens as it descends into a vortex."" src=""https://cdn-images-1.medium.com/max/1024/1*dUJ8BO0UjkYFStYuUgdqOA.png"" /><figcaption>Image by author</figcaption></figure><h3>More than the sum of its parts</h3><p>For a long time, AI bias and human bias have been treated as separate problems with separate fixes. We try to clean up training data and fine-tune models. We train users to recognise their own blind spots and think more critically. Both approaches matter, but they miss something essential. What emerges from human–AI interaction isn’t simply one plus the other. It’s compound bias. Something new that arises specifically from the interplay between the two.</p><p>Think of it less like an equation and more like a chemical reaction. When certain elements combine, they don’t just sit alongside one another. They interact, producing something neither had before they met.</p><p><a href=""https://arxiv.org/html/2504.18759v1"">A recent paper</a> makes exactly this argument. <strong>Bias doesn’t live solely in the model or in the user. It emerges dynamically through feedback loops that evolve over time, each party making the other a little worse with every exchange.</strong></p><p>Medical AI studies illustrate this clearly. In a <a href=""https://arxiv.org/html/2511.14591v2"">2025 study</a>, researchers examined what happens when class imbalance in AI systems meets human base rate neglect.</p><p>Class imbalance is a technical bias where rare conditions are underrepresented in training data. Base rate neglect is a human cognitive bias where we struggle to reason accurately about low-probability events. Individually, each causes predictable problems.</p><p>Together, they produce something worse.</p><p><strong>Class imbalance throws off people’s ability to gauge how much they should trust the AI. Base rate neglect makes users more vulnerable to skewed outputs.</strong> Each bias reinforces the other, creating feedback loops that lead to poorer outcomes than either would cause alone.</p><p>The errors don’t add up. They multiply.</p><h3>Fixing the wrong problem</h3><p>This is why traditional debiasing strategies often fall short. You can scrub training data and tune algorithms all you like. But if the interaction pattern itself amplifies bias, you haven’t solved the problem. You’ve just trimmed one input to a system that compounds errors regardless.</p><p>The same goes for human-focused interventions. <strong>Teaching users about cognitive bias is valuable, but if the interface encourages offloading and discourages verification, that knowledge stays theoretical.</strong> Filed under “good to know” and rarely retrieved.</p><p>The question shifts. It’s not “is the AI biased?” Or even “are the users biased?” Instead, it goes: “does this interaction create bias amplification over time?”</p><p>That’s a fundamentally different design challenge.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*WTzPaQG-Fs7yjFKMekYRLg.png"" /><figcaption>Image based on Caravaggio’s Narcissus (1597–1599)</figcaption></figure><h3>Mirror, mirror in the interface</h3><p>AI doesn’t just mirror our biases. It amplifies them. Quietly, incrementally, and often invisibly. The problem isn’t confined to training data or human psychology alone. It lives in the space between humans and machines, shaped by how each influences the other over repeated interaction.</p><p>For designers, this reframes what we’re solving for. We can’t simply debias algorithms and dust off our hands. We can’t rely solely on user education. <strong>The bias emerges from the interaction itself. From patterns of trust, deference, and cognitive offloading that compound with every exchange.</strong></p><p>Which brings us to the real question.</p><p>If this amplification happens through everyday interactions with AI, where is it already showing up most strongly in the products we use today?</p><p>The research points to a particular type of interface. One that feels natural, helpful, and conversational. In these interactions, users don’t just follow biased recommendations. They begin to internalise them, carrying those biases forward even after the interaction ends.</p><p>And that’s precisely what makes it so hard to see.</p><blockquote><strong><em>Thanks for reading! 📖</em></strong></blockquote><blockquote><em>If you liked this post, </em><a href=""https://medium.com/@doracee""><em>follow me on Medium</em></a><em> for more.</em></blockquote><h3>References &amp; Credits</h3><p>Glickman, M., &amp; Sharot, T. (2024). <em>How human–AI feedback loops alter human perceptual, emotional and social judgements</em>. <strong>Nature Human Behaviour</strong>. <a href=""https://www.nature.com/articles/s41562-024-02077-2"">https://www.nature.com/articles/s41562-024-02077-2</a></p><p>Sánchez-Amaro, A., &amp; Matute, H. (2023). <em>Humans inherit artificial intelligence biases</em>. <strong>Scientific Reports</strong>, 13, 15737. <a href=""https://www.nature.com/articles/s41598-023-42384-8"">https://www.nature.com/articles/s41598-023-42384-8</a></p><p>Goddard, K., Roudsari, A., &amp; Wyatt, J. C. (2012). <em>Automation bias: a systematic review of frequency, effect mediators, and mitigators</em>. <strong>Journal of the American Medical Informatics Association</strong>, 19(1), 121–127. <a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC3240751/"">https://pmc.ncbi.nlm.nih.gov/articles/PMC3240751/</a></p><p>von Felten, N. (2025). <em>Beyond Isolation: Towards an Interactionist Perspective on Human Cognitive Bias and AI Bias</em>. <strong>CHI 2025</strong>. <a href=""https://arxiv.org/html/2504.18759v1"">https://arxiv.org/html/2504.18759v1</a></p><p><em>Biased Minds Meet Biased AI: How Class Imbalance Shapes Appropriate Reliance and Interacts with Human Base Rate Neglect</em>. (2025). <a href=""https://arxiv.org/html/2511.14591v2"">https://arxiv.org/html/2511.14591v2</a></p><p><em>Automation bias</em>. (2025). <strong>Wikipedia</strong>. <a href=""https://en.wikipedia.org/wiki/Automation_bias"">https://en.wikipedia.org/wiki/Automation_bias</a></p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=af0b514a1314"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/bad-model-behaviour-by-design-af0b514a1314"">Bad (model) behaviour by design</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/the-problem-with-best-practices-in-the-age-of-ai-642987dc9ec9?source=rss----138adf9c44c---4,1771416915,The problem with best practices in the age of AI,"The problem with best practices in the age of AI

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/the-problem-with-best-practices-in-the-age-of-ai-642987dc9ec9?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/642/0*e0rMolDP61o_MdGx"" width=""642"" /></a></p><p class=""medium-feed-snippet"">I showed my team an AI-generated design. Two senior designers called it &#x2018;solid.&#x2019; None of them questioned where it came from.</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/the-problem-with-best-practices-in-the-age-of-ai-642987dc9ec9?source=rss----138adf9c44c---4"">Continue reading on UX Collective »</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/dinosaurs-and-designers-are-underrated-120137cc0221?source=rss----138adf9c44c---4,1771416863,Dinosaurs and designers are underrated,"Dinosaurs and designers are underrated

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/dinosaurs-and-designers-are-underrated-120137cc0221?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/1200/1*4SwGsnldv7l7R1uOOTX0Jw.png"" width=""1200"" /></a></p><p class=""medium-feed-snippet"">A myth from paleontology explains almost everything that hurts.</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/dinosaurs-and-designers-are-underrated-120137cc0221?source=rss----138adf9c44c---4"">Continue reading on UX Collective »</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/why-code-is-not-the-source-of-truth-df456a8d12de?source=rss----138adf9c44c---4,1771357096,Why code is not the source of truth,"Why code is not the source of truth

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/why-code-is-not-the-source-of-truth-df456a8d12de?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/2600/0*kQ8ryjt21G-fcrl7"" width=""4255"" /></a></p><p class=""medium-feed-snippet"">And why AI just made it urgent.</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/why-code-is-not-the-source-of-truth-df456a8d12de?source=rss----138adf9c44c---4"">Continue reading on UX Collective »</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/how-user-segmentation-rather-than-personas-helps-you-get-design-buy-in-6662d0a9a3ec?source=rss----138adf9c44c---4,1771330713,"How user segmentation, rather than personas, helps you get design buy-in","How user segmentation, rather than personas, helps you get design buy-in

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/how-user-segmentation-rather-than-personas-helps-you-get-design-buy-in-6662d0a9a3ec?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/2276/1*p_SpLi1JX-xKYeV1V5rsJA.jpeg"" width=""2276"" /></a></p><p class=""medium-feed-snippet"">Businesses need to know who their user is, and how many there are</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/how-user-segmentation-rather-than-personas-helps-you-get-design-buy-in-6662d0a9a3ec?source=rss----138adf9c44c---4"">Continue reading on UX Collective »</a></p></div>"
