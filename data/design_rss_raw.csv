source,domain,url,created_utc,title,text
rss,smashingmagazine.com,https://smashingmagazine.com/2025/10/javascript-for-everyone-iterators/,1761570000,JavaScript For Everyone: Iterators,"JavaScript For Everyone: Iterators

Here is a lesson on Iterators. Iterables implement the iterable iteration interface, and iterators implement the iterator iteration interface. Sounds confusing? Mat breaks it all down in the article."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/10/ambient-animations-web-design-practical-applications-part2/,1761138000,Ambient Animations In Web Design: Practical Applications (Part 2),"Ambient Animations In Web Design: Practical Applications (Part 2)

Motion can be tricky: too much distracts, too little feels flat. Ambient animations sit in the middle. They’re subtle, slow-moving details that add atmosphere without stealing the show. In part two of his series, web design pioneer Andy Clarke shows how ambient animations can add personality to any website design."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/10/ai-ux-achieve-more-with-less/,1760688000,AI In UX: Achieve More With Less,"AI In UX: Achieve More With Less

A simple but powerful mental model for working with AI: treat it like an enthusiastic intern with no real-world experience. Paul Boag shares lessons learned from real client projects across user research, design, development, and content creation."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/10/how-make-ux-research-hard-to-ignore/,1760619600,How To Make Your UX Research Hard To Ignore,"How To Make Your UX Research Hard To Ignore

Research isn’t everything. Facts alone don’t win arguments, but powerful stories do. Here’s how to turn your research into narratives that inspire trust and influence decisions."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/10/the-grayscale-problem/,1760349600,The Grayscale Problem,"The Grayscale Problem

From A/B tests to AI slop, the modern web is bleeding out its colour. Standardized, templated, and overoptimized, it’s starting to feel like a digital Levittown. But it doesn’t have to be."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/10/smashing-animations-part-5-building-adaptive-svgs/,1759755600,"Smashing Animations Part 5: Building Adaptive SVGs With `<symbol>`, `<use>`, And CSS Media Queries","Smashing Animations Part 5: Building Adaptive SVGs With `<symbol>`, `<use>`, And CSS Media Queries

SVGs, they scale, yes, but how else can you make them adapt even better to several screen sizes? Web design pioneer Andy Clarke explains how he builds what he calls “adaptive SVGs” using ``, ``, and CSS Media Queries."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/10/intent-prototyping-practical-guide-building-clarity/,1759485600,Intent Prototyping: A Practical Guide To Building With Clarity (Part 2),"Intent Prototyping: A Practical Guide To Building With Clarity (Part 2)

Ready to move beyond static mockups? Here is a practical, step-by-step guide to Intent Prototyping &mdash; a disciplined method that uses AI to turn your design intent (UI sketches, conceptual models, and user flows) directly into a live prototype, making it your primary canvas for ideation."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/09/desktop-wallpaper-calendars-october-2025/,1759230000,Shades Of October (2025 Wallpapers Edition),"Shades Of October (2025 Wallpapers Edition)

How about some new wallpapers to get your desktop ready for fall and the upcoming Halloween season? We’ve got you covered! Following our monthly tradition, the wallpapers in this post were created with love by the community for the community and can be downloaded for free. Enjoy!"
rss,smashingmagazine.com,https://smashingmagazine.com/2025/09/from-prompt-to-partner-designing-custom-ai-assistant/,1758880800,From Prompt To Partner: Designing Your Custom AI Assistant,"From Prompt To Partner: Designing Your Custom AI Assistant

What if your best AI prompts didn’t disappear into your unorganized chat history, but came back tomorrow as a reliable assistant? In this article, you’ll learn how to turn one-off “aha” prompts into reusable assistants that are tailored to your audience, grounded in your knowledge, and consistent every time, saving you (and your team) from typing the same 448-word prompt ever again."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/09/intent-prototyping-pure-vibe-coding-enterprise-ux/,1758733200,Intent Prototyping: The Allure And Danger Of Pure Vibe Coding In Enterprise UX (Part 1),"Intent Prototyping: The Allure And Danger Of Pure Vibe Coding In Enterprise UX (Part 1)

Yegor Gilyov examines the problem of over-reliance on static high-fidelity mockups, which often leave the conceptual model and user flows dangerously underdeveloped. He then explores whether AI-powered prototyping is the answer, questioning whether the path forward is the popular “vibe coding” approach or a more structured, intent-driven approach."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/09/ambient-animations-web-design-principles-implementation/,1758546000,Ambient Animations In Web Design: Principles And Implementation (Part 1),"Ambient Animations In Web Design: Principles And Implementation (Part 1)

Creating motion can be tricky. Too much and it’s distracting. Too little and a design feels flat. Ambient animations are the middle ground &mdash; subtle, slow-moving details that add atmosphere without stealing the show. In this article, web design pioneer Andy Clarke introduces the concept of ambient animations and explains how to implement them."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/09/psychology-trust-ai-guide-measuring-designing-user-confidence/,1758276000,The Psychology Of Trust In AI: A Guide To Measuring And Designing For User Confidence,"The Psychology Of Trust In AI: A Guide To Measuring And Designing For User Confidence

With digital products moving to incorporate generative and agentic AI at an increasingly frequent rate, trust has become the invisible user interface. When it works, interactions feel seamless. When it fails, the entire experience collapses. But trust isn’t mystical. It can be understood, measured, and designed for. Here are practical methods and strategies for designing more trustworthy and ethical AI systems."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/09/how-minimize-environmental-impact-website/,1758189600,How To Minimize The Environmental Impact Of Your Website,"How To Minimize The Environmental Impact Of Your Website

As responsible digital professionals, we are becoming increasingly aware of the environmental impact of our work and need to find effective and pragmatic ways to reduce it. James Chudley shares a new decarbonising approach that will help you to minimise the environmental impact of your website, benefiting people, profit, purpose, performance, and the planet."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/09/serpapi-complete-api-fetching-search-engine-data/,1758042000,SerpApi: A Complete API For Fetching Search Engine Data,"SerpApi: A Complete API For Fetching Search Engine Data

From competitive SEO research and monitoring prices to training AI and parsing local geographic data, real-time search results power smarter apps. Tools like SerpApi make it easy to pull, customize, and integrate this data directly into your app or website."
rss,uxdesign.cc,https://uxdesign.cc/sora-app-icon-a-visual-breakdown-179235bd7ab0?source=rss----138adf9c44c---4,1761862391,Sora app icon: a visual breakdown,"Sora app icon: a visual breakdown

<h4>How the new Sora app icon gives away more than meets the eye.</h4><figure><img alt=""Sora App Icon: A Visual Breakdown"" src=""https://cdn-images-1.medium.com/max/1024/1*h_U-o0S_igR0xLB9nQxcBg.png"" /><figcaption>The New Sora App Icon is Giving Away More Than Meets the Eye.</figcaption></figure><p>Two weeks ago we saw <a href=""https://www.theverge.com/news/797752/openai-sora-app-1-million-downloads"">the release of the new Sora app by OpenAI</a>. Unfortunately the app is only available in the US and Canada, and I’m in neither. Since I can’t use the app itself, I thought I’ll do a breakdown of what I can actually see — the app icon, it looks simple at first glance but it packs a lot if we can deconstruct it. I’m doing this breakdown as a fun exercise in design appreciation, I know my breakdown will probably not going to align with the authorial intent behind it. If you like this breakdown please share it, now let’s dive in.</p><p>These kinds of breakdowns are akin to exercises I used to do while earning my Master’s Degree in Visual Communications, they can be a lot of fun but they’re intended to reveal hidden truths or meanings lurking in plain sight. Seeing with our eyes is great but analyzing a visual component in a written long-form article tend to make us (or at least — make me) think a little harder, to notice the things I can feel when looking at a visual work, but don’t consciously see.</p><p>Breaking down Sora’s app icon will require some knowledge of what the Sora app is. Sora is a mix of a social video app, like TikTok, but all the content is AI generated videos, not unlike Midjourney’s Explore page.</p><p>The Sora app also has a feature called “cameo” where you can generate videos of yourself, or any other person who gave the app permission, could be your friends or could be celebrities like Michael Jackson or even OpenAI’s Sam Altman (who appears in many of Sora’s videos).</p><p>The generated videos are not more than 10 seconds long and rendered with both sound and a moving watermark, to make it harder to spread as real footage. The app itself is invite only right now, but will probably be available soon to everyone, with better and more precise rules and guardrails in place. Even though the app feels like TikTok and got a lot of comparisons to TikTok in both the traditional media and social media — its app icon is very different.</p><figure><img alt=""OpenAI Sora new website, screenshot shows 4 AI Generated videos."" src=""https://cdn-images-1.medium.com/max/1024/1*xMhzoToX8aixuIrFPpGpzw.png"" /><figcaption><em>OpenAI’s Sora website — on the right side are 4 generated videos.</em></figcaption></figure><h3>The little cloud</h3><p>The most prominent element in the Sora icon is without a doubt the cloud in the middle. At first we can see by its shape and angle of rotation that this cloud is the same as the ChatGPT icon — OpenAI’s only app except Sora.</p><figure><img alt=""Sora’s app icon, ChatGPT app icon and both overlayed on one another."" src=""https://cdn-images-1.medium.com/max/1024/1*sZUSB5cki_xyNyb23vFeHA.png"" /><figcaption><em>Left to right: Sora app icon, ChatGPT app icon, both icons overlayed.</em></figcaption></figure><p>The reason I started by calling it a “cloud” is that it looks kind of like a cloud since it’s floating in the sky. This also can fit the AI narrative of making everything, including videos, in the cloud (in the technological sense of clouds). Generated AI images are also often compared to humans looking at the clouds and seeing things, and in that sense it would be really easy to see a cloud in this icon too. But when I tried to compare it to other clouds iconography used by tech companies, something amazing happened- I realized it’s not a cloud at all.</p><figure><img alt=""Sora app icon, Apple’s iCloud app icon and Microsoft’s OneDrive App icon"" src=""https://cdn-images-1.medium.com/max/1024/1*PpcopKEAVo8yQaO0bvpmeg.png"" /><figcaption><em>Sora app icon, Apple’s iCloud app icon and Microsoft’s OneDrive App icon</em></figcaption></figure><p>Notice how the other “clouds” are flat at the bottom so the cloud can actually stand upright when left alone on a desk, like a computer would. This might be an intentional aspect of cloud computing since they are computers behind the curtains, after all.</p><p>The Sora icon is different, so it’s not a cloud, it can also reference a thought bubble in the style of comics, since they have no flat part. Notice that these bubbles are for thinking, not speaking.</p><p>The thinking behind this might be that Sora generates videos in the speed of thought, any video you can think of will be done in mere minutes. The only barrier now is your own imagination, or your thought process, just like the thought bubbles in comics that can only hold so much text. Also, both comics and AI prompts use text in a very similar ”conversational” manner, so it’s only natural to use it for this icon.</p><figure><img alt=""Left: Sora app icon, Right: a thought bubble (from Understanding Comics/ Scott McCloud, 1993)"" src=""https://cdn-images-1.medium.com/max/1024/1*_DErPy0iu-bjGI7V1kODdw.png"" /><figcaption><em>Left: Sora app icon, Right: a thought bubble (from Understanding Comics/ Scott McCloud, 1993)</em></figcaption></figure><p>At this point you might think sure I believe you it’s a thought bubble, but why does this bubble have eyes, well I’m getting to it.</p><h3>Smiling with your eyes</h3><p>First of all let’s get the obvious out of the way — the eyes make it seem like a friendly robot, signaling this AI is a friendly one. Sora’s AI is a cute little robot that just wants to help you be more creative and is capable of no harm (roll your eyes now, please).</p><p>Of course, that’s not the whole story, the eyes have little AI “sparkling” emoji, as we all know this emoji (✨) was intended to signal magical stuff but was hijacked by the AI industry to signal if anything has AI in it. Much like the AI industry highjacked all of our content and scrapped everything humans ever created- they also highjacked the icon to represent it (again, roll your eyes).</p><figure><img alt=""Sora’s app icon next to an anime girl with sparkling eyes."" src=""https://cdn-images-1.medium.com/max/1024/1*FGJHCLSs7F2EYhoVhcAUMA.png"" /><figcaption><em>Left: Sora app icon, Right: Smile Precure!, スマイルプリキュア！ (Episode 35)</em></figcaption></figure><p>This emoji, like all original emojis, was first made by Shigetaka Kurita for SoftBank- a Japanese telecom company. Kurita itself was asked by David Amel (<a href=""https://www.youtube.com/watch?v=g-pG79LOtMw""><em>YouTube link</em></a>) about the usage of this icon for AI features and he didn’t like it. Of course anybody can use any emoji but since its meaning was originally supposed to be something else and the literal father of emojis doesn’t like it, I’ll consider this emoji stolen because they highjacked its meaning.</p><figure><img alt=""A screenshot from David Amel’s video showing him talking and two sparkling amojis signifying either magical things or AI."" src=""https://cdn-images-1.medium.com/max/1024/1*ADT0DP5Uz6zR6_7Le47u4w.avif"" /><figcaption>David Amel Emoji video (<a href=""https://www.youtube.com/watch?v=g-pG79LOtMw"">YouTube Link</a>)</figcaption></figure><p>To bring it back to the Sora icon itself, if we’ll look closely at the eyes we can see that it’s not eyes at all, it’s holes that we can see the background through. What can be the meaning of those “holes for eyes” face, well — first of all let’s explore the most simple answer. A face with holes for eyes is a skull, the eyes decay and we’re left with only the bones.</p><figure><img alt=""Sora’s app icon next to an Emoji skull."" src=""https://cdn-images-1.medium.com/max/1024/1*52WES-3a-DkiTiXOWW9kUQ.png"" /><figcaption><em>Sora app icon vs a skull (via Apple’s Emoji)</em></figcaption></figure><p>A skull is obviously a dead person, but it can also refer to pirates because of their flag, known as the Jolly Roger or simply as the “skull and crossbones”. Pirates can mean a few things in the software business, first and the most likely explanation here is a nod to the famous Steve Jobs quote “it’s better to be a pirate than to join the navy”.</p><p>The meaning of this quote is that it’s better to explore the unexplored than to act inline — since the discovery of the new will always happen by “pirates”. Steve Jobs apparently felt so strongly about this quote that he did decorate Apple’s offices with the Jolly Roger flag in his first run at Apple.</p><figure><img alt=""Young Steve Jobs and John Sculley standing in frond of an Apple-made Jolly Roger pirate flag."" src=""https://cdn-images-1.medium.com/max/600/1*3aD_OJbftxwqTxSoODN3TA.avif"" /><figcaption><em>Steve Jobs standing in front of Apple’s Jolly Roger flag.</em></figcaption></figure><p>Of course we’re all thinking about the other explanation of a pirate flag in the software industry and that’s pirated software. Pirated software is what the software industry calls a piece of software that users share among themselves without proper licensing.</p><p>That was (and still is) a crime when people did that and some people even got sued or fined for it. But when big companies do that, like OpenAI and other AI companies it’s somehow not a crime and they get to be worth billions of dollars instead. So they do really get to be pirates.</p><p>Another popular entity with holes for eyes is the Golem. The Golem is a famous European Jewish myth about a clay creature that becomes alive when a wise rabbi does some magic to awake him. The magic this rabbi is doing is to carve a word into the Golem’s forehead, the word in this instance can be analogous to a prompt the AI needs to do its magic, to become “alive”.</p><p>Both AI and the Golem in the story are not really alive but rather have the illusion of being alive. The Jewish tale is meant to be cautionary, the Golem is becoming unstoppable and tries to upstage the rabbi, much like AI is threatening to upstage human workers in many different industries today.</p><p>If you don’t like this Jewish parable you can think of The Terminator instead, it’s also a man-made entity that, once built, we lost any control over it. The Terminator again looks like a skull and again has no eyes (only lights, much like the sparkling in Sora’s no-eyes), so it’s a good enough reference, although I prefer the Golem analogy. The Terminator is also regarded as a cautionary tale against the dangers of AI, of course.</p><figure><img alt=""Sora’s app icon next to a statue of the Golem."" src=""https://cdn-images-1.medium.com/max/1024/1*g_t9O3KNj1dY8W9Y0atZlA.png"" /><figcaption><em>Sora app icon vs The Golem (statue in Prague, Czech Republic)</em></figcaption></figure><h3>Stars at night</h3><p>Sora’s icon is also interesting, it is a linear gradient transitioning from blue up to a very dark blue, it’s brighter at the bottom and darker at the top, it also have tiny lighter pixels on it, presumably like stars in the sky.</p><p>OpenAI, I think, picked a blue background for a few reasons, first and foremost it’s reminiscent of other “social” apps, much like Facebook, LinkedIn and the original Twitter app. The transitioning to the darker color is meant to signal that it’s a more modern social app like TikTok, Threads and X (new Twitter) which all share a black background.</p><p>The Sora app isn’t just a video generation AI model, it’s a social app designed to feel much like TikTok and in that sense, they were spot on with the background colors, they signal they’re a new kind of social app but with some respect, or a nod, to the past that started it all.</p><figure><img alt=""First row: Sora, Facebook, LinkedIn and Twitter. Second row: Sora, TikTok, Threads and X."" src=""https://cdn-images-1.medium.com/max/1024/1*DSp5rkOSWWXWgWpxlHbYag.png"" /><figcaption><em>First row: Sora, Facebook, LinkedIn and Twitter.<br />Second row: Sora, TikTok, Threads and X.</em></figcaption></figure><p>We aren’t done yet with the background, the background also have stars in it, those stars might invoke the image of “The Starry Night” by Vincent Van Gogh (1889), which with many other classic artists, were replicated by AI to death, so now anyone could create in his iconic style.</p><figure><img alt=""A screenshot of a website offering anybody to generate their own Van-Gogh styled images."" src=""https://cdn-images-1.medium.com/max/1024/1*if7P6sHGdo0eyam1ScKUsQ.png"" /><figcaption>Get your AI Generated Van-Gogh (from <a href=""https://getimg.ai/styles/van-gogh"">Getimg.ai</a>)</figcaption></figure><p>If we’ll go to a more pop culture route we can also think of Disney’s famous “When You Wish Upon a Star” from the movie Pinocchio (1940), one of my favorite Disney movies.</p><p>Pinocchio also stars a wooden man-made doll that becomes alive and then gets into all kind of troubles, in that sense Pinocchio — like the Golem and Terminator, becomes a fitting metaphor for Sora and the AI industry as a whole.</p><figure><img alt=""Left: Sora app icon, Right: “When You Wish Upon a Star”, Pinocchio, Walt Disney, 1940."" src=""https://cdn-images-1.medium.com/max/1024/1*MHC7Xvi1SbJUiQmmmVNfdg.png"" /><figcaption><em>Left: Sora app icon, Right: “When You Wish Upon a Star”, Pinocchio, Walt Disney, 1940.</em></figcaption></figure><p>At first I found it strange that the Sora icon (and it’s “cloud”) were on a night-time background but as I started to understand the symbolism I was reminded of the hit show Game of Thrones most famous phrases: “For the night is dark and full of terrors”.</p><h3>Afterword</h3><p>This breakdown was done without experiencing the app itself (because of my region), so all I know is by the cultural impact it made on the internet: articles, podcasts, YouTube videos etc.</p><p>Take it with a grain of salt and I hope some of what I wrote did resonate with you after all, if it did please let me know. AI as a whole was a very interesting development in the tech landscape and as I saw many people for and against the technology I only left hoping that Sora has enough guardrails in place and that they will never change their fabulous icon.</p><p>I did saw many Sora videos in a few social networks and many people screaming against Sora or video generation as a whole, but I never saw anyone doing a deep dive on their icon like I did here, so hopefully it can serve as another way we can talk about these apps that is more academic and level-headed.</p><p>I do have my own blog where I publish more articles, some that will never get published on Medium since they might be too personal, so if you liked it and want more of my thoughts — here’s a <a href=""https://www.adirs.co""><em>link to my own blog.</em></a></p><p>Oh, and Sora is the name of the main character in the game Kingdom Hearts (2002) that yields a giant key, keys usually open doors, maybe the door to the future? to unite the past of early 2000s with the future of AI? Do with that what you will.</p><p>Goodbye.</p><h3>Sources</h3><p><a href=""https://necsus-ejms.org/the-golem-in-the-age-of-artificial-intelligence/"">The Golem in the age of artificial intelligence / Amir Vudka</a></p><p><a href=""https://basicappleguy.com/basicappleblog/jollyroger"">Apple’s Jolly Roger / Basic Apple Guy</a></p><p><a href=""https://mashable.com/article/apple-pirate-flag-40th-anniversary"">Here’s why Apple is flying a pirate flag to celebrate its 40th anniversary / Christina Warren</a></p><p><a href=""https://www.theverge.com/2020/4/2/21204498/art-transfer-google-artists-style-photos"">Art Transfer by Google lets you apply famous artists’ styles to your own photos / Kim Lyons</a></p><p><a href=""https://www.trafficsoda.com/why-is-every-app-blue/"">Why Is Every App Blue? / Amanda Turner</a></p><p><a href=""https://techcrunch.com/author/natasha-lomas/"">Blue Apps Are All Around / Natasha Lomas</a></p><p><a href=""https://biomedicalodyssey.blogs.hopkinsmedicine.org/2024/11/reading-pinocchio-and-frankenstein-in-the-age-of-artificial-intelligence/"">Reading Pinocchio and Frankenstein in the Age of Artificial Intelligence / Katie Pham</a></p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=179235bd7ab0"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/sora-app-icon-a-visual-breakdown-179235bd7ab0"">Sora app icon: a visual breakdown</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/after-the-aws-outage-cfd402d224d7?source=rss----138adf9c44c---4,1761846107,After the AWS outage,"After the AWS outage

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/after-the-aws-outage-cfd402d224d7?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/2600/0*exnS0Hn15QO5yGD9"" width=""5157"" /></a></p><p class=""medium-feed-snippet"">Can we trust Big Tech monopolies in essential infrastructure?</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/after-the-aws-outage-cfd402d224d7?source=rss----138adf9c44c---4"">Continue reading on UX Collective »</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/can-technology-fix-voting-cfd6674465a2?source=rss----138adf9c44c---4,1761823591,Can technology fix voting?,"Can technology fix voting?

<h4>What impact does digitization of electoral process actually have?</h4><figure><img alt=""Image of a hand dropping a ballot inside of a computer"" src=""https://cdn-images-1.medium.com/max/1024/1*fALUsm56qBpcYPmPfWG2fQ.png"" /><figcaption>Image of a hand dropping a ballot inside of a computer</figcaption></figure><p>The mayoral election is around the corner in my hometown of Montreal, so I wanted to confirm whether I was registered to vote at my current address. What I naively assumed would be a simple task turned out to require a trip to a local community centre. I gave my name, they flipped through a binder, and gave me the thumbs up within 3 seconds of arriving. It felt like the ultimate “this could have been an email” moment and I lost two hours of my day confirming something I should’ve been able to check online in those 3 seconds.</p><p>Technology isn’t a silver bullet, and I’m far from a techno-optimist, but it <em>can </em>be a powerful vector for improvement. When designed carefully, it can increase accessibility, surface critical information, maybe even make it easier for people to engage civically in this context. For a long time, I assumed the main reason we hadn’t digitized more of the democratic process was good old-fashioned bureaucracy, a sluggish, underfunded government with outdated tools. I held a fundamental belief that if we simply made it easier to vote by letting people cast ballots from their phones or laptops more people <em>would</em>! More access = more engagement.</p><p>So I decided to investigate some interesting examples of places where they’ve managed to digitize their electoral process and see what the results were!</p><figure><img alt=""Markhamian looking at an digital ballot on an iPad"" src=""https://cdn-images-1.medium.com/max/720/0*Pnh_PCN3gDlRGcYk"" /><figcaption>Markhamian looking at an digital ballot on an iPad</figcaption></figure><h3>Markham’s digital experiment</h3><p>We’ll start with an interesting homegrown example. <a href=""https://www.elections.ca/content.aspx?section=res&amp;dir=rec/tech/consult&amp;document=p5&amp;lang=e"">In 2003, the City of Markham,</a> a suburb north of Toronto, launched one of Canada’s first pilot projects to enable online voting in a municipal election. The initiative was small at first but had the ambitious aim of testing whether residents could safely and securely cast a ballot from their home computers!</p><p>As is often the case with public sector innovation, the city didn’t have the money or in-house expertise to develop its own secure digital voting infrastructure so it turned to a third-party vendor called <a href=""https://en.wikipedia.org/wiki/Scytl"">Scytl</a>, a Spanish company that specialized in using cryptography to secure elections. The way the system works is each eligible voter receives a Voter Information Letter by mail, this includes a unique Personal Identification Number (PIN). Then during the advance voting period, Markhamians? Markhamites? People from Markham would go to Markham’s election website, hosted by Scytl, enter their PIN, and authenticate their identity with personal information (such as date of birth and postal code) matched against the city’s registry. Once verified, they could cast their vote through a digital ballot interface! And voila, the vote was then encrypted and transmitted to a secure election server, where it remained sealed until game day(election day).</p><p>Seems straight-forward enough right? And, cherry on top, according to city officials, online voting participation increased in each cycle, particularly among seniors and people with mobility limitations! But here’s the kicker: despite the relatively simple process, turnout didn’t budge. Most of the people who voted online were the same people who would’ve voted anyway.</p><p>Unfortunately, Markham’s experiment became a reference point across Canada informing other municipalities’ approaches to digital voting. Also more importantly, the third party system that Markham was relying on, Scytl, eventually filed for <a href=""https://www.lavanguardia.com/economia/20200514/481138885893/scytl-sandton-capital-liquidacion-venta-negocio-juez-voto-electronico.html"">bankruptcy in 2020, citing over €75 million</a> in debt. Causing us to ask the question: what happens when the company holding your digital ballots no longer exists? Without money to invest in an internal system and having to rely on third parties, can be potentially devastating. What if Scytl had gone bankrupt, during an election? Needless to say, Markham no longer allows e-voting, which makes me e-sad.</p><figure><img alt=""ivote interface showing how votes could be stolen"" src=""https://cdn-images-1.medium.com/max/1024/0*I7c20DI7UwdOvEO3.png"" /><figcaption><a href=""https://ar5iv.labs.arxiv.org/html/1504.05646"">A proof of concept, that shows the exploit against iVote to inject malicious code that would surreptitiously manipulate the voter’s choices</a></figcaption></figure><h3>When e-voting went wrong</h3><p>While Markham was a valiant attempt, it was small potatoes dealing with local politics and might not in and of itself be representative of the potential impact of online voting at a national level. So let’s look at an example of how things can go, very, very wrong and very right. Starting with the bad.</p><p>In 2015, New South Wales Australia rolled out the <a href=""https://arxiv.org/abs/1504.05646?utm_source=chatgpt.com"">world largest ever e-vote</a> system called iVote. This system allowed voters to cast their ballots online during the state election. Again the goal was to increase accessibility for people with disabilities and those living remotely, while also trying to modernize the voting experience by letting people vote from personal devices.</p><p>iVote purportedly was actually quite a nice app and the user experience was smooth and intuitive. Public communication about this new way of voting was also strong, causing nearly 280,000 votes to be cast using iVote in that election! Trouble is, in the days following the election, some pesky independent researchers discovered <a href=""https://link.springer.com/chapter/10.1007/978-3-319-22270-7_3""><strong>severe vulnerabilities</strong></a> in the live system. Among them: an insecure external analytics script that could have allowed attackers to intercept or alter votes, and a verification process that could be manipulated to provide false assurances to voters.</p><p>What’s worse, there is no clear evidence of tampering, but the possibility was there and because of this they will never try to pilot a program like this again. This may have very well affected who took office and the downstream impacts of that are no joke. The point of this example is to highlight what is at stake if the system goes south, or should I say New South (I’m sorry). In this case, there was no clear evidence of hacking but the vulnerability existed and may have been exploited to favour one candidate or another. With consequences this high, how important is the convenience of voting online in contrast?</p><p><a href=""https://www.innovationaus.com/ivote-written-out-of-nsw-law-as-digital-voting-review-begins/"">iVote was quietly shelved</a> in the years that followed. They had lost the trust of the public despite a lack of evidence of hacking and what began as a promise of a modern democracy ended up as a prime example of how important security is in this conversation. The lesson from New South Wales is clear: building digital voting systems isn’t just about making them easy to use, it’s about making them impossible to doubt. And that requires more than good design. It requires deep, structural transparency, independent oversight, and a willingness to stop and re-assess when trust begins to weaken.</p><figure><img alt=""Image showing Estonia’s digital ID card"" src=""https://cdn-images-1.medium.com/max/1024/0*Wds6woYpODxmW6Pv.jpg"" /><figcaption>Example of Estonias Digital ID card</figcaption></figure><h3>The shining beacon of Estonia</h3><p>Lastly, I’d like to pull up the most frequently cited example of a successful techo-democracy. Estonia! A small but mighty nation that is often on the cutting edge. They were the first country to launch internet voting at a national level in <a href=""http://news.bbc.co.uk/1/hi/world/europe/4343374.stm"">2005</a>. Their system called i-voting, allowed citizens to cast their ballots online from any internet connected device, for both national and municipal elections. When it initially rolled out, online adoption was around <a href=""https://www.valimised.ee/en/archive/statistics-about-internet-voting-estonia"">2%</a> in the first year but it has since climbed to over <a href=""https://www.valimised.ee/en/archive/statistics-about-internet-voting-estonia"">40%</a> of its population using online voting.</p><p>But, the reason this is possible is because unlike most countries, Estonia had spent the prior decade building a national digital identity card. This is a critical part of the puzzle unfortunately, and it’s what allows their digital voting program to be so successful. They already use their digital ID’s to file taxes, access their own medical records, and sign legal documents. So when the opportunity came to use this same, state-backed secure authentication method, it was a no brainer. They had the full infrastructure to roll it out.</p><p>Their i-Voting process is built for security and usability. Voters insert their <a href=""https://en.wikipedia.org/wiki/Estonian_identity_card"">digital ID card</a> (or they can use a mobile app), log into a government portal, and are presented with a digital ballot. Once completed, their vote is encrypted and then sent to a secure server. To guard against coercion or vote buying, voters can re-vote multiple times online. This protects against a common concern people have in e-voting, which is buying votes! Only their final submission is counted. And if someone chooses to vote in person on election day, it overrides any previous digital votes. Pretty ingenious.</p><p>Crucially, Estonia’s system includes end-to-end cryptographic verification. Voters can use a smartphone app to verify that their encrypted ballot was received and counted as intended without revealing who they voted for. The entire system is publicly audited, with detailed post-election reviews conducted by independent experts.</p><p>The Estonian system really is as good as it gets when it comes to a thoughtful, comprehensive and secure approach to digitizing democracy, and because of its long history it also offers us some concrete data on what effect digitization has on voter turnout. So here it is: 0.2 to 0.8 percentage points. That’s it. From 2005 to 2025. The painful truth is that if you’re looking to get more empowered citizens, focusing on access to voting is actually the last step in the process.</p><p>There are tangibly good reasons to invest in e-voting and they genuinely matter. From an accessibility standpoint alone, giving people with reduced mobility the ability to vote independently and securely is a powerful step forward. That, in itself, is worth the effort.</p><p>But if the goal is to dramatically increase engagement, we’re nowhere near a solution and I’m confident that the solution won’t be a purely technological one. What I uncovered is, if you care about people voting, it’s clear that access, especially technological access, is not the main bottleneck.</p><p>We need to stop obsessing over the ballot interfaces and start asking harder questions. Why don’t people vote in the first place? What would make them want to?</p><p>That’s where the real design challenge lies and that’s where we should be putting our energy!</p><p>P.S Don’t forget to vote in your next election</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=cfd6674465a2"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/can-technology-fix-voting-cfd6674465a2"">Can technology fix voting?</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/designing-with-claude-code-and-codex-cli-building-ai-driven-workflows-powered-by-code-connect-ui-f10c136ec11f?source=rss----138adf9c44c---4,1761778883,Building AI-driven workflows powered by Claude Code and other tools,"Building AI-driven workflows powered by Claude Code and other tools

<h4>How agentic CLI tools extend Figma MCP and turn wireframes into production-ready prototypes</h4><figure><img alt=""Opening illustration (three browser windows): An abstract illustration showing three browser‑like windows: the left window depicts a grey wireframe mock‑up; the dark center window contains logos for AI/design tools (a knot‑shaped Claude symbol, a starburst, and the colorful Figma logo); the right window shows a colourful finished interface. Arrows between the windows illustrate the transformation from a rough mock‑up to a polished UI"" src=""https://cdn-images-1.medium.com/max/1024/1*cI8-cRj06yGzsx3uBcGQaA.png"" /></figure><p>I set out to explore how designers can use agentic CLI tools like <strong>Claude Code</strong> and <strong>Codex CLI</strong><em> </em>to build AI-driven workflows that turn rough wireframes into production-ready prototypes reflecting a real codebase, not generic mockups. After testing both tools, the results revealed that a product designer can truly prototype with real code and the help of <strong>Figma MCP</strong>.</p><p>With a solid design system in place, design and code share the same language, AI becomes a reliable partner, accelerating exploration and producing maintainable, real-world prototypes that teams can carry forward.</p><h3>The problem</h3><p>If you are a designer already experimenting with AI tools, you know this feeling: gorgeous prototypes that, after looking at the code, are totally disconnected from the codebase your team is using and practically useless.</p><p>I’ve explored multiple AI tools myself to speed up my design workflow, but something was always missing. I tried <a href=""https://medium.com/design-bootcamp/designing-with-prompts-a-product-designers-journey-into-exploring-ai-tools-116aefa70966""><strong>Replit, Lovable, v0</strong>, and <strong>Bolt</strong></a> only to end up with prototypes built with generic code, based on <strong>Shadcn</strong> or <strong>Tailwind CSS</strong>. They didn’t align with the existing code, and couldn’t be reused in production.</p><p>This changed significantly with <strong>MCP servers</strong>, and in my <a href=""https://medium.com/design-bootcamp/designing-with-mcp-server-bridging-design-systems-and-ai-for-developer-friendly-prototypes-4f08b0a0881d"">previous article</a>, I was optimistic enough to imagine a workflow where designers could instantly prompt rough ideas to prototypes, while also pulling from existing code and design tokens. This time, I brought this vision to life using <strong>Code Connect UI </strong>and its improved features like component mapping and MCP instructions.</p><p>In this article, I will share how I built an AI-ready workflow using agentic CLI tools like <strong>Claude Code</strong> and <strong>Codex CLI</strong>, combined with <strong>Figma</strong> <strong>MCP</strong> and <strong>Code</strong> <strong>Connect UI</strong>, to turn high-level prompts into production-ready code that directly reflects my design system and codebase.</p><h3>From Cursor to agentic CLI tools</h3><p>Cursor was my first real look into what AI-assisted coding could look like. But soon after, agentic CLI tools like Claude Code came to play offering more control and capabilities, making AI integration into traditional workflows more efficient than any other IDE plugin ever did before.</p><p>I began exploring Claude Code and Codex CLI, two command-line tools from <strong>Anthropic</strong> and <strong>OpenAI</strong>. Both can run locally inside a terminal or IDE, with immediate access to their respective models, and integrate with MCP<strong> </strong>Servers for external tool access.</p><figure><img alt=""Codex CLI terminal screenshot: A screenshot of a terminal running Codex CLI. It shows commands that call the Figma MCP server to verify whether a button component uses the correct design tokens and variables. The output lines display progress status and code‑verification messages"" src=""https://cdn-images-1.medium.com/max/1024/1*x1EyBYSiLGFuguher6lf6A.png"" /><figcaption><em>The Codex CLI running in terminal and using Figma MCP tools.</em></figcaption></figure><p>The key difference between using an AI-powered IDE and an agentic CLI lies in how they communicate with the models. While IDEs act as middlemen, translating user requests through APIs or extensions, agentic CLIs connect directly to the models themselves, making interactions faster, more contextual, and more reliable.</p><p>This direct connection enables a truly agentic experience: Claude Code and Codex CLI don’t just respond to prompts; they reason, plan, and adapt within a project.</p><figure><img alt=""A comparison diagram showing how AI-powered IDEs like Cursor and VS Code communicate with Claude through intermediary tools versus how Claude Code connects directly to the model. On the left, multiple arrows connect the user, Cursor, VS Code, and Claude 4.5 through “cursor-tools” and “vscode-tools.” On the right, a simplified diagram shows a direct two-way link between the user and “Claude Code with Claude 4.5,” highlighting the streamlined communication path."" src=""https://cdn-images-1.medium.com/max/1024/1*3JE8VxAZVpTmBKC0nkUzJA.png"" /><figcaption><em>One of the key differences between using an AI-powered IDE and Claude Code lies in how they communicate with the same models.</em></figcaption></figure><p>Both Anthropic and OpenAI are actively supporting these tools with regular updates. One of them was a more user-friendly way of interacting with Claude Code on VS Code without the terminal.</p><figure><img alt=""VS Code with official extensions: A view of Visual Studio Code with two panes. The left pane is the VS Code welcome page with recent projects and actions; the right pane displays the Claude Code extension interface, which looks like a chat window ready to accept commands"" src=""https://cdn-images-1.medium.com/max/1024/1*nGfcHlCM29sCrZ8_X9m1qg.png"" /><figcaption><em>Both tools come with a more user-friendly version within VS Code through their official extensions.</em></figcaption></figure><h3>Building an AI-ready design system</h3><p>To ensure my workflow will work as expected, my first priority was to build a design system that aligns perfectly with my codebase, so AI agents could truly understand. One where every color, type, scale, and component carries the same meaning in both Figma and code. Then, use that shared structure to build production-ready prototypes from simple prompts or wireframes.</p><p>To achieve this, I aligned Figma variables with CSS tokens, ensuring MCP Server, Claude Code, and Codex<strong> </strong>CLI all spoke the same design language, starting from the foundations.</p><p>The result was:</p><ul><li>A design system perfectly aligned with code</li><li>React components mapped to their corresponding design versions using Figma MCP and Code Connect</li><li>Storybook documentation for all the components</li><li>A GitHub repository for traceability</li><li>An automated workflow process for orchestrating the agentic CLI tools running in my project</li></ul><p>Building around these foundations was crucial. At the end of the day, when design and code share the same language, designers and developers don’t have to make assumptions, and similarly, AI tools don’t have to guess.</p><h3>Preparing and organizing the Figma file</h3><p>What makes perfect sense for every product designer is <em>structure</em>. In any design system, structure defines clarity, and documentation defines purpose. Together, they form the bridge that helps both humans and AI make accurate decisions about how design maps to code.</p><p>The design foundations must be well-defined and closely aligned with code. In my case, the Figma variables were not only given a name, but they were directly mapped to their code syntax, which in this case were CSS variables. This mapping is essential not only for smoother collaboration between designers and developers but also for making AI workflows possible by reducing ambiguity.</p><figure><img alt=""Figma variables mapped to CSS tokens: A screenshot from Figma’s variables panel listing color variables (e.g., black, white, grey‑200) alongside numeric scales and mapped values. A pop‑over shows the CSS token name and hex value for the selected variable, illustrating how design variables map directly to CSS tokens"" src=""https://cdn-images-1.medium.com/max/1024/1*ID0N95Pr7FpOO-6lYnibIw.png"" /><figcaption><em>All Figma variables are mapped directly to CSS tokens, keeping design and code perfectly aligned.</em></figcaption></figure><p>The next step was to create both primitive and semantic CSS files, ensuring that what’s defined in Figma is also defined in the code, resulting in <strong>what you see is what you get</strong> on both ends.</p><figure><img alt=""CSS variables file: A code snippet showing a CSS file where color palette variables such as — igds‑color‑grey‑50, — igds‑color‑grey‑100 and so on are defined with their hex colour values. This demonstrates that the CSS variables mirror the Figma design tokens"" src=""https://cdn-images-1.medium.com/max/1024/1*X9LY8Tgu0jMf2qFPhicp2Q.png"" /><figcaption><em>The CSS variables reflect their corresponding Figma variables.</em></figcaption></figure><p>Once primitive and semantic tokens were in place, every design component exposed its variables in Figma through <strong>Dev Mode</strong>, making it easy for AI and developers to understand which ones to use when generating or writting code.</p><figure><img alt=""Figma Dev Mode overlay: Screenshot of Figma’s Dev Mode for a Button component. The button on the canvas is overlaid with blue labels indicating spacing variables (e.g., var( — igds‑space‑12)). On the right, a code panel shows the CSS layout and style definitions using those design tokens"" src=""https://cdn-images-1.medium.com/max/1024/1*30RNbgqCoiUJ7n95tLNf6g.png"" /><figcaption><em>By defining the code syntax for each component, Dev Mode now displays the correct format instead of inconsistent naming.</em></figcaption></figure><p>More design tokens were created to maintain consistency and define typography across the design system. Providing all this level of context also helps AI agents understand design intent more accurately and make fewer assumptions when it comes to decision-making.</p><figure><img alt=""Typography tokens: An image from Figma showing a table of typography scale variables. Rows such as “5xl”, “4xl”, “3xl”, etc. list corresponding numeric values (e.g., 72, 48, 40), demonstrating consistent type scale definitions"" src=""https://cdn-images-1.medium.com/max/1024/1*DNYw3UhQlP2xzdjQ2AtZKA.png"" /><figcaption><em>Typography tokens ensure consistent hierarchy and type scales.</em></figcaption></figure><h3>Code Connect UI and component mapping</h3><p>I was fortunate to <strong>participate in Figma’s early alpha testing</strong> for the new <strong>Code Connect UI</strong>, giving me the opportunity to try its new features and capabilities early, provide direct feedback, and see them evolve. From day one, I was impressed with how Code Connect integrated more seamlessly with <strong>Figma MCP</strong>, while also connecting to <strong>GitHub</strong> repositories, creating a stronger and more reliable connection between design and code.</p><figure><img alt=""Figma Code Connect UI with GitHub: A screen showing Figma Code Connect UI for a checkbox component. There’s a search bar for design components and, on the right, a GitHub panel indicating the repository and directories (e.g., igds-design‑system, src/components). It illustrates how Figma connects to GitHub to pull source code"" src=""https://cdn-images-1.medium.com/max/1024/1*Qt3F0AZpNysWitUDc0ZM3Q.png"" /><figcaption><em>Figma now integrates with GitHub to pull source code directly.</em></figcaption></figure><p>With the latest update, component mapping now allows specific <strong>MCP instructions</strong> for each design component. This way, AI agents can better understand how a component should behave, with custom instructions, and even apply specific code overrides without touching the source code — all within Figma.</p><figure><img alt=""MCP instructions comparison (before/after): Two side‑by‑side panels comparing a component before and after applying an MCP code override. Each panel contains a small design preview and associated code, and there is a modal window showing where custom instructions are added"" src=""https://cdn-images-1.medium.com/max/1024/1*o0fPc43kWjOge54IebiD9g.png"" /><figcaption><em>The MCP Instructions modal, with a comparison of a component before and after the code override.</em></figcaption></figure><p>This becomes especially valuable for rapidly prototyping new ideas without editing the source code or asking a fellow developer to help. Additionally, you can now adjust component properties and watch the code respond to these changes.</p><figure><img alt=""Component playground &amp; live code update: Image of Code Connect’s playground where component properties such as “disabled”, “size” and “label” are adjustable via controls. The right‑hand panel displays the generated code snippet that updates in real time as these properties change"" src=""https://cdn-images-1.medium.com/max/1024/1*ikGkxFBmHWQ7KFKFfUyXng.png"" /><figcaption><em>The playground on the left lets you adjust component properties, while the code snippet on the right updates accordingly in real time.</em></figcaption></figure><p>When the components get connected to the source code, you can view the active mapping of each design component with its corresponding code reference directly in Code Connect UI, with all variant mappings and the proper framework support.</p><figure><img alt=""Code mapping &amp; source code preview: A Code Connect UI screen showing a design component preview on the left and a panel on the right that connects the design to a code component path. The source‑code section contains a snippet verifying the match between the design and the code (e.g., import statements and component definitions)"" src=""https://cdn-images-1.medium.com/max/1024/1*MmKHS1N4l5Z2e2Gip-cBrg.png"" /><figcaption><em>The design component and its linked source code are now visible directly in Figma.</em></figcaption></figure><p>On the IDE side, Figma MCP server now works hand-in-hand with Code Connect, giving AI agents both design context and production awareness. When prompted, Claude Code can pull implementation details directly from the codebase, ensuring the generated prototypes mirror production components accurately.</p><figure><img alt=""VS Code showing Claude Code understanding design/code link: A Visual Studio Code window demonstrating that Claude Code can interpret the link between design and code: the left pane lists project files, while the right pane displays a markdown description explaining how Code Connect ensures components are mapped correctly"" src=""https://cdn-images-1.medium.com/max/1024/1*tywrih93ylNEPSij0Asaag.png"" /><figcaption><em>Claude Code can understand the connection between a design and a code component.</em></figcaption></figure><h3>Claude Code in action</h3><p>After the design system was ready, all I had to do was connect it to real code. My goal wasn’t only to make Claude Code generate components from designs, <strong>but</strong> <strong>to reuse the proper tokens, syntax, and patterns that already existed in the codebase</strong>.</p><h4>Project structure</h4><p>I built a<strong> React + Vite</strong> codebase, structured around three core folders:</p><ul><li><strong>styles</strong>: with <em>primitives.css</em>, <em>semantic.css</em>, and <em>components.css</em></li><li><strong>components</strong>: where each component included its .<em>tsx</em>, .<em>stories.tsx</em>, and .<em>css</em> file</li><li><strong>pages</strong>: a directory used for testing prototypes</li></ul><p>I provided both primitive and semantic CSS variables to establish a solid foundation of design tokens. With the help of the Figma MCP Server, Claude Code was then instructed to populate the components CSS file and implement the corresponding React components by referencing and applying those variables.</p><h4>Working with Claude Code</h4><p>The best way to work with Claude Code is within an <strong>IDE</strong> such as <strong>VS Code</strong>, either via terminal or its official extension. That way, code changes appear directly in the IDE, even if it is running in the terminal.</p><figure><img alt=""VS Code terminal: A screenshot of a dark terminal within VS Code running Claude Code. It shows logs of tasks and the integrated terminal output while Claude Code executes commands"" src=""https://cdn-images-1.medium.com/max/1024/1*DTvqCnVjRIj1MihjPpwtVQ.png"" /><figcaption><em>Claude Code within the IDE’s terminal.</em></figcaption></figure><p>A key feature of Claude Code is its ability to run multiple agents at the same time to perform different tasks. That way, I didn’t have to restrict myself to working only on one task at a time, but run multiple instances simultaneously to tackle different ones or break a complicated task into smaller ones.</p><figure><img alt=""VS Code terminal: Side-by-side terminal windows in VS Code showing Claude Code and Codex CLI running simultaneously. The left panel lists design token variables from a CSS file, while the right panel displays Claude’s analytical output summarizing token counts and usage patterns. The interface demonstrates both tools working in parallel on separate tasks within the same development environment."" src=""https://cdn-images-1.medium.com/max/1024/1*JQrJmGx758ZoytXvDnXxSA.png"" /><figcaption><em>Claude Code and Codex CLI can also run at the same time and perform different tasks.</em></figcaption></figure><h4>Optimizing the workflow</h4><p>The most important thing to keep in mind when working with Claude Code is to provide it with <strong>clear context </strong>and<strong> detailed instructions</strong>. It needs to always know how to perform a task and under what conditions. When it runs, it consumes both time and tokens, which can come at a cost (especially if you are using API pricing), so the goal is to optimize its workflow to always follow the shortest, most accurate route.</p><p>To do that, I created a centralized file at the root of my repository that outlined the project structure and defined how Claude Code should behave across the entire project when handling different tasks. These guidelines included information such as which MCP Servers and tools should be used, when to run verification checks on the implementation, which naming convention should be followed for new files, and other details.</p><figure><img alt=""CLAUDE.md files illustration: Illustration of a central yellow folder connected to three subfolders below it, representing an organized project structure. The top folder contains a document with a globe icon, while the three lower folders contain documents with icons symbolizing components (puzzle piece), styles or design tools (paintbrush), and typography (letters “Aa”). The image conveys a structured system linking design elements such as components, styles, and fonts."" src=""https://cdn-images-1.medium.com/max/1024/1*YFZY-HMlbfVdNQ2bPN8ALw.png"" /><figcaption><em>Image created by ChatGPT.</em></figcaption></figure><p>This was achievable with <strong>CLAUDE.md</strong> files — special configuration files that Claude automatically loads into context when starting a session. These files became the governance layer of my design system, ensuring that every implementation went through structured verification before being approved, and specific steps were consistently followed. The <em>root</em> file defines the global steps, while directories like <em>styles</em> and <em>components</em> have their own localized rules.</p><p>This modular approach led to faster responses and better accuracy. Each directory had its own <em>CLAUDE.md</em> file with localized instructions, like defining the format of the output I expected to get from Claude when it performed validation checks on components:</p><pre>#### Component-Specific Matrix Examples:<br /><br /><br />##### Button Matrix:<br />| Element | Property | Default | Hover | Active | Focus | Disabled |<br />|---------|----------|---------|-------|--------|-------|----------|<br />| Container | Background | #123c80 | #0c2855 | #092341 | #123c80 | #b7b8b9 |<br />| Container | Border | none | none | none | 2px outline | none |<br />| Text | Color | #fff | #fff | #fff | #fff | #6f7071 |<br />| Icon | Color | #fff | #fff | #fff | #fff | #6f7071 |<br />| Icon | Transform | none | none | scale(0.95) | none | none |<br /><br /><br />##### Card Matrix:<br />| Element | Property | Default | Hover | Selected | Focus | Disabled |<br />|---------|----------|---------|-------|----------|-------|----------|<br />| Container | Shadow | sm | md | lg | outline | none |<br />| Container | Transform | none | translateY(-2px) | none | none | none |<br />| Title | Color | #3c3e3f | #123c80 | #123c80 | #3c3e3f | #6f7071 |<br />| Description | Color | #6f7071 | #6f7071 | #5b5c5d | #6f7071 | #b7b8b9 |<br />| Icon | Opacity | 0.8 | 1 | 1 | 0.8 | 0.4 |</pre><p>This analysis compared the styling consistency of components against their Figma versions, producing a matrix to help Claude identify discrepancies in structure or token usage with precision:</p><figure><img alt=""Claude’s output matrix: A terminal screenshot presenting Claude’s output matrix. It lists UI elements and properties (e.g., “Container”, “Background”, “Border”), with columns for default tokens and Figma design tokens, helping to spot mismatches between implementation and design"" src=""https://cdn-images-1.medium.com/max/1024/1*IKm6i7GOVd5_TmnLuO6WYw.png"" /><figcaption><em>Claude’s output matrix comparing implementation tokens and Figma design values.</em></figcaption></figure><p>These are some of the methods ensuring that instead of generating arbitrary color values or Tailwind classes, Claude Code consistently references the correct semantic tokens from CSS.</p><p>That way, the AI wasn’t just generating generic code anymore; <strong>it was</strong> <strong>following specific logic and standards</strong> defined by the design system and reinforced by the project’s structure.</p><h4>Subagents and automation</h4><p>It’s worth noting that Claude Code did not always behave predictably. At times, it disregarded instructions or produced inaccurate results. Acknowledging this, I designed my agents and <em>CLAUDE.md</em> files with layered safeguards, ensuring reliable fallback mechanisms when things went off track.</p><figure><img alt=""Verify‑design agent results: A terminal output showing the verify‑design agent’s assessment of a button component. The report lists items like token architecture, ALT token matches, typography, spacing and states coverage, each followed by green check‑marks to indicate correctness"" src=""https://cdn-images-1.medium.com/max/1024/1*srlZhJq5n1zYqiLulmOpMg.png"" /><figcaption><em>The verify-design agent serves as a fallback, ensuring that the design verification process has been followed as it should.</em></figcaption></figure><p><strong>Claude Code agents</strong> (or <em>subagents</em>) act as autonomous assistants that can work on complex tasks following specific rules and using a defined set of tools. They can plan and execute these tasks independently while the main agent continues focusing on the overall workflow.</p><p>When a subagent is invoked, it operates on its own isolated context window. Think of it like a workspace that includes only the information relevant to its assigned subtask. Once it completes its work, it returns a distilled summary of its findings to the main agent, which then integrates them into the primary context.</p><figure><img alt=""A flow diagram illustrating how Claude Code subagents operate. The “Main Agent” handles the primary task and invokes “Subagent #1” and “Subagent #2,” each working in isolated context windows. Arrows show subagents sending back distilled insights to the main agent. The caption explains that this setup keeps the main workflow clean and focused while subagents handle subtasks independently."" src=""https://cdn-images-1.medium.com/max/1024/1*taCOtWWes1HpuW-6TFqfRw.png"" /><figcaption><em>Subagents work in isolated context windows and return only distilled insights to the main agent, keeping the primary workflow clean and focused.</em></figcaption></figure><p>This architecture helps keep <strong>lower token usage and costs</strong>, since subagents process smaller portions of information. Meanwhile, the main agent maintains a slightly expanded context window without becoming overloaded. It’s important to note that <strong>a larger context window doesn’t always mean better performance</strong>; it often leads to vaguer reasoning and a higher risk of errors.</p><h4>Optimizing the agents’ workflow</h4><p>Making it easy for the agent to understand when to split a task and which agent to invoke is crucial. In my setup, the root file lists all available agents, ensuring Claude is always aware of which one to use and under which circumstances.</p><pre>## Available Agents (Overview)<br /><br />1. **design-verification**: For verifying component implementations match Figma designs and use correct tokens<br />2. **component-composition-reviewer**: For component creation/modification with nested components<br />3. **figma-code-connect-generator**: For ALL Code Connect related tasks<br />4. **token-analyzer**: For analyzing component token usage patterns and optimization recommendations<br />5. **a11y-accessibility-orchestrator**: Main coordinator for comprehensive accessibility audits using Playwright MCP and axe-core<br />6. **a11y-wcag-compliance-auditor**: Specialized WCAG 2.1 AA/AAA compliance testing and legal compliance assessment<br />7. **a11y-color-contrast-specialist**: Expert color contrast analysis <br />8. **a11y-keyboard-navigation-tester**: Comprehensive keyboard accessibility testing and validation<br />9. **a11y-screen-reader-tester**: Screen reader compatibility and ARIA implementation testing<br />10. **claude-md-compliance-checker**: MANDATORY final step for EVERY task</pre><p>All agents were assigned to distinct responsibilities and triggered under specific conditions or manually when needed. Certain rules were also established to ensure that Claude never skipped or overrode these validations.</p><pre>## Quality Gates &amp; Blocking Rules<br /><br />### Cannot Proceed If:<br />- design-verification finds violations (when applicable)<br />- component-composition-reviewer finds violations (when applicable)<br />- token-analyzer recommends changes (when applicable)<br />- Build fails<br />- Lint fails<br />- claude-md-compliance-checker finds violations<br /><br />### If An Agent Fails:<br />1. Fix ALL reported issues<br />2. Re-run the agent<br />3. Only proceed when it passes<br />4. Then run claude-md-compliance-checker</pre><p>At the end of each task, my <strong>compliance-checker</strong> agent ensured that Claude had successfully followed the workflow outlined in the root <em>CLAUDE.md</em>. If not, then Claude would iterate again, fixing all the flagged issues before marking the task as complete.</p><p>The <strong>design-verification </strong>agent also had another crucial role,<em> </em>continuously validating that all React components matched the original Figma designs.</p><pre>name: design-verification<br />description: Use this agent to verify that component implementations match <br />their Figma design specifications. This agent automatically extracts design <br />data from Figma, analyzes component implementations, and ensures proper design <br />token usage and visual property compliance.<br /><br /># Tool Restrictions<br />tools: [&quot;Read&quot;, &quot;Grep&quot;, &quot;Glob&quot;, &quot;WebFetch&quot;, &quot;mcp__figma__get_code&quot;, <br />&quot;mcp__figma__get_variable_defs&quot;, &quot;mcp__figma__get_screenshot&quot;, <br />&quot;mcp__figma__get_metadata&quot;]</pre><p>This system of subagents and automation turned my workflow into a self-regulating environment. The next step was to extend this framework with <strong>MCP servers</strong>, enabling my agentic CLI tools to operate beyond their local boundaries.</p><h3>Beyond local boundaries by integrating MCP Servers</h3><p>I’ve already mentioned the <strong>Figma MCP Server</strong> and how crucial it is for bridging design and code in my <a href=""https://medium.com/design-bootcamp/designing-with-mcp-server-bridging-design-systems-and-ai-for-developer-friendly-prototypes-4f08b0a0881d"">previous article</a>. What unlocks Claude Code’s and Codex CLI’s potential even further is their ability to interact with a wider range of <strong>external toolsets</strong> to perform actions that were previously impossible.</p><p>By connecting them to multiple MCP Servers, my CLI environment became much more than a local sandbox. It could navigate, test, and verify components in real time on <strong>Storybook</strong>, or fetch design context directly from Figma, all within a single prompt.</p><figure><img alt=""A diagram showing how Claude Code integrates with external tools via MCP servers. “Claude Code” sits at the center, connected to the “Playwright MCP” on the left and “Figma MCP” on the right. Each MCP lists its available tools — Playwright includes browser navigation and form tools, while Figma includes screenshot, metadata, and design context tools. The diagram illustrates how Claude Code uses these integrations to test interfaces and fetch design data."" src=""https://cdn-images-1.medium.com/max/1024/1*N4tiPRMhYSGA4AhVKFR8nw.png"" /><figcaption><em>MCP integrations connecting Claude Code to tools from Playwright and Figma.</em></figcaption></figure><p>The diagram above shows two of my most valuable integrations. One way to provide Claude with feedback is by sharing a screenshot, asking for a visual misalignment fix. A better way is to let Claude experience the misalignment itself, opening the browser, inspecting the interface, and analyzing the issue in real-time.</p><p>This is one of the many cases where <strong>Playwright MCP</strong> and its browser-level tools allowed Claude to test component interactions, collect console data, or even simulate user behavior during validation.</p><p>Combined with Figma MCP and other integrations, these workflows become truly agentic and automated. Subagents can now run these tools independently to perform validation checks while pulling design context from Figma and behavioral data from the browser simultaneously.</p><h3>Exploring Codex CLI</h3><p>For my experiment, Claude Code acted as the builder, and Codex CLI as the reviewer. I initially started using Codex CLI to verify Claude Code’s implementation and plan fixes. It proved especially helpful for accessibility improvements and code refinement, often suggesting more consistent or efficient component structures.</p><p>At first, Codex CLI didn’t natively support the Figma MCP Server, so it could only access designs indirectly through Playwright MCP or shared screenshots. Despite that limitation, it achieved great results, mostly because the codebase was already well structured, and each React component was built using the correct design tokens, accurately reflecting the Figma original designs.</p><p>However, this changed recently since Figma MCP is now officially supported, allowing Codex CLI to access design data directly.</p><figure><img alt=""Codex CLI and Claude Code side‑by‑side terminals: Visual Studio Code showing two terminal windows side by side: the left terminal runs Codex CLI tasks, while the right terminal runs Claude Code, illustrating that both tools can operate concurrently"" src=""https://cdn-images-1.medium.com/max/1024/1*gAzLbOcVr9AkTybifYku9g.png"" /><figcaption><em>Codex CLI and Claude Code running in parallel on separate terminal windows.</em></figcaption></figure><h3>Which one to choose</h3><p>Although Codex provides tools and features similar to Claude Code, I found that each tool excelled in its own domain. Claude Code was better at structuring the project, setting up governance files, automating and defining workflows, while Codex CLI specialized in code refinement, improving readability, accessibility, and performance.</p><p>Both are also accessible through the web or their mobile apps, allowing you to connect to your GitHub repository remotely. They can answer questions about code architecture and implementation, fix bugs, and even start or manage coding tasks while you’re away from your laptop. Once Claude completes its work, you can review the changes, create pull requests directly from the app, and continue working from anywhere.</p><p>From my point of view, together, they formed a balanced collaboration: <strong>Claude Code defined the framework and logic, while Codex CLI perfected the implementation</strong>. This pairing turned my workflow into an iterative design-code cycle that continuously refined itself.</p><h3>From wireframes to prototypes</h3><p>Once code and design share the same foundation, prototypes stop being mere mockups; they become <strong>previews of production</strong>.</p><p>To test this, I decided to put my setup to the ultimate test: <strong>transforming a simple, hand-drawn wireframe into a working prototype with a single prompt</strong>. Both Claude Code and Codex CLI were asked to execute the same task, with the same prompt, wireframe, design tokens, and design system.</p><h4>The setup and wireframe input</h4><p>Both tools communicated with the same design file through Figma MCP Server, where each design component linked to its corresponding code implementation via Code Connect.</p><figure><img alt=""Code Connect snippet (React): A Code Connect panel containing a React code snippet for a Button component. It shows import statements and a &lt;Button&gt; element with props such as label=”Button”, variant=”primary”, size=”medium” and disabled"" src=""https://cdn-images-1.medium.com/max/1024/1*bAYpeiYisOSBRJgRIhqvEA.png"" /><figcaption><em>Once connected, the Code Connect snippet for each component surfaced its </em>React source code.</figcaption></figure><p>This connection ensured that both AI agents could pull from the same source of truth, including:</p><ul><li>The design tokens that define colors, typography, and spacing</li><li>The component structure that’s already aligned with the codebase</li><li>The GitHub repository</li></ul><p>For input, I provided a <strong>hand-drawn wireframe</strong> with a few annotations, the sort of sketch I’d usually share to communicate an idea to another designer. I intentionally kept it simple, and a bit imperfect on purpose, focusing on layout and hierarchy instead of visual refinement.</p><figure><img alt=""Hand‑drawn wireframe: A hand‑drawn wireframe sketch of a web page. It depicts a header at the top, a “My Work” section with three rectangular placeholders, a “Contact me” form with fields for full name, email and message, an “Any Questions?” area with checkboxes or dropdowns, and a footer at the bottom"" src=""https://cdn-images-1.medium.com/max/1024/1*8u-phWYYF6iMkx2c3RNqtg.png"" /><figcaption><em>The hand-drawn wireframe provided as input to the AI agents (not my proudest sketch!).</em></figcaption></figure><p>The generated page should use only existing components, replicating a real scenario where the prototype reflects production code. As it turns out, both Claude Code and Codex CLI handled this perfectly, using the existing code and confirming that the Code Connect mapping worked as expected.</p><p>Below you can see some of the key components in Storybook:</p><figure><img alt=""Storybook design system (GIF): A recording from inside Storybook showing various components like “Small Accordion.” The interface displays controls for properties like “descriptionText” and “expanded,” along with a description, while the left sidebar lists various components categories such as Buttons, Avatars and Alerts"" src=""https://cdn-images-1.medium.com/max/800/1*b4KPOxsSgLGu_e4Zd29iCg.gif"" /><figcaption><em>A look inside Storybook, with some of the key components used to generate the prototype. (GIF)</em></figcaption></figure><h4>The single prompt test</h4><p>To keep the comparison fair, I used the exact same prompt for both tools. The only variation was the phrase<em> “Ultrathink this task”</em>, added for Claude Code to activate its deeper reasoning mode. Everything else, from instructions to references, remained identical, ensuring the results reflected each tool’s true capabilities.</p><pre>Ultrathink this task. <br /><br />- I want you to implement the wireframe shown in [Image #5] as a new page <br />called ClaudeWireframeToPrototype, using our existing design system. <br />You must only use our current design tokens and components to recreate <br />this page.<br />- Do not rely on any previously implemented pages as a reference.<br />- You'll notice that the wireframe includes annotations to help you understand <br />the required components and specifications. For any elements that aren't <br />explicitly defined, use your judgment to apply the most appropriate existing <br />components or tokens.<br />- Capitalize all button labels and align their placement according to the <br />wireframe.<br />- Maintain a clear visual hierarchy across all UI elements.<br />- The page must be fully responsive, so make sure to implement responsive <br />breakpoints consistent with our existing patterns.<br />- You need to act as both a Senior Designer and a Senior Front-End Engineer <br />for this task. As a designer, carefully select the correct tokens and <br />components that align with our design system constraints. As an engineer, <br />implement the page using proper composition, semantic HTML, and the defined <br />tokens and components.<br />- Once the implementation is complete, use Playwright MCP tools to verify <br />button positioning, text capitalization, and visual hierarchy consistency. Fix <br />accordingly if any issues arise.</pre><p><strong>The goal wasn’t perfection; it was</strong> <strong>alignment</strong>. I wanted to see whether both agents could translate low-fidelity wireframes into a functional prototype using the right components and tokens.</p><p>Additionally, since these prototypes pull components and instructions directly from the codebase (or the Figma file), the resulting output wouldn’t just resemble production; it would be <strong>production-ready</strong>.</p><h4>Claude Code workflow</h4><p>Claude Code was the first to take on the challenge. I used its <em>ultrathink</em> capability and <em>plan mode</em>, allowing it to thoughtfully execute this task while separating research and planning from code execution.</p><p>Once the implementation was complete, I prompted it to use Playwright MCP Server to get visual context from its implementation and verify that everything was in place. It took approximately <strong>14 minutes</strong> to complete the implementation, resulting in a responsive behavior and proper use of components and design tokens such as typography, colors, etc.</p><figure><img alt=""Claude’s implementation result: Side‑by‑side image comparing the original hand‑drawn wireframe to the page generated by Claude Code. The final page faithfully reproduces the wireframe’s layout with a header, a grid of content cards, a contact form and a questions section, followed by a dark footer"" src=""https://cdn-images-1.medium.com/max/1024/1*Wz19fWi3x7mHYejMYW22Zg.png"" /><figcaption><em>A visual of the final result implemented by Claude.</em></figcaption></figure><p>Although the implementation looked great, there was an issue with importing the header component properly on tablet and mobile viewports. However, this got easily fixed with a simple prompt right after.</p><p>There was definitely more room for improvement; however, I only intended to test what the first iteration looks like with no major refinements or changes. For what it’s worth, <strong>it overall managed to pull the right components and use them as expected, delivering a satisfactory result.</strong></p><p>You can watch Claude Code working on this task as I recorded its progress and present it through this video:</p><a href=""https://medium.com/media/ba08873025d6b4e7b2e11278b8a1cb97/href"">https://medium.com/media/ba08873025d6b4e7b2e11278b8a1cb97/href</a><h4>Codex CLI workflow</h4><p>Next, I gave Codex CLI the same brief and design input. Codex approached the task a bit differently, being less conversational and more autonomous in its decisions.</p><figure><img alt=""Codex implementation result: Side‑by‑side image comparing the wireframe to the page generated by Codex CLI. The final page resembles the desired layout but adds unexpected category labels and descriptions above each section; the contact form layout differs slightly, and a dark footer appears at the bottom"" src=""https://cdn-images-1.medium.com/max/1024/1*uWYR-ZelpF01WYvKKkg20A.png"" /><figcaption><em>A visual of the final result implemented by Codex.</em></figcaption></figure><p>Its process ran for about <strong>20 minutes</strong>, slightly longer than Claude’s execution. The outcome, however, wasn’t ideal. All sections included an unexpected category label above each title and a short description underneath. The contact form layout also deviated from the original wireframe, while the logo wasn’t imported properly, and several issues were spotted on the responsive viewports.</p><p>These were relatively minor issues that could have been easily fixed with a few additional prompts. However, I intentionally chose not to do that, as my goal was to test how both tools would perform under identical conditions using just one prompt. It’s worth noting, though, that in other tests I conducted, Codex produced much stronger visual results and more sophisticated code, particularly in accessibility and structural logic, where Claude occasionally fell short.</p><p>You can watch Codex executing this task in my video below:</p><a href=""https://medium.com/media/c67dee28e17e955b25e1919b23a241b5/href"">https://medium.com/media/c67dee28e17e955b25e1919b23a241b5/href</a><h3>Takeaway</h3><p>In this experiment, <strong>Claude Code delivered a stronger overall result</strong>, while Codex took a few initiatives when it shouldn’t have. That was not a deal breaker, but for this single-prompt test, Claude was the clear winner.</p><p>What truly matters, though, is what this workflow unlocks. By combining tools like <strong>Claude Code</strong>, <strong>Codex CLI</strong>, and <strong>Figma MCP Server</strong>, designers can now transform wireframes or Figma designs into interactive, production-ready prototypes.</p><p>This approach accelerates ideation, bridges design and engineering, and enables early feedback from stakeholders or even users on pages powered by real, working code.</p><h3>What’s next</h3><p>From my perspective, the next step is <strong>bringing generated screens back into Figma</strong> in a way smart enough to detect components from code, translate design tokens into variables, and replicate auto-layout behavior directly from implementation. It only feels like the natural evolution of this workflow, and I think we’re getting very close to it.</p><p>Most importantly, this experiment proved that <strong>AI doesn’t replace design; it complements it</strong>. It can safely become part of both the design and production workflow, not by generating random layouts or meaningless code, but by following specific guidelines defined by the designer.</p><p>What becomes really essential is the <strong>design system</strong> itself. Now is the time to invest in them, especially after Figma’s latest <a href=""https://www.figma.com/blog/schema-2025-design-systems-recap/""><strong>Schema 2025</strong></a> updates, including features like <strong>Extended<em> </em>Collections</strong>, <strong>Slots</strong>, and more.</p><p>At the end of the day, it’s up to each organization to decide how it wants to move forward. Some may prefer quick solutions that generate designs or prototypes detached from any codebase, while others, especially larger teams with established systems and legacy code, will benefit more from connected workflows like this, where AI integrates with existing infrastructure.</p><p><strong>Design systems are now at a defining moment</strong>, proving their value not only to humans but also to machines. Structure will always take time, and it’s not always appreciated since it’s invisible, but it remains a small yet crucial investment toward a much greater opportunity; one where AI aligns with both the designer’s and developer’s intent to produce maintainable code and realistic, production-ready prototypes.</p><h3>Thanks for reading!</h3><p>👋 I’m Iasonas, a Lead Product Designer, currently exploring how better tooling can bring design and development closer together. I write about design tools, AI-powered prototyping, and the intersection of UX and engineering.</p><p>🔗 Feel free to connect with me on <a href=""https://www.linkedin.com/in/iasonas-georgiadis/"">LinkedIn</a> or check more of my work at <a href=""https://www.iasonasgeorgiadis.com"">iasonasgeorgiadis.com</a>.</p><p>♻️ If you enjoyed this article, feel free to follow or reach out. I’m always up for a conversation over coffee about good design and better handoffs</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f10c136ec11f"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/designing-with-claude-code-and-codex-cli-building-ai-driven-workflows-powered-by-code-connect-ui-f10c136ec11f"">Building AI-driven workflows powered by Claude Code and other tools</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/how-estonia-ai-gardens-and-plywood-make-designers-prolific-039a356465ca?source=rss----138adf9c44c---4,1761688350,"How Estonia, AI gardens and plywood make designers prolific","How Estonia, AI gardens and plywood make designers prolific

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/how-estonia-ai-gardens-and-plywood-make-designers-prolific-039a356465ca?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/2600/1*bEqCMhRh-iug-6VSJ7cs8w.png"" width=""3295"" /></a></p><p class=""medium-feed-snippet"">From Bauhaus flats to digital nations, creativity needs fertile ground</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/how-estonia-ai-gardens-and-plywood-make-designers-prolific-039a356465ca?source=rss----138adf9c44c---4"">Continue reading on UX Collective »</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/recorded-design-walkthroughs-the-best-way-to-give-your-design-a-voice-ae0cba6eafbb?source=rss----138adf9c44c---4,1761650577,Recorded design walkthroughs: the best way to give your design a voice,"Recorded design walkthroughs: the best way to give your design a voice

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/recorded-design-walkthroughs-the-best-way-to-give-your-design-a-voice-ae0cba6eafbb?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/2600/1*84YxZKf4NVoKMD69Tfcg9Q.jpeg"" width=""4240"" /></a></p><p class=""medium-feed-snippet"">How to make design work understandable with teams in different time zones</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/recorded-design-walkthroughs-the-best-way-to-give-your-design-a-voice-ae0cba6eafbb?source=rss----138adf9c44c---4"">Continue reading on UX Collective »</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/design-prompt-building-interfaces-b362b6614fa3?source=rss----138adf9c44c---4,1761650554,Design prompt-building interfaces,"Design prompt-building interfaces

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/design-prompt-building-interfaces-b362b6614fa3?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/2495/1*yj810TGI94uNMf2PB8SIWQ.png"" width=""2495"" /></a></p><p class=""medium-feed-snippet"">How to help users articulate their intents strategically</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/design-prompt-building-interfaces-b362b6614fa3?source=rss----138adf9c44c---4"">Continue reading on UX Collective »</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/copiloting-a-perceptual-uniform-color-triad-921b1eea3f52?source=rss----138adf9c44c---4,1761595639,Copiloting a perceptual uniform color triad,"Copiloting a perceptual uniform color triad

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/copiloting-a-perceptual-uniform-color-triad-921b1eea3f52?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/1966/1*yaQyzbZHXRZ8kqHFoAUkag.png"" width=""1966"" /></a></p><p class=""medium-feed-snippet"">An adventure in Gen AI design.</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/copiloting-a-perceptual-uniform-color-triad-921b1eea3f52?source=rss----138adf9c44c---4"">Continue reading on UX Collective »</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/hey-designer-do-you-know-what-you-are-doing-aaec674d5532?source=rss----138adf9c44c---4,1761567586,"Hey designer, do you know what you are doing?","Hey designer, do you know what you are doing?

<h4>How to build your design competence in the age of AI.</h4><figure><img alt=""The image shows a person opening a door, symbolizing the unlocking of new knowledge"" src=""https://cdn-images-1.medium.com/max/1024/1*nfHFozEsJo7EVnuHv2KNUA.png"" /><figcaption>Unlocking New Knowledge. Source: Filipe Nzongo (2025)</figcaption></figure><p>Being a designer has always been synonymous with being creative, transforming abstract and complex ideas into concrete solutions. However, in recent years, many professionals have been losing this creative spark, and with it, the very essence of the profession. We’ve witnessed an exponential growth of artificial intelligence tools within the creative industry. This, in itself, isn’t a problem; the real risk lies in becoming dependent on these tools.</p><p>Abandoning your own creative capacity and relying on AI to generate design concepts or evaluate solutions is a mistake that can compromise your professional autonomy and creative confidence.</p><p>In this article, I want to discuss why you shouldn’t rely solely on artificial intelligence tools, but instead focus on building your intellectual foundation as a designer. This foundation develops through both theoretical study and consistent practice in real projects; over time, it becomes the cornerstone of your <a href=""https://sci-hub.ru/https://doi.org/10.1007/978-3-030-51038-1_36"">repertoire and professional competence</a>.</p><h3>How to build knowledge in design</h3><p>Before discussing knowledge in design, it’s important to first understand what knowledge actually is. According to the dictionary, knowledge is the act of knowing or the state of being aware of something, whether through reason or experience. This definition highlights three central elements that deserve attention:</p><ul><li><strong>Act of knowing:</strong> understanding a phenomenon — knowing what it is, how it works, and why it works in a certain way.</li><li><strong>Through reason:</strong> acquiring knowledge through reflection, study, and theory.</li><li><strong>Through experience:</strong> referring to tacit knowledge, which is built over time through practice and observation of reality.</li></ul><p>When we bring this definition into the field of design, <a href=""https://dl.designresearchsociety.org/cgi/viewcontent.cgi?article=2267&amp;context=drs-conference-papers"">knowledge in design</a> means mastering the principles, philosophies, methods, and practices that help us to solve problems in a structured and creative way.</p><blockquote>In the context of interaction design or digital product design, the nature of this knowledge is particularly complex. There is no universal list of skills that defines a good designer.</blockquote><p><a href=""https://medium.com/u/68e9c90252b3"">André Grilo, Ph.D.</a> (2023), addresses this issue in his <a href=""https://www.researchgate.net/publication/379221346_Competencias_em_Experiencia_do_Usuario_no_Design_de_Produto_Digital"">doctoral thesis</a> when discussing the competencies of digital product designers. According to Grilo, the field still lacks consensus on which knowledge is essential, precisely because the discipline continues to evolve.</p><p>Even so, companies set expectations, anticipating that designers will perform certain activities and deliver specific results, which is often mistaken for actual professional knowledge.</p><p>This is where we need to be careful: a job description is not synonymous with knowledge. A job description talks about tasks; knowledge concerns the judgment, understanding, and autonomy required to execute those tasks with insight. As Grilo (2023, p.94) points out:</p><blockquote><strong>“There are some epistemological issues in UX Design (UXD) that, in my view, are unavoidable in this theoretical discussion. The first concerns the scope of UX Design, due to its wide range of responsibilities. The second revolves around the relationship of UX with diffuse problems. The third relates to the specificity of User Experience as a construct within the theory of Digital Product Design.”</strong></blockquote><p>Several authors have attempted to define what interaction design and UX are, as well as how the discipline is organized, including Garrett (2001), Saffer (2006), and Six (2010). However, most focus on describing the discipline and its connections with other fields, without clearly specifying the types of knowledge a UX designer must master to excel in their practice.</p><p>Let’s take a look at the famous diagram by <a href=""https://medium.com/u/534e55e898c0"">Dan Saffer</a> (2008). While it is useful for visualizing the intersections and scope of the discipline, the diagram by itself does not reveal anything about the types of knowledge and competencies required for the practice of an interaction designer or a UX designer.</p><figure><img alt=""The image illustrates the disciplines and interdisciplinary connections within UX design"" src=""https://cdn-images-1.medium.com/max/1024/0*lPHCuGVD-NU8jxal.jpg"" /><figcaption>Disciplines and Interdisciplinarity in UX. Source: Dan Saffer (2008)</figcaption></figure><p>It is common for designers to feel confused when it comes to professional competencies and skills. The question often arises: Should I know everything about visual design? human factors or usability engineering? This confusion is natural when there is no clear competency model to guide professional development in the field.</p><p>Sometimes, designers wonder which skills are truly necessary to work as an interaction designer or UX designer. There was a time when mastering tools like Photoshop, Dreamweaver, Fireworks, and Flash was enough to be considered a competent interaction designer or web designer. That period produced many excellent software operators, but not all of them were truly designers.</p><p>When many of these professionals were questioned about their design decisions, there was often surprise or even responses like: “Why do you want to know how I did it?” There has always been a certain mystery surrounding designers’ decisions. I believe this is, in part, one of the reasons why our relationship with other professionals, such as developers, has historically been complicated.</p><p>Being unable to justify a design decision is as incoherent as hearing a surgeon, when questioned about a decision during surgery, say that they don’t know or cannot explain. Yet, it is still common to find designers who cannot articulate the reasoning behind their actions, a clear sign of a lack of reflection and professional maturity.</p><p>Being a designer goes beyond knowing how to use tools. The new generation needs to be careful not to fall into the same trap, now fueled by the use of artificial intelligence. The risk is producing professionals who master prompt engineering but do not truly understand the problems they are solving.</p><p>Building knowledge in design requires practice, reflection, and purpose. I personally developed my competence by working directly in the discipline, learning from projects, mistakes, and discoveries, rather than relying on tools. A designer needs to know how to do something and, above all, know why they are doing it that way.</p><p>Every design decision needs to be intentional, never vague or accidental. Design is, by nature, an act of choice, and every choice reveals an intention, whether conscious or not. As I discussed in my article <strong>“</strong><a href=""https://brasil.uxdesign.cc/quando-a-intencao-orienta-o-design-3c603728254f""><strong>When intention drives design</strong></a><strong>”</strong>, designing without intention is like navigating without a compass: you may reach somewhere, but it is unlikely to be the right destination.</p><blockquote>Intentionality is what distinguishes the designer as an author from the designer as an operator. The former creates meaning; the latter merely executes commands.</blockquote><p>Therefore, to build knowledge in design, designers need to understand which competencies they need to develop. Theory can be useful for this, helping to cultivate what Schön calls <a href=""https://libguides.hull.ac.uk/reflectivewriting/schon"">reflection-in-action</a>, <a href=""https://libguides.hull.ac.uk/reflectivewriting/schon"">reflection on action</a>, and <a href=""https://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=1390&amp;context=eandc"">knowledge-in-action.</a></p><p>However, designers must not forget to apply their knowledge in practice, as this is the ground where understanding is validated and transformed. It is through practice that new theories emerge, new solutions are tested, and the designer truly becomes a reflective practitioner and creator of knowledge.</p><h3><strong>Types of knowledge in design</strong></h3><p>The types of knowledge in design vary greatly, depending on the professional’s experience and the context in which they work. For instance, a digital product designer working in the finance sector will have different competencies from someone working in healthcare, dealing with domain-specific challenges, regulations, and user expectations.</p><p>In the finance sector, there are mandatory trainings related to anti-money laundering, as well as regulations from the central bank, that drive the development of financial software, knowledge that a designer in the healthcare sector may not possess or need. Additionally, the design standards and UX practices applied in each sector are also completely distinct.</p><p>If we analyze the <a href=""https://elizabethbacon.com/the-ux-sundial-model-iterated-now-including-a-0/"">UX competencies</a> proposed by Elizabeth Bacon (2009), we notice a generalization that does not always reflect the reality of the designer. What the professional should know to move beyond tool proficiency and build true intellectual and creative depth.</p><figure><img alt=""The image illustrates the field of User Experience (UX)"" src=""https://cdn-images-1.medium.com/max/900/0*pUzMClic7_e2daTC.png"" /><figcaption>Field of User Experience. Source: Elizabeth Bacon (2009)</figcaption></figure><p>In the model, disciplines such as information architecture, service design, and interaction design are treated collectively, as if they shared the same set of knowledge. In practice, each of these areas has distinct philosophies, principles, and methods. Evaluating a designer’s knowledge based on such a generalized model can be problematic.</p><p>This reflection reinforces the need for each designer to build their own repertoire and develop critical judgment, adapting their knowledge to the real demands of the context in which they work, rather than relying solely on theoretical models. What will differentiate you from the majority is not following generic standards, but the unique knowledge that only you possess.</p><p>To explore this topic further, I recommend reading my article <a href=""https://uxdesign.cc/becoming-good-at-something-619c1a61e924""><strong><em>“Becoming Good at Something”</em>.</strong></a></p><h3>The knowledge development model</h3><p>Considering the complexity involved in defining the types of knowledge a designer should possess, I propose that knowledge development in design should be structured into three levels: <strong>declarative knowledge, procedural knowledge, and reflective knowledge.</strong></p><p>This approach applies regardless of whether the professional works as an interaction designer, service designer, or industrial designer.</p><figure><img alt=""The image illustrates the knowledge model proposed by Filipe Nzongo."" src=""https://cdn-images-1.medium.com/max/1024/1*AkNv8j4DyEYLAmw5Yyz9Sg.png"" /><figcaption>Knowledge model. Source: Filipe Nzongo (2025)</figcaption></figure><h3><strong>Declarative knowledge (knowing what)</strong></h3><p>Declarative knowledge refers to the factual information an individual possesses, including both general and specific knowledge about their environment. <a href=""https://link.springer.com/article/10.1007/s43546-022-00402-3"">Ahmet Demir <em>et al.</em> (2022)</a> explain that declarative knowledge relates to an individual’s understanding and intentions, which are crucial for problem-solving and decision-making.</p><blockquote>In other words, declarative knowledge is connected to the process of learning and recalling information that one already knows.</blockquote><p>Wikipedia provides a broader definition:</p><blockquote><strong>“Declarative knowledge is the awareness of facts that can be expressed through declarative sentences. It is also called theoretical knowledge, descriptive knowledge, propositional knowledge, or knowledge-that. It is not restricted to a specific use or purpose and can be stored in books or computers.”</strong></blockquote><p>In my model, I start with declarative knowledge because it establishes the foundation upon which all other learning in design is built. If you want to work as an interaction designer, service designer, or information designer, it is essential to understand the fundamental principles of the discipline, such as typography, visual hierarchy, information architecture, usability heuristics, user-centered design, and so on.</p><p>This type of knowledge is crucial because it enables designers to understand the foundations of their discipline before advancing to specific skills, such as prototyping techniques or the use of generative AI tools. Without this foundation, any practice or decision risks being superficial or disconnected from the critical reasoning necessary to solve complex design problems.</p><p>In declarative knowledge, the focus is on understanding theory. For example, knowing that Jakob Nielsen and Rolf Molich created the 10 usability heuristics in 1990 for evaluating interactive products, or understanding what the <a href=""https://www.nngroup.com/articles/two-ux-gulfs-evaluation-execution/"">gulfs of execution and evaluation</a> are and their purpose in interface evaluation.</p><p>Furthermore, declarative knowledge serves as a reference for procedural knowledge (knowing how to perform tasks) and reflective knowledge (understanding why a design decision works or fails). It is the starting point for building repertoire and autonomy: knowing what allows the designer to understand the rules of the game before playing it creatively and strategically.</p><h3><strong>Procedural knowledge (Knowing how)</strong></h3><p>Procedural refers to knowledge that depends on a process, rules, or steps. According to <a href=""https://books.google.no/books/about/The_First_person_Point_of_View.html?id=-s7eoQEACAAJ&amp;redir_esc=y"">Wolfgang Carl (2014)</a>, procedural knowledge, also called know-how or practical knowledge, is the type of knowledge that manifests in the ability to perform actions effectively, sometimes referred to as imperative or performative knowledge.</p><blockquote><strong>This is the second level of knowledge a designer needs to develop. The model is flexible and can be adapted to any design discipline, serving as a guide to structure learning, practice, and professional development.</strong></blockquote><p>In interaction design or digital product design, this knowledge is reflected in a designer’s ability to carry out concrete tasks, such as conducting a heuristic evaluation, designing user interfaces, prototyping, or organizing navigation flows. In other words, it is not enough to know the theory; one must know how to apply it in practice and demonstrate tangible results.</p><p>There is a fusion between knowing what (declarative knowledge) and knowing how (procedural knowledge), and it is precisely this combination that makes all the difference in a designer’s career. A professional who only masters the theory may understand concepts but will not be able to transform that knowledge into concrete solutions.</p><p>Let’s observe the sketch below, which I created as a simple concept to illustrate the importance of having both declarative and procedural knowledge. When a designer receives a sketch like this from a product manager or design director, they must be able to translate the conceptual idea into a real, functional design.</p><p>However, this transformation requires the designer to have strong declarative knowledge, in other words, a solid grasp of the theory that guides the design process. Being able to translate this sketch into a coherent, consistent, and aesthetically pleasing interface is what separates a professional from an amateur.</p><figure><img alt=""The image shows a user interface concept sketch for an e-commerce checkout."" src=""https://cdn-images-1.medium.com/max/1024/1*iov5AtXIeY6CeXz8u0KeWQ.png"" /><figcaption>User Interface concept for E-commerce. Source: Filipe Nzongo (2018)</figcaption></figure><p>Furthermore, procedural knowledge complements this process by enabling the designer to execute the translation effectively, choosing the right <a href=""https://www.eleken.co/blog-posts/grid-layout-design-history-tips-and-best-examples"">visual hierarchy, layout structure</a>, <a href=""https://en.wikipedia.org/wiki/Interaction_design_pattern"">interaction patterns</a>, and <a href=""https://improvement.stanford.edu/resources/usability-principles"">usability principles</a> to bring the concept to life. It’s not only about knowing what should be done, but also how to do it in a way that preserves both functionality and aesthetic integrity.</p><p>In practice, this means understanding how each design decision affects user perception and behavior, for instance, where to place primary actions or how to balance visual weight. These are not random choices; they emerge from a deep understanding of <a href=""https://dovetail.com/product-development/human-computer-interaction/"">HCI principles</a> and accumulated experience through continuous reflection and iteration.</p><p>Ultimately, the ability to move from concept to execution, grounded in both theoretical and practical knowledge, is what defines true design mastery.</p><figure><img alt=""The image shows a user interface concept for an e-commerce checkout."" src=""https://cdn-images-1.medium.com/max/1024/1*_o7AlwYIRI_V0cgklhWvxw.png"" /><figcaption>User Interface concept for E-commerce. Source: Filipe Nzongo (2018)</figcaption></figure><p>The concept above encapsulates several design principles, such as <a href=""https://www.nngroup.com/articles/chunking/"">chunking</a>, where information is divided into clear, logical sections; <a href=""https://www.interaction-design.org/literature/topics/gestalt-principles?srsltid=AfmBOoqWJmStmoUZ-85rWbwVkVpfVIiBBSNFcbYNlrIr0FqiSThc9iOV"">Gestalt principles</a>, where related elements are positioned close to one another; and contrast, achieved through the use of <a href=""https://www.nngroup.com/articles/visual-hierarchy-ux-definition/"">visual hierarchy</a> to guide the user’s eye. I also applied alignment to create order and readability, and <a href=""https://www.interaction-design.org/literature/article/the-power-of-white-space?srsltid=AfmBOophvmeT4OhmzUA4AXQ7OPiDZioAFQb3h6bbLcgHlm8YZ7vdEiCI"">white space to reduce visual clutter, improve legibility</a>, and allow the eyes to rest during scanning.</p><p>Additionally, the design adheres to good practices from Nielsen’s usability heuristics, such as prioritizing <a href=""https://www.nngroup.com/articles/recognition-and-recall/"">recognition over<em> recall</em></a>. For instance, using logos (“FedEx,” “UPS,” “Sedex” etc.) instead of plain text makes it easier for users to recognize and select an option quickly.</p><blockquote>An interface is never just a simple arrangement of elements; it embodies a series of reflections and intentional decisions made by the designer to make it what it is.</blockquote><p>Through this example, I aim to demonstrate that being a designer extends far beyond knowing how to operate tools; it’s about understanding the rationale behind every design decision and the purpose of the proposed solution.</p><h3><strong>Reflective knowledge (Knowing why)</strong></h3><p>Reflective knowledge is the ability to understand why we are approaching a design problem in a certain way. It goes beyond knowing what (declarative) and knowing how (procedural); it involves understanding the reasons behind choices, anticipating consequences, and adjusting decisions based on context and experience.</p><p>Donald Schön calls this <a href=""https://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=1390&amp;context=eandc"">reflection-in-action</a>, which is real-time thinking during the execution of a task, when the designer observes, adjusts, and decides while acting on the problem. He also describes reflection-on-action, which is the retrospective analysis of what happened after the action, allowing one to learn from experience and improve future design decisions.</p><p>In the design context, professionals are constantly alternating between these two modes of reflection. When testing an interface, for example, a designer immediately evaluates the impact of a change (reflection-in-action) and later revisits the process to understand what worked or didn’t, and why (reflection-on-action).</p><p>This continuous cycle builds repertoire, strengthens critical judgment, and enables designers to make informed decisions without relying exclusively on external tools, such as AI.</p><h3><strong>AI is an ally, not a substitute</strong></h3><p>Artificial Intelligence has become a widely discussed topic among designers. Yet, the question that arises most often is: <strong>Will we be replaced?</strong> It’s still too early to give a definitive answer.</p><p>Although the exponential growth of generative AI worries many designers, it’s important to emphasize that it is an ally, not a replacement for human competence. We still master our craft. This isn’t the first time designers have faced new technologies, and adaptation has always been part of the discipline’s natural evolution.</p><p>When I say AI is an ally, I mean it can be great for accelerating repetitive tasks, organizing information, or generating initial design alternatives — but it doesn’t guarantee creativity or critical judgment.</p><blockquote>A designer’s judgment is grounded in declarative, procedural, and reflective knowledge — the foundation of their professional repertoire.</blockquote><p>This repertoire enables designers to perceive nuances, prioritize problems, and propose appropriate interventions. AI, no matter how advanced, lacks this repertoire: it doesn’t <em>see</em> the interface the way you do, nor does it understand the intention behind your design choices.</p><p>In my article <a href=""https://uxdesign.cc/how-can-humans-and-intelligent-computers-work-together-d349328ce270""><strong>How can humans and computers work together</strong></a>, I discuss how our relationship with intelligent machines should be one of mutual support, not dependence.</p><p>What new-generation designers must be careful about is becoming overly dependent on AI. My appeal to designers is this: let’s not outsource our cognitive capacity. <a href=""https://www.mdpi.com/2075-4698/15/1/6"">Research shows that excessive reliance on AI can lead to dependency and a reduction in cognitive ability.</a> Moderation is always essential.</p><p>In the past, when I worked as a graphic designer, even though there were plenty of websites offering graphic bundles and mockups, I made a point of creating my own. For a simple reason: knowing how to make something is better than just using something ready-made. Moreover, creating your own assets helps you stand out; it’s what builds repertoire, practical skills, and professional identity.</p><p>For digital product designers, for example, AI can generate a list of applicable heuristics, but it’s up to you, the designer, to actually evaluate the interface: to identify which heuristics have been violated, which have not been met, and from there, propose solutions to address the interface’s problems.</p><p>Without the kinds of knowledge I described earlier, you won’t be able to spot flaws, let alone suggest improvements, much like trying to judge a case without knowing the laws that govern it.</p><p>Learn to evaluate an interface like a specialist. That’s what allows you to grow as a professional and develop practical, <a href=""https://design-instability.com/tacit-knowledge-and-skills"">tacit knowledge</a>. A designer must be capable of tackling the problem in front of them, not through external tools, but through the knowledge developed over years of study and practice.</p><p>Your intellectual repertoire grows through practice, study, and reflection. Every interface you evaluate, every prototype you test, every design decision you discuss with peers, all of it contributes to building your judgment. This accumulation of experience is what turns information into applicable knowledge, not just data that can be fed into a machine.</p><p>A designer cannot expect AI to do everything for them. They must build their knowledge through study and hands-on experience — collaborating with other professionals, both more and less experienced. Use AI as your creative ally, not your crutch.</p><h3><strong>Conclusion</strong></h3><p>To conclude, the message I want to leave for designers is that, in the age of AI, what will truly set you apart in an increasingly competitive market is your intellectual capacity. Building knowledge is more important than mastering tools. New technologies come and go, but knowledge remains. Knowing how to create, thinking critically, and reflecting on your own work are values no one can take away from you.</p><p>What distinguishes an excellent designer from an amateur is not the speed of delivery, but the ability to create projects aligned with both user and business goals, combined with solid technical mastery.</p><p>That’s why you should prioritize building knowledge in design: study, practice, and talk to people who know more than you. Mastering a tool doesn’t make you a designer. Next, learn to identify what you don’t know yet, map your knowledge gaps, and work to fill them with new skills that will drive your professional growth.</p><p>Finally, understand that AI is an ally, not a substitute for human competence. Use it as support, but maintain your ability to think, create, and decide for yourself. Developing creative autonomy is what will ensure you remain relevant, with or without the tools of the moment.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=aaec674d5532"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/hey-designer-do-you-know-what-you-are-doing-aaec674d5532"">Hey designer, do you know what you are doing?</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/aws-outage-writing-is-designing-the-secret-life-of-prototypes-2b7ad74ca787?source=rss----138adf9c44c---4,1761564525,"AWS outage, writing is designing, the secret life of prototypes","AWS outage, writing is designing, the secret life of prototypes

<h4>Weekly curated resources for designers — thinkers and makers.</h4><figure><a href=""https://uxdesign.cc/the-day-the-internet-crashed-what-the-aws-outage-teaches-us-about-dependancies-cf0aa066cd1c?sk=3112a031662a92398245ce77e95118f0""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*X8noD8RyYoTEN8ib.png"" /></a></figure><p>“AWS, often called the ‘backbone of the internet,’ supports essential tools like Slack, Zoom, and Office 365, as well as popular games like Fortnite and Roblox. It also powers financial services such as Coinbase and Venmo. Even the coffee we order through the Starbucks app relies on AWS technology. (…)</p><p>Millions of users were locked out, delayed, and frustrated, facing broken workflows and halted transactions. For businesses large and small, hours of downtime translated into millions in lost productivity and revenue.”</p><p><a href=""https://uxdesign.cc/the-day-the-internet-crashed-what-the-aws-outage-teaches-us-about-dependancies-cf0aa066cd1c?sk=3112a031662a92398245ce77e95118f0""><strong>What the AWS outage teaches us about dependencies</strong></a><strong> →<br /></strong>By <a href=""https://medium.com/u/af434fd67626"">Ian Batterbee</a></p><h3><strong>Editor picks</strong></h3><ul><li><a href=""https://uxdesign.cc/designing-the-jarvis-moment-8f1a2f201d4a""><strong>Designing the Jarvis moment</strong></a><strong> →</strong><br />Apps SDK, design principles, and the future of contextual UX.<br />By <a href=""https://medium.com/u/5881151a6f2c"">Hao Liu 柳灝</a></li><li><a href=""https://uxdesign.cc/spotify-has-an-ai-problem-a4d6729c715b""><strong>Spotify has an AI problem</strong></a><strong> →</strong><br />A platform flooded with songs made by no one.<br />By <a href=""https://medium.com/u/e7efcd3b7bd3"">Allan MacDonald</a></li><li><a href=""https://uxdesign.cc/making-medical-appointments-this-month-made-me-miss-phone-trees-e1bc3757e505?sk=093cd0021e55c108a6b759be59348af8""><strong>A tale of UX, AI, and medical practices</strong></a><strong> →</strong><br />Making medical appointments this month made me miss phone trees.<br />By <a href=""https://medium.com/u/e5a420850009"">Chris Raymond</a></li></ul><p><em>The UX Collective is an independent design publication that elevates unheard design voices and helps designers think more critically about their work.</em></p><figure><a href=""https://www.navbar.gallery/?ref=sidebar""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*AudiQzkY4WUkLzpJ.png"" /></a></figure><p><a href=""https://www.navbar.gallery/?ref=sidebar""><strong>Website navbar design inspiration</strong></a><strong> →</strong></p><h3>Make me think</h3><ul><li><a href=""https://frankchimero.com/blog/2025/beyond-the-machine/?ref=sidebar""><strong>Beyond the machine: creative agency in the AI landscape</strong></a><strong> →</strong><br />“I’m trying to figure out how to use generative AI as a designer without feeling like shit. I am fascinated with what it can do, impressed and repulsed by what it makes, and distrustful of its owners. I am deeply ambivalent about it all. The believers demand devotion, the critics demand abstinence, and to see AI as just another technology is to be a heretic twice over.”</li><li><a href=""https://www.anildash.com//2025/10/17/the-majority-ai-view/?ref=sidebar""><strong>The majority AI view</strong></a><strong> →</strong><br />“Even though AI has been the most-talked-about topic in tech for a few years now, we’re in an unusual situation where the most common opinion about AI within the tech industry is barely ever mentioned.”</li><li><a href=""https://www.baldurbjarnason.com/2025/the-inevitability-of-anger/?ref=sidebar""><strong>The inevitability of anger</strong></a><strong> →</strong><br />“You can’t fully avoid anger in your life. Even if you aren’t the type to get angry, sometimes the world just steps up and puts you in a position where your coping mechanisms break down and unregulated emotion breaks to the surface, usually in the form of an unproductive outburst.”</li></ul><h3>Little gems this week</h3><figure><a href=""https://uxdesign.cc/when-innovation-deceives-escaping-the-value-mirage-bcbf5511aecf?sk=36a4f40197761c31821c81560d6d832d""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*91Vruf_qnFMdJ9tw.png"" /></a></figure><p><a href=""https://uxdesign.cc/when-innovation-deceives-escaping-the-value-mirage-bcbf5511aecf?sk=36a4f40197761c31821c81560d6d832d""><strong>When innovation deceives: Escaping the value mirage</strong></a><strong> →<br /></strong>By <a href=""https://medium.com/u/af434fd67626"">Ian Batterbee</a></p><figure><a href=""https://uxdesign.cc/right-narratives-shape-lasting-products-9a50e28caae9""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*_beJsTmhXeP2tGJT.png"" /></a></figure><p><a href=""https://uxdesign.cc/right-narratives-shape-lasting-products-9a50e28caae9""><strong>Right narratives shape lasting products</strong></a><strong> →<br /></strong>By <a href=""https://medium.com/u/61ae6d11d226"">Mark Raja</a></p><figure><a href=""https://uxdesign.cc/the-writing-is-the-design-a8ccde162d35""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*VxmnI7deKOWmHWjS.png"" /></a></figure><p><a href=""https://uxdesign.cc/the-writing-is-the-design-a8ccde162d35""><strong>The writing is the design</strong></a> →<br />By <a href=""https://medium.com/u/3e9789243339"">Oluwatosin Obalana</a></p><h3>Tools and resources</h3><ul><li><a href=""https://uxdesign.cc/designers-often-do-invisible-work-that-matters-heres-how-to-show-it-f446c182d6a2?sk=deeb1901f86fb241211e12ef926031d4""><strong>How to show invisible work</strong></a><strong> →</strong><br />Designers often do invisible work that matters.<br />By <a href=""https://medium.com/u/808d6e761022"">Kai Wong</a></li><li><a href=""https://uxdesign.cc/the-secret-life-of-prototypes-a81abcdd6eda?sk=be66941579dbd302ed23c54a87d7928e""><strong>The secret life of prototypes</strong></a><strong> →</strong><br />Prototypes aren’t deliverables, they’re influencers.<br />By <a href=""https://medium.com/u/6bda956401f8"">Elaine</a></li><li><a href=""https://uxdesign.cc/typography-basics-5544d272a701""><strong>Typography basics</strong></a><strong> →</strong><br />A practical introduction to typography.<br />By <a href=""https://medium.com/u/f93f3824f683"">Bora</a></li></ul><h3>Support the newsletter</h3><p>If you find our content helpful, here’s how you can support us:</p><ul><li>Check out <a href=""https://bit.ly/uxc-div9"">this week’s sponsor</a> to support their work too</li><li>Forward this email to a friend and invite them to <a href=""https://newsletter.uxdesign.cc/"">subscribe</a></li><li><a href=""https://uxdesigncc.medium.com/sponsor-the-ux-collective-newsletter-bf141c6284f"">Sponsor an edition</a></li></ul><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2b7ad74ca787"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/aws-outage-writing-is-designing-the-secret-life-of-prototypes-2b7ad74ca787"">AWS outage, writing is designing, the secret life of prototypes</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
