source,domain,url,created_utc,title,text
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/designer-guide-eco-friendly-interfaces/,1771840800,A Designer’s Guide To Eco-Friendly Interfaces,"A Designer’s Guide To Eco-Friendly Interfaces

Every high-resolution hero image, autoplay video, and complex JavaScript animation carries a cost. Sustainable UX challenges the era of “unlimited pixels” and reframes performance as responsibility. In 2026, truly sophisticated design is defined not by how much it adds, but by how thoughtfully it reduces its footprint."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/designing-streak-system-ux-psychology/,1771426800,Designing A Streak System: The UX And Psychology Of Streaks,"Designing A Streak System: The UX And Psychology Of Streaks

What makes streaks so powerful and addictive? To design them well, you need to understand how they align with human psychology. Victor Ayomipo breaks down the UX and design principles behind effective streak systems."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/building-empathy-centred-ux-framework-mental-health-apps/,1770994800,Building Digital Trust: An Empathy-Centred UX Framework For Mental Health Apps,"Building Digital Trust: An Empathy-Centred UX Framework For Mental Health Apps

Designing for mental health means designing for vulnerability. Empathy-Centred UX becomes not a “nice to have” but a fundamental design requirement. Here’s a practical framework for building trust-first mental health products."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/designing-agentic-ai-practical-ux-patterns/,1770814800,"Designing For Agentic AI: Practical UX Patterns For Control, Consent, And Accountability","Designing For Agentic AI: Practical UX Patterns For Control, Consent, And Accountability

Autonomy is an output of a technical system. Trustworthiness is an output of a design process. Here are concrete design patterns, operational frameworks, and organizational practices for building agentic systems that are not only powerful but also transparent, controllable, and trustworthy."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/css-scope-alternative-naming-conventions/,1770278400,CSS <code>@scope</code>: An Alternative To Naming Conventions And Heavy Abstractions,"CSS <code>@scope</code>: An Alternative To Naming Conventions And Heavy Abstractions

Prescriptive class name conventions are no longer enough to keep CSS maintainable in a world of increasingly complex interfaces. Can the new `@scope` rule finally give developers the confidence to write CSS that can keep up with modern front ends?"
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/combobox-vs-multiselect-vs-listbox/,1770112800,Combobox vs. Multiselect vs. Listbox: How To Choose The Right One,"Combobox vs. Multiselect vs. Listbox: How To Choose The Right One

Combobox vs. Multi-Select vs. Listbox vs. Dual Listbox? How they are different, what purpose they serve, and how to choose the right one. Brought to you by <a href=""https://ai-design-patterns.com"">Design Patterns For AI Interfaces</a>, **friendly video courses on UX** and design patterns by Vitaly."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/desktop-wallpaper-calendars-february-2026/,1769850000,"Short Month, Big Ideas (February 2026 Wallpapers Edition)","Short Month, Big Ideas (February 2026 Wallpapers Edition)

Let’s make the most of the shortest month of the year with a new collection of desktop wallpapers that are sure to bring a smile to your face — and maybe spark your creativity, too. All of them were designed with love by the community for the community and can be downloaded for free. Happy February!"
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/practical-use-ai-coding-tools-responsible-developer/,1769778000,Practical Use Of AI Coding Tools For The Responsible Developer,"Practical Use Of AI Coding Tools For The Responsible Developer

AI coding tools like agents can be valuable allies in everyday development work. They help handle time-consuming grunt work, guide you through large legacy codebases, and offer low-risk ways to implement features in previously unfamiliar programming languages. Here are practical, easy-to-apply techniques to help you use these tools to improve your workflow."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/unstacking-css-stacking-contexts/,1769508000,Unstacking CSS Stacking Contexts,"Unstacking CSS Stacking Contexts

In CSS, we can create “stacking contexts” where elements are visually placed one on top of the next in a three-dimensional sense that creates the perception of depth. Stacking contexts are incredibly useful, but they’re also widely misunderstood and often mistakenly created, leading to a slew of layout issues that can be tricky to solve."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/beyond-generative-rise-agentic-ai-user-centric-design/,1769086800,Beyond Generative: The Rise Of Agentic AI And User-Centric Design,"Beyond Generative: The Rise Of Agentic AI And User-Centric Design

Developing effective agentic AI requires a new research playbook. When systems plan, decide, and act on our behalf, UX moves beyond usability testing into the realm of trust, consent, and accountability. Victor Yocco outlines the research methods needed to design agentic AI systems responsibly."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/rethinking-pixel-perfect-web-design/,1768903200,Rethinking “Pixel Perfect” Web Design,"Rethinking “Pixel Perfect” Web Design

Amit Sheen takes a hard look at the “Pixel Perfect” legacy concept, explaining why it’s failing us and redefining what “perfection” actually looks like in a multi-device, fluid world."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/smashing-animations-part-8-css-relative-colour/,1768384800,Smashing Animations Part 8: Theming Animations Using CSS Relative Colour,"Smashing Animations Part 8: Theming Animations Using CSS Relative Colour

CSS relative colour values are now widely supported. In this article, pioneering author and web designer [Andy Clarke](https://stuffandnonsense.co.uk/) shares practical techniques for using them to theme and animate SVG graphics."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/ux-product-designer-career-paths/,1768212000,UX And Product Designer’s Career Paths In 2026,"UX And Product Designer’s Career Paths In 2026

How to shape your career path for 2026, with decision trees for designers and a UX skills self-assessment matrix. The only limits for tomorrow are the doubts we have today. Brought to you by <a href=""https://smart-interface-design-patterns.com/"">Smart Interface Design Patterns</a>, a **friendly video course on UX** and design patterns by Vitaly."
rss,uxdesign.cc,https://uxdesign.cc/hidden-cost-of-ai-prototypes-leadership-myths-how-designers-use-ai-568b86eb87f2?source=rss----138adf9c44c---4,1771850022,"Hidden cost of AI prototypes, leadership myths, how designers use AI","Hidden cost of AI prototypes, leadership myths, how designers use AI

<h4>Weekly curated resources for designers — thinkers and makers.</h4><figure><a href=""https://uxdesign.cc/the-hidden-cost-of-ai-prototypes-that-are-made-to-die-00cc4d491dec""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*xDjJNqS0ZKVp-TRi.png"" /></a></figure><p>“Prototypes are no longer as special as they once were. They’re now the bare minimum.</p><p>But this speed comes with limitations. Many AI-generated prototypes are never meant to survive past the moment they’re validated. They do their job in a meeting or a user test, but then they’re rebuilt by engineering or even trashed. But the prototypes didn’t “fail,” they were just created with a different intention and outcome.”</p><p><a href=""https://uxdesign.cc/the-hidden-cost-of-ai-prototypes-that-are-made-to-die-00cc4d491dec""><strong>The hidden cost of AI prototypes that are made to die</strong></a><strong> →<br /></strong>By <a href=""https://medium.com/u/b2f44e1879c9"">Allie Paschal</a></p><h3>Editor picks</h3><ul><li><a href=""https://uxdesign.cc/escaping-the-ennui-in-ui-dbc1d63c977d?sk=31b24cf502ac85976ea7edb00a1be6cb""><strong>Escaping the ennui in UI</strong></a><strong> →</strong><br />How vibedesign created a vicious slope of AI slop.<br />By <a href=""https://medium.com/u/17dab133f2ba"">Darren Yeo</a></li><li><a href=""https://uxdesign.cc/your-research-tools-got-smarter-did-you-9fd4339617ca""><strong>Your research tools got smarter… Did you?</strong></a><strong> →</strong><br />Which side of the line you’re standing on.<br />By <a href=""https://medium.com/u/4b19995ab7ca"">Josh LaMar</a></li><li><a href=""https://uxdesign.cc/get-behind-me-ai-writer-b783ba2f9851""><strong>Get behind me, AI writer</strong></a><strong> →</strong><br />A reverse-engineered drafting process that keeps humans in charge.<br />By <a href=""https://medium.com/u/592a18ec83da"">Tanner Walsh</a></li></ul><p><em>The UX Collective is an independent design publication that elevates unheard design voices and helps designers think more critically about their work.</em></p><figure><a href=""https://www.creativeboom.com/inspiration/li-wang-captures-life-love-and-longing-in-colour/""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*W1Y1EwdqiRGM_QIe.png"" /></a></figure><p><a href=""https://www.creativeboom.com/inspiration/li-wang-captures-life-love-and-longing-in-colour/""><strong>Li Wang captures life, love, and longing in color</strong></a><strong> →</strong></p><h3>Make me think</h3><ul><li><a href=""https://jonnyburch.com/life-after-figma/?ref=sidebar""><strong>Life after Figma is coming (and it will be glorious)</strong></a><strong> →</strong><br />“As engineers speed up the bottleneck in any given product team is being felt further up the stack in the design team. In modern teams it’s no longer acceptable for a designer to spend 2 weeks in their mind palace creating the perfect UI.”</li><li><a href=""https://pjonori.blog/posts/design-systems-tomorrows-cause-for-shitty-software/?ref=sidebar""><strong>Design systems are today’s cure and tomorrow’s cause of shitty software</strong></a><strong> →</strong><br />“A new kind of claustrophobia has kicked in now. Software ships across countless platforms and device types. UI frameworks are all but considered mandatory. There are layers and layers of complexity that make 2005 feel like a kindergartner’s naive daydream. Managing the complexity of “modern” software takes considerable focus. And focus doesn’t grow on trees.”</li><li><a href=""https://ilyabirman.net/meanwhile/all/design-vs-evolution/?ref=sidebar""><strong>Design is dead, it’s all evolution now</strong></a><strong> →</strong><br />“There was a time when products were designed with intent. Sections were organized into a hierarchy, features were given logical places. You could feel a system behind the product: what parts it consists of, how screens are organized, what kinds of data it has. Users didn’t analyze it consciously, but it helped them navigate and gave them a sense of control.”</li></ul><h3>Little gems this week</h3><figure><a href=""https://uxdesign.cc/openai-ads-as-content-708666ddab2b""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*OSVZofLgd90C-i9G.png"" /></a></figure><p><a href=""https://uxdesign.cc/openai-ads-as-content-708666ddab2b""><strong>OpenAI: from ads to content</strong></a><strong> →<br /></strong>By <a href=""https://medium.com/u/a0f35cc87dd4"">Rodrigo Osornio</a></p><figure><a href=""https://uxdesign.cc/getting-carried-away-when-intelligence-is-replaced-by-compliance-f6c63585f7af""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*_yFegXZURrXnb0SR.png"" /></a></figure><p><a href=""https://uxdesign.cc/getting-carried-away-when-intelligence-is-replaced-by-compliance-f6c63585f7af""><strong>Getting carried away: When intelligence is replaced by compliance</strong></a><strong> →<br /></strong>By <a href=""https://medium.com/u/a23f14c0ef83"">Gaurav Ramesh</a></p><figure><a href=""https://uxdesign.cc/why-your-ceo-acts-like-a-clown-the-tribal-myths-of-leadership-b3c2a5f6cc17?sk=1de04a1232f05d8d7e6d95d2b562c873""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*AHKFU1FAtcUtNRL7.png"" /></a></figure><p><a href=""https://uxdesign.cc/why-your-ceo-acts-like-a-clown-the-tribal-myths-of-leadership-b3c2a5f6cc17?sk=1de04a1232f05d8d7e6d95d2b562c873""><strong>The tribal myths of leadership</strong></a> →<br />By <a href=""https://medium.com/u/161b4eee2ac1"">Maxim Kich</a></p><h3>Tools and resources</h3><ul><li><a href=""https://uxdesign.cc/the-80-job-how-design-leads-are-using-ai-and-its-not-about-mockups-ce5df0ed78cf""><strong>How design leads are using AI</strong></a><strong> →</strong><br />The 80% job.<br />By <a href=""https://medium.com/u/7e26e3cc0dfd"">Vlad Derdeicea</a></li><li><a href=""https://uxdesign.cc/practice-notes-on-including-citizens-in-the-design-process-d28bf115700f""><strong>Including citizens in the design process</strong></a><strong> →</strong><br />Field notes on trust and shared agency.<br />By <a href=""https://medium.com/u/e1ffebbf296c"">Jack Strachan</a></li><li><a href=""https://uxdesign.cc/field-study-prototypes-over-mockups-8581f20102ff""><strong>Prototypes over mockups</strong></a><strong> →</strong><br />A practical guide to designing with code in 2026.<br />By <a href=""https://medium.com/u/ef9e6832a598"">Édouard Wautier</a></li></ul><h3>Support the newsletter</h3><p>If you find our content helpful, here’s how you can support us:</p><ul><li>Check out <a href=""https://bit.ly/uxc-mob1"">this week’s sponsor</a> and support their work too</li><li>Forward this email to a friend and invite them to <a href=""https://newsletter.uxdesign.cc/"">subscribe</a></li><li><a href=""https://uxdesigncc.medium.com/sponsor-the-ux-collective-newsletter-bf141c6284f"">Sponsor an edition</a></li></ul><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=568b86eb87f2"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/hidden-cost-of-ai-prototypes-leadership-myths-how-designers-use-ai-568b86eb87f2"">Hidden cost of AI prototypes, leadership myths, how designers use AI</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/what-designers-can-learn-from-the-first-iphone-moment-of-ai-4643d1f14171?source=rss----138adf9c44c---4,1771849318,What designers can learn from the first iPhone moment of AI,"What designers can learn from the first iPhone moment of AI

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/what-designers-can-learn-from-the-first-iphone-moment-of-ai-4643d1f14171?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/1632/0*jHu1HSSgUgnIFpej"" width=""1632"" /></a></p><p class=""medium-feed-snippet"">Fifteen years ago, the iPhone killed Flash and nearly erased an entire design discipline.</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/what-designers-can-learn-from-the-first-iphone-moment-of-ai-4643d1f14171?source=rss----138adf9c44c---4"">Continue reading on UX Collective »</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/designers-we-should-be-killing-it-right-now-c0a0c535f456?source=rss----138adf9c44c---4,1771849297,"Designers, we should be killing it right now","Designers, we should be killing it right now

<h4>Designers should be thriving in the age of AI. Here’s why we aren’t, why it’s probably our fault, and how we can fix it.</h4><figure><img alt=""The Beatles on red pointy hats with sunflowers"" src=""https://cdn-images-1.medium.com/max/1024/1*wXwxelMpQ2KNjnTN1Tzm-w.png"" /><figcaption>The Beatles, Magical Mystery Tour (<a href=""https://www.imdb.com/de/title/tt0061937/mediaviewer/rm3997504000/"">source</a>)</figcaption></figure><p>Much has been said about the future of design in the age of AI. Some think the role will disappear completely. Others say only super-seniors will survive. And yet others say it’s all just a blip in time and there will be no fundamental change.</p><p>I think all are wrong.</p><p>For two reasons:</p><ol><li>Designers are naturally attuned to adapt to technological change. We are trained to identify and acknowledge change, manage adaptation, and find solutions for friction.</li><li>We’re moving towards a totally new definition of digital products. Away from interfaces, and towards fluid use cases that we’re only beginning to imagine now. To make this accessible and valuable to all, we will need designers at all levels. <em>Obviously</em>.</li></ol><p>Why, then, has the design community been in absolute panic since AI has been taking off?</p><p>Seniors are telling juniors to count themselves lucky if they’ll ever find a job.</p><p>Design leaders are jumping from one AI-tool hypetrain to the next in mere weeks.</p><p>Monday, it’s all about prototypes. Thursday, it’s vibe coding. Friday, we’re preaching that output no longer matters (everyone can design now!) and that we should be brilliant strategists instead. By next Monday, we’ll be half-heartedly debating which soft skills are absolutely vital to survive.</p><p><em>Survive.</em> As designers.</p><p>As if our skills have evaporated overnight, and we can only stick around if we somehow show that we’re good for (undefined) other stuff.</p><p>No.</p><p>We should be thriving.</p><p>Where has it all gone wrong? Why aren’t designers winning in the age of AI, which, at its core, is about <em>making</em>?</p><p>The thing we do best.</p><p>Here’s what I think.</p><h4>We’ve been sabotaging ourselves.</h4><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*7ogC1IGrkuO2HuDnFDBoKA.png"" /><figcaption>The Beatles, Magical Mystery Tour (<a href=""https://www.imdb.com/de/title/tt0061937/mediaviewer/rm1950683648/"">source</a>)</figcaption></figure><p>Please don’t be enraged. Take a deep breath and think about it.</p><p>Ever since the first AI tools with strong visual output started emerging, we seemingly overnight started screaming from all rooftops about:</p><p><strong><em>taste</em></strong>.</p><h4>What the fck is taste?</h4><p><a href=""https://www.nngroup.com/articles/taste-vs-technical-skills-ai/"">According to NNG</a> it’s “the learned, subjective ability to discern and select the most effective, harmonious, and high-quality elements for a project”.</p><p>Maybe, if you’re really smart, you can picture what that means. But really, if you’re honest, taste is a vague concept. Taste is subjective. Taste is not something that makes a product win (for the most part).</p><p>And vague is not <em>good</em> — especially not in a time of transformation where no one really knows where we’re going yet. Where tech stacks are changing weekly, and user needs are developing at rocket speed.</p><p>You know what else <em>vague</em> is bad for? Convincing an executive to invest in something.</p><p>It generally takes three things to grab an executive's attention.</p><ol><li>Connection (Do you get their problem?).</li><li>Authority (Do you know your stuff?).</li><li>Vision (Do you know where this is going?).</li></ol><p>Taste doesn’t answer any of those. You know what else doesn’t?</p><h4>Shitty prototypes.</h4><p>Yeah, I’ve made my fair share of them, too. But the time when these impressed anyone is over. Even my 0% designer husband can now vibe code stuff and knows how to whip up a design prototype.</p><p>There is a time and place for them. Prototypes and vibe-coded projects are great to show a vision, build excitement, or unblock a team that’s stuck in process. But they’re not what’s gonna convince your engineering stakeholders to give you access to their codebase so you can actually start shipping fixes from your design backlog.</p><p>I’d like to argue that excessive prototyping is the next big issue that’s messing with the status of design in the age of AI.</p><p>Because they undermine our authority. There’s the executive’s next red flag. <em>Do you know your stuff?</em></p><p>Of course, designers recognised this concern quickly. How did we respond?</p><p>By starting never-ending discussions about:</p><p><strong><em>craft</em>.</strong></p><h4>What the heck is craft?</h4><p>I like <a href=""https://www.nsead.org/resources/teaching-inspiration/craft/defining-craft/"">this definition</a>: “Craft can be defined as intelligent making. It is technically, materially and culturally informed. Craft is the designing and making of individual artefacts and objects, encouraging the development of intellectual, creative and practical skills, visual sensitivity and a working knowledge of tools, materials and systems”.</p><p>Welp.</p><p><em>Taste and craft walk into a bar…</em></p><p>Just kidding.</p><p>You get my point. Here is another vague concept that most people associate with activities they liked to do back in kindergarten.</p><p>And listen, I’m not knocking craft. I love writing poetry, painting, throwing pots at the wheel. All that takes craft and skill, just as my designs at work do. But craft should be so obvious to us as designers that we should not make it our <em>main</em> selling point.</p><p>Obviously, we develop incredible craft as our experience builds. Obviously, individual designers have different styles. Obviously, we put thought and care into what we make.</p><p>Craft is the baseline. That’s what we want the executives to know. By debating it and what it even means, we’re again undermining our authority. And I’d argue we’re also not really connecting with them OR showing much vision.</p><p>Well, shit.</p><p><strong>How do we get out of this mess?</strong></p><p>We need to remember — and remind others — why design is irreplaceable.</p><h4>We’re the OG makers.</h4><p>We’re “growth mindset”, personified.</p><p>In most classic product companies, there are <a href=""https://www.producttalk.org/glossary-discovery-product-trio/"">trios</a>: A product manager, a designer, and an engineer. The point? To ensure that the perspectives of the users and the business are balanced with the technical capabilities throughout the development process.</p><p>While a well-functioning trio feels like a cheat code to achieving product velocity, an unbalanced one can slow things down, ship mediocre stuff, or not at all.</p><p>The best trios I’ve worked in and observed had one very obvious trait in common: we all respected each other's practice, but we also understood enough about each other’s domains to be unafraid to push back.</p><p>Only a designer who understands the business side of things can really translate business goals into conversion flows.</p><p>Only a product manager who understands the fundamentals of good design can push a designer to level up.</p><p>And only an engineer who cares for users and understands how the business works is an equal partner in a trio.</p><p>And while AI is bringing all of our roles closer together, we are still the builders of <em>the experience</em>.</p><p>The thing that is actually being used. The thing executives aim to deliver.</p><ol><li><strong>Connection</strong>: You want to make something. We can tell you if it’s possible. If yes, we can ensure it’s made in the best possible way.</li><li><strong>Authority</strong>: We work at the intersection of what the business wants, the user needs, and what actually works.</li><li><strong>Vision</strong>: We have the skills, tools, and mindset to create something you <em>can’t even/can only</em> imagine.</li></ol><p>Sold.</p><h4>The design mindset is unique.</h4><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*iI-tGWT87XJnnLmo7XwFKA.png"" /><figcaption>The Beatles, Magical Mystery Tour (<a href=""https://www.imdb.com/de/title/tt0061937/mediaviewer/rm725418752/"">source</a>)</figcaption></figure><p>And it’s beautiful.</p><p><strong>It’s human-centred. </strong>Deep empathy for the user is the foundation, ensuring solutions address real needs, feelings, and motivations.</p><p><strong>It embraces ambiguity.</strong> Viewing problems as opportunities rather than roadblocks.</p><p><strong>It kills its darlings by default and favours learning.</strong> A willingness to “fail fast,” learn from prototypes, and constantly adapt solutions based on feedback.</p><p><strong>It’s, at its core, optimistic and inclusive.</strong> Bringing diverse perspectives together, believing that better solutions can always be created.</p><p><strong>It’s the most curious of mindsets.</strong> Asking “why” and “what if” to challenge the status quo at every turn.</p><p>You don’t have to read this twice to know: hell yes, we need designers with AI taking over more and more space in our lives.</p><h4>Here’s what we need to do. Now.</h4><p>In most classic design teams, there are different roles: Product designers, content designers, user researchers, sometimes motion designers, and ops people. Their educational background is essentially the same: developing an understanding of users, business, and products. Some focus more on the look and feel, others on going in depth on the “how it works”.</p><p><strong>So, for the love of everything good, can we please stop debating our titles?</strong></p><h4>We are all designers.</h4><p>Designers make stuff. Some use words and taxonomy, others use pixels. Some use language, others use code. The mindset is the same. The mindset is what matters.</p><p><strong>Yes, AI is muddying the lines between design roles. But it doesn’t matter.</strong></p><p>Not because it can replace people, but because it can save time that, in return, can be used to develop the skills traditionally only another role would need. Not so you can replace someone, but:</p><p><strong>So you can become a more holistic, better maker.</strong></p><p>This is not new.</p><p>There’s always been a push for all designers to do their own research. For content designers to develop visual skills. For product designers to get strong at prototyping. Now, finally, we can do so — with far less effort than before.</p><p>That doesn’t mean we should think less. It means we <em>can</em> think more.</p><p>I, personally, love thinking. Nothing gets me as high as solving a problem. It’s the reason I got into design, and specifically design in tech, in the first place. There were actual (cool) problems to solve.</p><p>I originally picked UX content as my toolkit. Now, my toolkit has expanded. I love that. And I don’t want to waste time wondering what my title should be, whether my taste is good enough, or what <em>exactly</em> my craft is now.</p><p>I want to make stuff.</p><p>And I think you do, too.</p><p>Nicole is a Content Designer turned Design Director based in Stockholm, Sweden. She potters, writes poetry, and raises little girls in a house by a meadow. You can follow her writing here or get it directly to your inbox via her publication, <a href=""https://eggwoman.substack.com/"">eggwoman</a>. Nicole is on <a href=""https://www.linkedin.com/in/nicoletells/"">Linkedin</a>.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c0a0c535f456"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/designers-we-should-be-killing-it-right-now-c0a0c535f456"">Designers, we should be killing it right now</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/something-big-might-be-happening-16cdba2df6ad?source=rss----138adf9c44c---4,1771849175,Something big “might” be happening,"Something big “might” be happening

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/something-big-might-be-happening-16cdba2df6ad?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/1200/0*8j9GZ3604R5ROnc8"" width=""1200"" /></a></p><p class=""medium-feed-snippet"">Why the future of design belongs to creative thinkers and problem solvers</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/something-big-might-be-happening-16cdba2df6ad?source=rss----138adf9c44c---4"">Continue reading on UX Collective »</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/no-vr-cant-make-you-walk-in-others-shoes-608a1ba03e97?source=rss----138adf9c44c---4,1771849139,"No, VR can’t make you walk in others’ shoes","No, VR can’t make you walk in others’ shoes

<h4>The shallowness of the “empathy machine.”</h4><figure><img alt=""a guy using VR headset in a fancy living room"" src=""https://cdn-images-1.medium.com/max/1024/0*5waTlYqbHeJwFdxz"" /><figcaption>Photo by <a href=""https://unsplash.com/@silverkblack?utm_source=medium&amp;utm_medium=referral"">Vitaly Gariev</a> on <a href=""https://unsplash.com?utm_source=medium&amp;utm_medium=referral"">Unsplash</a></figcaption></figure><p>I once saw a “poverty simulation” designed to help raise funds and “put people in others’ shoes.” I felt… weird. Can you imagine someone going through a 10-minute fancy VR experience and suddenly claiming they understand the struggles?</p><p>VR allows users to step into any experience from a first-person perspective. Some people call it an “<a href=""https://www.youtube.com/watch?v=iXHil1TPxvA\"">empathy machine</a>” and argue that it could influence decision-makers. I bought into it at first. However, after going through several simulations, I noticed that no matter how moved I was in the moment, none of it led to any real behavioral change.</p><p>Is it just me? Or is this “trigger empathy and cause behavioral change” design goal too impractical and overhyped?</p><h3>Why do people believe that VR can trigger empathy?</h3><p>VR allows users to inhabit avatars, and their brains might start believing that the virtual body is theirs (<a href=""https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2022.814565/full"">researchers</a> call this “sense of embodiment”). Some designers believe this can <strong>encourage perspective-taking, reduce implicit bias, and eventually induce empathy.</strong></p><p>In theory, it makes sense. But in reality, this assumption is too ideal and shallow. First, your brain isn’t that easy to trick. A tiny glitch — a slight delay between movement and render, or weird skin texture — will remind you it’s fake. More importantly, embodiment alone is just a shallow representation of identity. For example, putting users in a Black body doesn’t give them hundreds of years of history or lived experience. It doesn’t make them part of the Black community, and it certainly won’t create sustainable empathy.</p><h3>What is empathy, exactly?</h3><p>This word seems a bit abstract, so let’s define it better here. When <a href=""https://tmb.apaopen.org/pub/vr-improves-emotional-empathy-only/release/2"">researchers </a>and designers discuss empathy, they’re usually talking about two different kinds: <strong>emotional </strong>and <strong>cognitive</strong>.</p><p>Emotional empathy is immediate and direct, like seeing someone in pain makes you feel discomfort too. Cognitive empathy goes deeper. It’s when you actively try to understand someone’s mental state and decision-making process, and this kind of empathy is what usually leads to behavioral change.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*AiXVB-J8JEHS2BtKZ3n9VA.png"" /></figure><p>VR is pretty good at triggering emotional empathy because it provides users with intense sensory information. But the reaction doesn’t last. For me, it usually starts fading about 10 minutes after removing the headset. I think the problem is that while I’m in the experience, I’m so overwhelmed by everything I’m sensing that I can’t actually think about what the character is going through mentally.<strong> Without that deeper engagement, cognitive empathy never gets triggered.</strong></p><h3><strong>VR’s shallow content</strong></h3><p>Beyond sensory overload, VR simulations lack the depth needed for cognitive empathy.</p><p>The content is often shallow. Let’s compare it with other storytelling media: movies and books are usually built on years of research and interviews, while VR simulations prioritize technical execution, focusing on realistic graphics and interactions over storytelling depth. <strong>Users often end up distracted by the novelty of technology instead of engaging with the issues.</strong></p><p>Time is another constraint. Most VR simulations run under 10 minutes, which isn’t enough to convey background or context. Understanding social issues often requires hours of learning and engagement just to grasp the basics. However, creating longer experiences isn’t practical either because VR headsets are bulky and uncomfortable, and the sensory intensity creates fatigue quickly.</p><h3><strong>When VR simulation might work</strong></h3><p>VR probably isn’t the best tool for creating sustainable empathy or long-term behavioral change. However, it can still be effective when applied with clear, specific design goals instead of vague “triggering empathy” objectives.</p><p>Take healthcare training for example. <a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC6148621/"">Research </a>shows that medical students better understand conditions like Alzheimer’s through VR, improving their compassionate care. It works because <strong>VR complements their existing education.</strong> These students already study these topics extensively through patient shadowing, workshops, and reflective practice.</p><p>Unlike general audiences with no background, these students approach VR with clear learning objectives. They know they’re not trying to “feel like” elderly patients but learning to recognize specific physical symptoms. They’re less likely to get distracted by the technology and can navigate the experience better. The actual empathy develops later, when they interact with real patients.</p><p>No one can really “walk in others’ shoes.” Empathy is a complex emotion. Social issues are far more complicated than any simulation can capture, and technology only scratches the surface: loud explosions simulate sound, not fear. Disorientation replicates physical sensations, not the psychological toll of experiencing it every day.</p><p>New technology isn’t always the shortcut to the solution. Real perspective-taking requires a more time-consuming traditional approach: engaging with communities, reading, and listening. As a developer, I try to remember these limitations instead of chasing technology hype with vague, overambitious goals.</p><h3>Reference:</h3><ol><li>Milk, C. [TED]. (2015). How virtual reality can create the ultimate empathy machine [Video]. YouTube. <a href=""https://www.youtube.com/watch?v=iXHil1TPxvA"">https://www.youtube.com/watch?v=iXHil1TPxvA</a></li><li>Martingano, A. J., Hererra, F., &amp; Konrath, S. (2021). Virtual Reality Improves Emotional but Not Cognitive Empathy: A Meta-Analysis. <em>Technology, Mind, and Behavior</em>, <em>2</em>(1). <a href=""https://doi.org/10.1037/tmb0000034"">https://doi.org/10.1037/tmb0000034</a></li><li>Dyer, E., Swartzlander, B. J., &amp; Gugliucci, M. R. (2018). Using virtual reality in medical education to teach empathy. <em>Journal of the Medical Library Association : JMLA</em>, <em>106</em>(4), 498–500. <a href=""https://doi.org/10.5195/jmla.2018.518"">https://doi.org/10.5195/jmla.2018.518</a></li><li>Sora-Domenjó C (2022) Disrupting the “empathy machine”: The power and perils of virtual reality in addressing social issues. Front. Psychol. 13:814565. doi: 10.3389/fpsyg.2022.814565</li></ol><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=608a1ba03e97"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/no-vr-cant-make-you-walk-in-others-shoes-608a1ba03e97"">No, VR can’t make you walk in others’ shoes</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/on-craft-and-connivence-6fa58e2c865e?source=rss----138adf9c44c---4,1771849127,On craft and connivence,"On craft and connivence

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/on-craft-and-connivence-6fa58e2c865e?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/2600/1*-wfGuiGXJriR43ufzSNCTQ.png"" width=""26245"" /></a></p><p class=""medium-feed-snippet"">Lessons in design, from a USB drive to the age of AI, and beyond.</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/on-craft-and-connivence-6fa58e2c865e?source=rss----138adf9c44c---4"">Continue reading on UX Collective »</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/surveillance-by-default-consent-by-assumption-928b2919c960?source=rss----138adf9c44c---4,1771849019,"Surveillance by default, consent by assumption","Surveillance by default, consent by assumption

<h4>How consumer security products turn physical presence into assumed consent</h4><figure><img alt=""Illustration of a Ring-style doorbell camera watching people in a shared neighbourhood space."" src=""https://cdn-images-1.medium.com/max/1024/1*BJZf6ZZrNZDCKe6Iz76Jrw.jpeg"" /><figcaption>Illustration of a home security camera extending its gaze into shared neighbourhood space, symbolising the shift from private protection to ambient surveillance. <strong><em>Image generated using Grok Imagine.</em></strong></figcaption></figure><p>When <a href=""https://ring.com/eu/en"">Amazon’s Ring</a> aired its <a href=""https://www.youtube.com/watch?v=OheUzrXsKrY""><em>Search Party</em> advertisement</a> during the Super Bowl, it presented a reassuring narrative: neighbours’ cameras cooperate, a missing dog is found, and communal order is restored. The unease the advert provoked did not stem from its goal. It arose from its premise, namely that individuals can be enrolled into a neighbourhood-scale, real-time AI <a href=""https://www.theverge.com/tech/876866/ring-search-party-super-bowl-ad-online-backlash"">surveillance network</a> without ever explicitly consenting to participate.</p><p>Within days, Ring terminated its planned partnership with <a href=""https://www.flocksafety.com/"">Flock Safety</a>, a company criticised for operating large-scale <a href=""https://www.npr.org/2026/02/17/nx-s1-5612825/flock-contracts-canceled-immigration-survillance-concerns"">automated licence plate recognition systems</a> used by <a href=""https://www.eff.org/deeplinks/2025/12/effs-investigations-expose-flock-safetys-surveillance-abuses-2025-review"">US law enforcement</a>. Ring cited <a href=""https://www.bbc.com/news/articles/cwy8dxz1g7zo"">implementation complexity</a>. The timing suggests a more candid explanation. The partnership made visible a widening gap between what surveillance technologies enable and what the public is prepared to tolerate.</p><p>This episode illustrates a defining feature of contemporary AI deployment. Individuals are no longer incorporated primarily as deliberate users; they are absorbed as ambient data contributors. Consent is not requested. It is inferred from physical presence. And when the implications of that inference become politically salient, withdrawal tends to be tactical rather than principled.</p><h3>When presence becomes participation</h3><p>Ring’s <em>Search Party</em> feature queries nearby cameras when a missing pet is reported. As <a href=""https://www.markey.senate.gov/news/press-releases/senator-markey-demands-amazon-abandon-plan-to-include-facial-recognition-technology-in-ring-doorbells"">Senator Ed Markey observed</a>, this closely resembles neighbourhood-scale surveillance infrastructure. Crucially, <em>Search Party</em> does not operate in isolation. Ring’s <a href=""https://ring.com/support/articles/z3yhg/familiar-faces""><em>Familiar Faces</em></a> feature applies facial recognition to anyone passing within camera range, continuously scanning and categorising faces without their explicit knowledge or agreement.</p><figure><img alt=""Smartphone screen showing Ring’s Search Party interface asking a user to verify whether their camera footage matches a reported missing dog."" src=""https://cdn-images-1.medium.com/max/1024/0*RkS0GTIqXOE7qOGO.jpg"" /><figcaption>Ring’s Search Party interface prompts users to verify footage from their cameras, illustrating how neighbourhood-scale surveillance continues through everyday product features. <strong>Image source: </strong><a href=""https://www.aboutamazon.com/news/devices/ring-search-party-for-dogs-united-states-missing-pets""><strong>Amazon</strong></a><strong>.</strong></figcaption></figure><p>Combined, these capabilities enable tracking across residential space through distributed, privately owned sensors. The system no longer merely secures private property. It facilitates continuous monitoring of public movement.</p><p>The central tension lies in default inclusion. Neighbours are not asked whether their footage should be analysed for AI searches. Passers-by are not consulted before their biometric features are processed. Cameras function as environmental infrastructure, and physical presence alone becomes sufficient grounds for enrolment.</p><p><strong>This is not informed consent. It is participation assumed through exposure.</strong></p><p>Yuval Noah Harari captures this historical inversion precisely in <a href=""https://www.ynharari.com/book/nexus/""><em>Nexus</em></a>:</p><p><em>“In a world where humans monitored humans, privacy was the default.”</em></p><p>In sensor-saturated environments, <strong>the opposite is now true</strong>.</p><figure><img alt=""Cover images of Yuval Noah Harari’s book Nexus, titled “A Brief History of Information Networks from the Stone Age to AI”."" src=""https://cdn-images-1.medium.com/max/1024/0*iaX961XQIJtbLeHh.jpg"" /><figcaption>Cover of <em>Nexus</em> by Yuval Noah Harari, which examines the historical shift from human-to-human monitoring to large-scale information networks. <strong>Image source:</strong> <a href=""https://www.ynharari.com/book/nexus/"">Yuval Noah Harari, official website.</a></figcaption></figure><h3>The normalisation of suspicion</h3><p>The problem is not only technical but social. Platforms such as Ring’s <a href=""https://apps.apple.com/us/app/neighbors-by-ring/id1218902777""><em>Neighbours</em> app</a> have reformatted the neighbourhood watch into what critics describe as participatory mass surveillance. Real-time alerts about “suspicious activity” gamify vigilance, rewarding users who flag anomalies in their surroundings.</p><p>The consequences are not evenly distributed. Research consistently shows that people of colour are disproportionately labelled as suspicious on these platforms. What begins as a private perception becomes a data point: timestamped, geotagged, and <em>potentially</em> routed toward <a href=""https://www.cnbc.com/2025/10/16/amazon-ring-cameras-surveillance-law-enforcement-crime-police-investigations.html"">police response</a>. The technology does not create implicit bias, but it institutionalises it, converting individual prejudice into operational logic.</p><p>When private cameras become the mechanism by which those biases trigger formal intervention, the question of <strong>who gets watched, and who gets to watch</strong>, becomes a civil rights issue rather than a consumer preference.</p><h3>The Infrastructure of Inevitable Surveillance</h3><p>The abandoned Flock Safety partnership reveals how such systems scale institutionally. The proposed integration could have allowed police departments to <a href=""https://www.cnet.com/home/security/amazons-ring-cameras-push-deeper-into-police-and-government-surveillance/"">request Ring footage</a> through Flock’s platform, following a <a href=""https://doi.org/10.1080/01972243.2015.1107166"">familiar trajectory</a> in which consumer products evolve into mechanisms for data aggregation and institutional access.</p><p>This pipeline matters beyond the cancelled partnership. Where law enforcement lacks the legal grounds for neighbourhood-wide warrants, access to aggregated community camera data can serve a functionally equivalent role. Even when platforms restrict direct police access, data frequently travels through intermediaries whose business models involve selling location and behavioural data to government agencies operating well outside local democratic oversight.</p><p><strong>Cancelling one visible integration does not seal the pipeline. It merely reroutes it.</strong></p><p>Ring’s transformation from consumer hardware to data-centric platform makes this structural rather than accidental. When one person installs a camera, they consent on their own behalf. That camera nonetheless captures neighbours, delivery workers, and passers-by. As camera density increases, opting out becomes practically impossible, since avoidance requires withdrawal from shared space altogether. At that point, consent functions largely as theatre.</p><p>Harari notes that in 2023 more than <strong>one billion CCTV cameras</strong> were operative globally, roughly <strong>one for every eight people</strong>. Surveillance is no longer exceptional. <a href=""https://wifitalents.com/cctv-surveillance-industry-statistics/"">It is infrastructural.</a></p><h3>The security paradox</h3><p>There is also a pragmatic argument against centralised surveillance systems that receives less attention than it deserves: they are extraordinarily attractive targets for attack.</p><p>Aggregating millions of home cameras into a unified platform creates not only a privacy risk but a security one. A successful breach of a centralised AI hub would grant attackers what security researchers call <a href=""https://www.npr.org/sections/alltechconsidered/2014/12/10/369740829/forget-creepy-nunbergs-word-of-the-year-is-bigger-and-two-god-view""><em>god-view</em> access</a>: real-time visibility into private residences across entire cities. Footage intended to protect households becomes the means by which those households are exposed.</p><p><a href=""https://www.theguardian.com/technology/2020/dec/23/amazon-ring-camera-hack-lawsuit-threats"">Ring itself experienced security breaches in its early years</a>, including incidents in which attackers accessed cameras and harassed residents. Those were isolated failures. The risk posed by fully centralised systems, in which aggregation, AI analysis, and institutional access converge, is categorically larger.</p><p><a href=""https://kijero.com/blog/post/cctv-cameras-with-cloud-storage-vs-local-storage#:~:text=Privacy%20is%20one%20of%20the,data%20as%20you%20see%20fit."">Security researchers have long argued</a> that local storage, where footage remains within the home rather than uploaded to corporate servers, is the only architecture that eliminates this class of risk entirely. The dominance of cloud-based models reflects economic incentives tied to data extraction, not technical necessity.</p><h3>Beyond naive data optimism</h3><p>Harari describes the <em>naive view of information</em> as the belief that <a href=""https://medium.com/credtent-on-content/the-naive-view-of-engagement-why-more-isnt-better-c4b3227c7b6d"">more data naturally yields better outcomes</a>, while ignoring how power and incentives shape its use. Ring’s AI features exemplify this assumption. Expanded footage access and continuous biometric analysis are framed as unambiguous improvements to safety, while questions of agency, governance, and asymmetrical control receive minimal attention.</p><figure><img alt=""Diagram showing information leading to truth, which then branches toward either wisdom or power."" src=""https://cdn-images-1.medium.com/max/1024/1*OdHvYopGD6Fr5bADW_sRwg.png"" /><figcaption><strong>Diagram from <em>Nexus</em> by Yuval Noah Harari</strong> illustrating the naive assumption that more information naturally leads to truth and better outcomes, while in practice diverging toward either wisdom or power.</figcaption></figure><p>The proposed Flock integration made this logic explicit. Convenience justified infrastructure. Infrastructure produced data. Data became the substrate for AI analysis and policing. The progression reflects economic alignment rather than malicious intent, but the outcome is the same regardless.</p><h3>Making consent meaningful again</h3><p>The challenge is not whether AI should exist in sensor-rich environments. It is how legitimacy is established in spaces that are collectively experienced.</p><p>Three governance approaches already exist and could be implemented without delay.</p><p>First, <strong>default opt-in rather than opt-out</strong>. AI surveillance features should require active, informed activation. <a href=""https://medium.com/@www.manuelmiles/the-psychology-of-defaults-79d492bc99cc"">Behavioural research shows that most users never alter default settings.</a> Reversing this would not restrict the technology; it would make participation genuinely voluntary.</p><p>Second, <strong>community-level governance</strong>. Municipal oversight bodies can determine which capabilities are permissible within neighbourhoods, under what conditions, and with what limits on data retention. Legally constituted community trusts could require approval and independent auditing for third-party and law-enforcement partnerships.</p><p>Third, <strong>privacy-preserving technical architectures</strong>. Federated learning allows AI models to operate locally on devices without centralising raw data. A neighbourhood system built this way could detect unusual activity without footage ever leaving individual homes. These methods are already deployed in healthcare and finance. Their absence from consumer surveillance products reflects incentive structures, not technical limits.</p><h3>From retreat to redesign</h3><p>Ring’s retreat from the Flock Safety partnership does not resolve the underlying problem. The cameras remain installed. The AI capabilities persist. Partnerships can be quietly reconfigured through different intermediaries. The infrastructure endures.</p><p><a href=""https://arcisai.io/blog/ai-cctv-cameras-for-smart-city"">This pattern extends far beyond Ring</a>. Smart-city CCTV, mobile location tracking, and digital health monitoring follow the same trajectory. Presence becomes participation. Consent becomes assumed. Public concern produces episodic retreats, not structural change. The systems remain intact.</p><p>The question, then, is not whether AI inevitably erodes human agency. It is whether we continue to permit systems to treat mere presence as default input rather than as a condition requiring collective authorisation. In sensor-saturated environments, individual consent loses its protective force. When biometric processing is continuous, public anonymity erodes invisibly. When surveillance infrastructure is centralised, a single breach can undo every promise of safety on which the system was sold.</p><p>Crucially, these systems do not meaningfully self-correct through public discomfort alone. Left unconstrained, data-driven infrastructures expand until legitimacy breaks, at which point withdrawal becomes reputational rather than corrective. Ring’s retreat is best understood not as a resolution, but as a pressure release.</p><p>If AI is to be deployed in the name of safety, it must be governed with mechanisms equal to its reach: transparency that is operational rather than rhetorical, collective decision-making rather than individualised settings, and enforceable limits that bind platforms as tightly as they bind users. What is required are structures that prevent legitimacy crises from accumulating in the first place, rather than relying on backlash to correct them after the fact.</p><p>In environments where visibility has become the default, the task is not to abandon AI. <strong>It is to redesign it so that consent becomes meaningful again: deliberate rather than incidental, collective rather than individual, enforceable rather than assumed.</strong></p><p><a href=""https://www.linkedin.com/in/aflucas26/""><strong>Andrea Filiberto Lucas</strong></a> is a <a href=""https://www.um.edu.mt/profile/andreaflucas"">Research Support Officer</a> and MSc student in Artificial Intelligence at the University of Malta. He graduated <em>summa cum laude</em> with a BSc (Hons.) in IT (AI) and has published an <a href=""https://arxiv.org/abs/2602.09154"">IEEE peer-reviewed paper</a>, with interests in Computer Vision and Applied AI.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=928b2919c960"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/surveillance-by-default-consent-by-assumption-928b2919c960"">Surveillance by default, consent by assumption</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/ux-questionnaires-is-it-rocket-science-9473ad7ff386?source=rss----138adf9c44c---4,1771848978,UX questionnaires. Is it rocket science?,"UX questionnaires. Is it rocket science?

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/ux-questionnaires-is-it-rocket-science-9473ad7ff386?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/1278/0*sXsbUTRKTmjNlF4p.jpg"" width=""1278"" /></a></p><p class=""medium-feed-snippet"">Mission control didn&#x2019;t run on intuition. Neither should your design process.</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/ux-questionnaires-is-it-rocket-science-9473ad7ff386?source=rss----138adf9c44c---4"">Continue reading on UX Collective »</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/why-most-ai-products-fail-before-the-first-user-interaction-133e4588fbff?source=rss----138adf9c44c---4,1771714329,Why most AI products fail before the first user interaction,"Why most AI products fail before the first user interaction

<h4>Most AI features fail because they start with hype, not humans.</h4><figure><img alt=""Blurred office scene with a humanoid robot seated across from a man at a desk. Overlaid headline reads, “Why Most AI Products Fail Before the First User Interaction,” with a visible “Save This” button."" src=""https://cdn-images-1.medium.com/max/1024/1*JUhjp2-rcUq3oOaZl9Vt4g.png"" /><figcaption>Image Credit: AI Generated Image</figcaption></figure><p>Most AI products fail before the first user interaction because they don’t solve a real user problem. That may sound dramatic, but I keep hearing the same sentence in executive rooms: “We need an AI feature. Our competitor just launched one.” And just like that, features are built out of fear of being left behind rather than from a clear understanding of what users actually need.</p><p>And this isn’t just my opinion. Recently, I came across a report titled <a href=""https://www.rand.org/pubs/research_reports/RRA2680-1.html"">The Root Causes of Failure for Artificial Intelligence Projects and How They Can Succeed</a> by RAND.</p><p>The researchers interviewed 65 experienced data scientists and engineers who have spent years building AI and machine learning systems. Their goal was simple: understand why so many AI initiatives fail and what differentiates the few that succeed.</p><p>One of their core conclusions is striking. The most successful AI projects focus relentlessly on the problem they are meant to solve, not on the technology used to solve it. That sentence could have come straight out of an UX strategy deck.</p><p>Designers have been trained for decades to understand users and their problems before building solutions. What if AI projects are struggling not because of the models, but because design thinking was missing from the room?</p><h3>Building AI products for the mere sake of tech</h3><p>I see many companies building and releasing AI products because they feel they have to (often because their competitor just launched a tool with “AI” in the name), rather than because they’ve identified a real user need.</p><p>This appears to be a major trend, so it doesn’t surprise me to read what, for some, may be shocking statistics: Gartner predicts that <a href=""https://www.gartner.com/en/newsroom/press-releases/2025-06-25-gartner-predicts-over-40-percent-of-agentic-ai-projects-will-be-canceled-by-end-of-2027""><strong>more than 40% of agentic AI projects</strong></a> will be canceled by the end of 2027. The reasons for failure cited are escalating costs, unclear business value, and inadequate risk controls. Their own analyst called it out directly: most of these projects are early-stage experiments driven by hype and often misapplied.</p><p>Gartner also estimates that only about 130 of the thousands of agentic AI vendors out there offer genuine capabilities. The rest are engaging in what they call “agent washing,” just rebranding chatbots and RPA tools as something they’re not.</p><p>When everything is AI-powered, nothing is AI-powered. The label becomes meaningless. Companies end up with a product that technically “has AI” but solves no real problem for anyone.</p><figure><img alt=""Blurred office background with bold text stating, “40% of agentic AI products will be canceled by the end of 2027.” Source noted as Gartner 2025."" src=""https://cdn-images-1.medium.com/max/1024/1*oN8afkLbaGy1LtQcg6dMjw.png"" /></figure><h3>The “not having the right people” problem</h3><p>The data scientists and engineers who were interviewed for the RAND report (cited in the article’s introduction) say it’s challenging to find the right talent for AI projects.</p><p>Hiring skilled specialists is an issue, but also how organizations value the talent they already have. Data engineers, the people doing the hard work of cleaning and structuring data so models can learn from it, are treated like second-class citizens. One interviewee literally called them “the plumbers of data science.”</p><p>So they leave. And when they leave, they take all their institutional knowledge with them. No one knows which datasets are reliable anymore; projects stall, and leadership loses interest.</p><p>What about designers? Most AI teams don’t even have one. Or they bring one in at the tail end, when the model is already built, and someone realizes the interface is unusable. <a href=""https://www.linkedin.com/pulse/designers-vs-developers-wrong-fight-age-ai-arin-bhowmick-ja5af""><strong>Designers and developers depend on each other more than ever</strong></a> and should be at the table from day one when building any product, especially those with AI features.</p><p>To support my views and opinions on the topic, I cited a <a href=""https://www.library.hbs.edu/working-knowledge/when-ai-joins-the-team-better-ideas-surface""><strong>2025 field experiment at Harvard Business School</strong></a> that found that AI-augmented cross-functional teams were three times more likely to generate high-performing ideas than individuals working alone. The designer-developer divide is precisely the kind of silo that the experiment suggests we should break down. And so is the data engineer vs. data scientist one. Different roles, same problem: the people getting sidelined are often the ones who could have “saved” the project, or, at least, brought the most clarity to it.</p><h3>What we can do as design leaders</h3><p>The RAND report focuses on AI/ML model development rather than product design. But the failures they describe, miscommunication, unclear goals, and users as an afterthought, are the same ones killing AI products today. There’s a golden opportunity for <a href=""https://medium.com/user-experience-design-1/what-design-leaders-must-unlearn-to-lead-in-an-ai-first-world-f131652f828d"">design leaders to step into that gap.</a></p><p>Here’s how should move forward:</p><h4>Be THE communicators</h4><p>Industry stakeholders often misunderstand or miscommunicate what problem needs to be solved using AI. Design leaders have a unique opportunity to be the missing link that bridges communication between key stakeholders and software engineers, data scientists, and other AI technologists. We’ve been closely working with them for years building products, so we are the ones who know better how to communicate the right specs. We speak the same language.</p><h4>Follow the one-year rule</h4><p>RAND’s report says that before they begin any AI project, leaders should be prepared to commit each product team to solving a specific problem for at least a year. If an AI project is not worthy of such a long-term commitment, it most likely is not worth committing to at all, especially because an AI project with an overly accelerated timeline is likely to fail without ever achieving its intended goal. Designers are essential for creating products people want to use, so this rule extends to our teams. We need that time for research, testing, iterating, and validating. You can’t design a meaningful AI experience in a two-week sprint.</p><h4>Put the problem statement on the wall, not the tech stack</h4><p>The AI products that succeed are those focused on the problem they are meant to solve. As design leaders, we should be the ones writing the problem statement that guides the entire project. What I mean by this is a clear, human-centered problem statement that everyone, from the CTO to the junior data engineer, can rally around.</p><h4>Own the workflow mapping</h4><p>One of the most common failure patterns mentioned in the RAND report is building an AI model that doesn’t fit into the business workflow. It works in isolation but breaks when it meets reality. That’s design territory, because we are trained to map user journeys, service blueprints, and task flows. We see where an AI feature fits into someone’s day, and where it doesn’t. If we’re mapping these workflows before the engineering team starts building, we save the entire project from that painful moment when a technically sound model turns out to be operationally useless.</p><h4>Ask, “Do we need AI for this?”</h4><p>Not every problem requires AI. As design leaders, we should have the confidence to ask the uncomfortable questions. Sometimes the best design decision is the simplest one, and saying “this doesn’t need AI, it needs a better interface” might be the most valuable contribution you make to the entire project.</p><figure><img alt=""A group of designers and developers gathered around a whiteboard labeled “Problem Statement.” One man presents while pointing to sticky notes and diagrams, as others sit at computer workstations listening and collaborating."" src=""https://cdn-images-1.medium.com/max/1024/1*Kxik8bvZ0dJw4JFZuGmFYw.png"" /><figcaption>Image Credit: AI Generated Image</figcaption></figure><h3>The missing piece to AI product success</h3><p>Designers and engineers focused on solving real user problems. That is how we build AI products that genuinely improve people’s lives.</p><p>The gap between what companies say they want AI to achieve and what users actually need is a design leadership issue. To achieve real success, we must reframe the conversation from shiny new features to solving real user problems.</p><p><a href=""https://www.linkedin.com/in/arinb""><em>Arin Bhowmick</em></a><em> (</em><a href=""https://twitter.com/arinbhowmick""><em>@arinbhowmick</em></a><em>) is Chief Design Officer at SAP, based in San Francisco, California. The above article is personal and does not necessarily represent SAP’s positions, strategies or opinions.</em></p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=133e4588fbff"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/why-most-ai-products-fail-before-the-first-user-interaction-133e4588fbff"">Why most AI products fail before the first user interaction</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/the-craft-of-the-instruction-f0da34c445cb?source=rss----138adf9c44c---4,1771681999,The craft of the instruction,"The craft of the instruction

<h4><strong>Writing AI prompts isn’t just a new technical skill — it’s how we can make our own thinking visible</strong></h4><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*bRZ9l9RUCOKKTpZD.png"" /><figcaption><em>In this collage: Baby photo in front of Frank Lloyd Wright’s Fallingwater in Mill Run, Pennsylvania; “Towers on String — Variant Dispersed” by Haegue Yang, 2013; example of a writing instruction with corresponding output.</em> <em>Image credit: Personal photograph (Fallingwater, 2005); Haegue Yang, “Towers on String — Variant Dispersed,” 2013, Henry Art Museum; original writing instruction document by author.</em></figcaption></figure><p>I have a confession. The article I wrote last month — was written with Claude. This one too. Not by Claude. With Claude. And the difference between those two words is what I want to unpack here.</p><p>Both pieces were written using instructions. Instructions I created. Instructions I am continuing to hone — instructions that required me to study my own old essays, identifying what I do when I write. The sentence rhythms. The way I move between timescales. The zooming in and out from concept to detail. The instructions tell Claude how I would like ideas composed. I pull together concepts and experiences from my lived expertise to formulate a point of view — in this case, on this new AI technology we are all metabolizing into our lives and work. For this piece, we’ve gone back-and-forth eleven times to get to this published draft. I rework most sentences but not all.</p><p>This is a powerful and highly personalizable tool. Not the output. The instruction.</p><p>/</p><p>Christopher Alexander published <em>A Pattern Language</em> in 1977. Two hundred and fifty-three patterns — from the scale of entire regions down to the placement of a doorknob — each one describing a recurring problem in the built environment and offering the core of a solution. Pattern 159: Light on Two Sides of Every Room. Pattern 88: Street Cafe. Pattern 252: Pools of Light. The patterns are specific, tested, structural. They give you a system.</p><p>But Alexander was never interested in just the system. He was interested in what he called “the quality without a name” — something alive, whole, experienced — that emerges when patterns are followed well but can never be reduced to logic alone. He tried word after word to capture it. None were sufficient. The quality resists naming. It can only be felt.</p><p>I studied such phenomena at the University of Pennsylvania. Architecture history and theory. I spent countless hours thinking about the relationship between ideas on form and proportion, the structures created from those ideas, and the life that happens inside of them. My final thesis examined how concepts of nature in urban theory shape the way we read and design cities. Three canonical texts on urbanism, three entirely different readings of the same city, because each began with a different understanding of what was “natural.” The foundational mental models we carry into a problem structure everything that follows.</p><p>Attunement to that tension has stayed with me. The pattern is the structure. The pattern came from distilling ideas and experiences. But what makes a place — what makes any made thing — feel alive is something the pattern enables but does not contain.</p><p>What I am uncovering — and what has been genuinely surprising — twenty years later and in an entirely different medium, is that the same tension lives inside the instructions we write for AI.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*ut-R4ivcHKM9dNChdr_b1Q.png"" /><figcaption><em>In this collage: Instructions from “A Pattern Language” — Pattern 128, Indoor Sunlight; photograph of Fallingwater; covers of Collage City (Rowe &amp; Koetter), Architecture of the City (Rossi), S,M,L,XL (Koolhaas).</em> <em>Image credit: Christopher Alexander, A Pattern Language, Oxford University Press, 1977 (pattern excerpt); personal photograph (Fallingwater, 2005); book covers from Collage City, The Architecture of the City, and S,M,L,XL used for educational reference.</em></figcaption></figure><p>/</p><p>I used to write a lot. I have stacks of essays no one will ever read. But they sharpened something — the way I pull ideas together, the way I form very long sentences, the way I know in my intuition, not my intellect, when something lands.</p><p>Then life happened. Architecture for eight-ish years, a pivot into product design for the past ten-ish, two kids. Most days I vacillate between corporate theater, recreating K-pop demon hunter hairstyles, long conversations with engineers to grok the technical enough to translate it into the experiential, and putting nutritious meals on the table that go largely underappreciated. Between all this life, I still had ideas I wanted to better formulate with words. I craved that time at a carrel stacked with texts in a library — putting concepts together into an essay, hard-coding what I was feeling into a cohesive perspective. But the hours for that type of work had disappeared. The practice of writing had largely gone quiet.</p><p>So I ran an experiment. I took writing I loved — my own writing, from when I had the time to be careful with it — and I fed it to Claude. Not to generate new text. To analyze. This was my exact prompt:</p><blockquote>“I want you to look at these articles I wrote and analyze the writing mechanics I used in them to create instructions for this project space. I will write several articles in this project and I want them to sound like me and leverage some of the things I did in these articles and apply them to articles in the future.”</blockquote><p>What Claude identified was a penchant for varying sentence lengths. Shocking, right? Moving back and forth in time — from cave paintings at Lascaux, 40,000 BC, to Haegue Yang, a South Korean sculptor working in installation today. The “/” section breaks I use instead of headers. The way I embed lists in prose rather than bullets. The em dashes that let me think sideways mid-sentence — which I get called out for at work as a negative, but get to trademark as an aesthetic signature here.</p><p>A pattern language for <em>my</em> writing.</p><p>I don’t use that phrase flippantly. Because what I found myself holding was exactly what Alexander described: a collection of recurring solutions to recurring problems of expression, each one specific enough to be actionable but open enough to generate infinite variation. Not a template. Not a formula. A language — in the Alexandrian sense — that I can use to build something that feels like mine even when I am not building it alone.</p><p>This move — from intuitive practice to explicit instruction — is something designers in the field are increasingly grappling with. In a thoughtful<a href=""https://uxdesign.cc/a-ux-designer-guide-to-prompt-f045cd839489""> UX Collective piece on prompting as design craft</a>, Paz Perez frames prompting not as a trick for getting AI to comply, but as the new medium through which designers articulate intent: “Effective communication, whether with people or machines, depends on context.” She’s describing something deeper than prompt hygiene. She’s describing the need to externalize what we usually carry internally — the instinct, the judgment, the taste.</p><p>/</p><p>This changed something. Not because Claude now “writes like me” — that’s too simplified. It changed something because I now understand my own craft in a way I didn’t before. I can see the moves I make. I can name them.</p><p>Alexander wanted patterns to be legible to ordinary people — not just architects. When he interviewed for a position as head of the architecture department at Cambridge and was asked who his first hire would be, he said a carpenter. The panel pressed: which <em>named architect</em>? He persisted. Who after the carpenter? “A mason.” He didn’t get the job.</p><p>The point was radical to academics: the people who make things should understand the patterns they’re working with. The knowledge shouldn’t live with experts alone. It should be owned by the people who construct and inhabit the spaces.</p><p>That’s what happened when I extracted my writing patterns. The knowledge stopped being locked inside intuitive practice and became something I could hold, examine, refine. The instructions aren’t just for Claude. They’re for me — a mirror held up to accumulated craft.</p><p>The philosopher Michael Polanyi captured this asymmetry in a phrase — “We can know more than we can tell.” His point was that most sophisticated human skill is tacit — embedded in the body, in habit, in intuition — and therefore almost impossible to fully articulate. What I find remarkable about writing instructions for AI is that it forces exactly this kind of articulation. The tacit becomes explicit, not because the AI demands it, but because without that explicitness, the AI produces something competent and hollow. You have to tell it what you know.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*-bFFsDcP6F0XSxBTYfz6sg.png"" /><figcaption><em>Collage includes: K-pop inspired hairstyle recreation; the Rosetta Stone; an illustration from Wired Magazine by Koma Zhang.</em> <em>Image credit: Personal photograph (hairstyle); The Rosetta Stone, © The Trustees of the British Museum; illustration by</em><a href=""https://www.instagram.com/komaciel/""><em> Koma Zhang</em></a><em> for Wired Magazine.</em></figcaption></figure><p>/</p><p>My writing process these days is messy, but <em>it is happening</em>. I am always the first one up, before the kids are fully conscious, and I talk. Speech input — Wispr Flow into a Google Doc on my phone — putting ideas down while making breakfast or packing lunches. It gives my writing a spoken quality I actually prefer to what comes out when I type carefully. Then I bring those fragments to the Claude project space where my instructions are housed — along with other writings as context and memory — and we go back and forth. Claude generates a draft. I edit, weave concepts in and out, rewrite in a Google Doc, send it back. And so on. The AI holds the scaffolding; I do the building.</p><p>I want to be clear: this is not content generation. Not slop. I hope you are experiencing the difference.</p><p>Language is a technology. This is easy to forget because we are born into it, but it is one of the oldest and most sophisticated tools humans have ever developed — and some people have a mastery of it that I do not. My spelling is terrible. My punctuation, left to my own devices, is creative at best. But I know how I want something to feel. I know how I want it to land. And a large language model is not magic. It is human language, compressed. Billions of words written by people, distilled through neural networks into mathematical weights and probabilities — patterns of how language follows language, learned at a scale no individual could achieve. When I give it instructions drawn from my own writing, I am not outsourcing my voice. I am using the largest pattern-recognition engine ever built to help me stay faithful to patterns I already own but cannot always execute alone.</p><p>That’s what the instructions capture. Not rules. Sensibility. The pattern is the structure. What emerges — if the pattern is well crafted — is the quality without a name.</p><p>/</p><p>I was listening to the<a href=""https://www.youtube.com/watch?v=_1KQRPtgiM0&amp;list=PLB9gMmtMLXxsa8C0PzHFL2tJFh7FrKrYD""> Hard Fork podcast</a> recently and they covered romance novel writers using AI. New York Times reporter Alexandra Alter talked about how the explosion of AI-assisted books has made one thing obvious: you need very specific instructions if you want anything convincing for the genre. There are tells. Phrases the models default to when you don’t guide them. AI is still bad at human emotion, at nuance, at the art of the slow burn. Apparently “he said her name like a ragged prayer” appears in book after book — a phrase the model’s weights favor heavily, surfacing again and again across novels by different AI-assisted authors. One prolific romance writer turned instructor, Carol Hart, tells her students to always add the LLM instruction: <em>make it slow and agonizing. Do not rush to the finish.</em></p><p>This made me laugh, but also made sense. The unguided, uninstructed output has a specific lack of texture. You can feel it. Competent and smooth and saying nothing of depth. Falling back on the same tricks. No quality without a name. It’s the architectural equivalent of a building that follows code but has no life in it — technically correct, humanly dead. Alexander spent his career fighting exactly this. The pattern exists to prevent it. The instruction exists to prevent it.</p><p>The craft is in the specificity. In the personal. In the particular.</p><p>Katy Neale, a UX designer who writes about the intersection of AI and practice, recently articulated this from the product design side. In<a href=""https://medium.com/@katyneale/prompt-engineering-is-the-new-ux-skill-b3a58cb00e14""> her piece on Medium</a>, she argues that prompt engineering is less a technical skill than a design skill — that it requires the same kind of user-contextual thinking designers already do, just applied to the AI system itself: “We’re not just learning a new tool — we’re discovering a fundamental skill that will define how humans interact with technology for decades to come.” She’s right. But I want to push it a little further: we’re not just communicating with the machine. We’re learning to articulate ourselves.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*4JRJlDprL3QfPB0zl0SuZA.png"" /><figcaption>Collage includes: <em>A few example Topics with their corresponding Descriptions and Instructions from testing on a fictional shipping company called “ShipIT.”</em> <em>Image credit: Original documentation created by the author for AI testing purposes.</em></figcaption></figure><p>/</p><p>As you might imagine, I am finding deep resonance between this personal writing practice and my day job. I lead the design of an AI agent for customer service representatives. It helps reps resolve cases by generating step-by-step service plans grounded in a company’s actual policies, knowledge base, and case data. The quality of those plans depends almost entirely on one thing: the quality of the instructions.</p><p>In the system we’re working within, you write <em>topics</em> — categories that tell the agent what kind of case it’s looking at — and <em>instructions</em> — the specific guidance the agent should follow. Add a skill and those intructions can take actions. A topic might be “Return Request.” An instruction might be: “Make sure the customer has provided the required information, including a valid return date, customer name, and receipt or order number.” Another: “If the customer provides the order number, then search the return in the Order Management System using the order number and return date.” Simple language. Clear conditional logic. One task per instruction. If this, then that. Always, must, verify.</p><p>What we’re asking enterprise customers to do is exactly what Alexander proposed: build a pattern language. Each topic is a category. Each instruction is a pattern — a recurring problem paired with its solution, expressed clearly enough that anyone — or any reasoning engine — can apply it, but specific enough that what emerges is grounded in that company’s actual reality.</p><p>What I didn’t expect was what happens to companies in the process of writing those instructions. The act of articulating how your business works — clearly enough for a reasoning engine to follow — is a form of forced introspection. Organizations discover that processes they assumed were well-defined actually live in someone’s head, or differ team to team, or were never documented at all. You try to tell an agent how to handle a return and realize there <em>is</em> no documented policy for the edge case. You write an instruction for escalation and discover that different offices escalate differently and nobody has reconciled the approaches.</p><p>This connects to something knowledge management researchers have been tracking for decades: what Polanyi called tacit knowledge is precisely what organizations lose when experienced employees leave, and precisely what AI instruction-writing forces them to surface. Writing instructions isn’t just configuration. It is, as one research framework puts it, <em>externalization</em> — converting what has lived in practice into something that can be shared, reviewed, and refined. The same way extracting my writing patterns made my craft visible to me, writing AI instructions makes a company’s operational logic visible to itself.</p><p>And the testing is where the craft deepens. The word “artisan” has started circulating in conversations about developers working with AI agents — and it resonates. You write instructions, test them against hundreds of simulated interactions, review where the agent fails or drifts, trace its reasoning, refine, test again. It’s iterative and nondeterministic — you cannot predict the output the way you can with traditional software, because these are reasoning systems, not rule engines. Success requires a disposition toward experimentation, toward relationship with the system rather than control of it.</p><p>And like Alexander’s patterns, the best instructions generate something that exceeds them. A dynamic service plan that evolves in real time as new information arrives — creating resolution steps on the fly, checking whether an action can be automated, falling back to human guidance when it can’t. The instructions provide the structure. But the quality without a name — that lives in the encounter itself. How the customer experiences the company. How they feel treated. How what started as a support interaction begins to feel like relationship.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*SuFqHJNASOGRrhdWcsRcVQ.png"" /><figcaption>Collage includes: <em>Diagram showing the flow of information from a voice call through an AI reasoning engine, through protocol identification, to the AI agent generating a service plan for the representative — with Ryoji Ikeda’s “data-verse 1” (2019) layered underneath.</em> <em>Image credit: Original process diagram by the author; Ryoji Ikeda, “data-verse 1,” 2019. Image of artwork used for illustrative/editorial purposes.</em></figcaption></figure><p>/</p><p>B. Joseph Pine II — the same thinker who gave us <em>The Experience Economy</em> — argues in his most recent work that we are entering what he calls the Transformation Economy. Goods, services, even memorable experiences are no longer enough. Customers want to change. They want to become healthier, more knowledgeable, more capable. They want to reach their aspirations. The highest form of economic value, Pine argues, is guiding that transformation.</p><p>I’ve been sitting with this idea because it sharpens everything I’ve been describing. If Pine is right — and I think he is — then the bar for enterprise instructions rises dramatically. It’s no longer enough to resolve a case or close a ticket. The instructions need to create structure for personalized experiences that move beyond traditional support. The interaction between a brand and its customer — at the point of contact, in the moment of need — becomes an opportunity not just for resolution but for growth.</p><p>This is where I think instruction-writing becomes intellectual property.</p><p>Just as I understand the mechanics of how I write — because I know how I want it to land — companies can understand how they want their brand to be experienced and begin to codify that into instructions. We used to have knowledge articles and training manuals. Static documents. One-size-fits-all. Now we have instructions that generate dynamic, adaptive experiences — addressing a specific person, a specific problem, in a specific moment. The interaction is repeatable in structure but unique in execution.</p><p>The companies that invest in the quality and depth of their instructions will differentiate themselves. Their instructions will reflect how deeply they understand their own customers, their own processes, their own values. That understanding — encoded into patterns clear enough for AI to follow but rich enough to feel alive — becomes a form of brand. Not brand as logo or tagline, but brand as experience. The felt quality of interacting with an organization that knows itself and knows what it wants for the people it serves.</p><p>The instruction set will be the IP. The craft of writing it will be the competitive advantage.</p><p>/</p><p>There’s a flip side. The explosion of content — writing, agents, messages, recommendations — is real. More people will use these tools, more output will flood every channel, and the question of how we digest it all doesn’t have an answer yet.</p><p>But I feel good about being able to formulate my thoughts in a way that feels like me. It’s using a technology built from human language to amplify the intelligence I already have. To carry ideas I would have lost in the noise of daily life. To maintain a practice of thinking that the full weight of living had nearly crowded out.</p><p>Alexander’s first book was called <em>Notes on the Synthesis of Form</em> — rigid, scientific, prescriptive. By the time he wrote <em>The Timeless Way of Building</em>, he’d moved toward something softer, more alive. He’d realized that the pattern was necessary but never sufficient. That the life in a building — or a sentence, or an AI-aided support interaction — comes from somewhere the pattern points to but cannot reach.</p><p>The craft of the instruction is the same. It’s the pattern you build so that something beyond the pattern can emerge. Know your own patterns. Name them. Refine them. Own them.</p><p>The craft is always yours. The instruction helps you remember.</p><p><em>This article was written with Claude using writing style instructions developed by analyzing previous work. I also drew on</em><a href=""https://uxdesign.cc/a-ux-designer-guide-to-prompt-f045cd839489""><em> Paz Perez’s guide to prompting for UX designers</em></a><em> published in UX Collective,</em><a href=""https://medium.com/@katyneale/prompt-engineering-is-the-new-ux-skill-b3a58cb00e14""><em> Katy Neale’s writing on prompt engineering as a UX skill</em></a><em>, and the</em><a href=""https://www.youtube.com/watch?v=_1KQRPtgiM0&amp;list=PLB9gMmtMLXxsa8C0PzHFL2tJFh7FrKrYD""><em> Hard Fork podcast episode on AI-assisted romance writing</em></a><em>. Christopher Alexander’s pattern language framework is drawn from A Pattern Language (Oxford University Press, 1977) and The Timeless Way of Building (Oxford University Press, 1979). Michael Polanyi’s concept of tacit knowledge is from The Tacit Dimension (Doubleday, 1966). B. Joseph Pine II’s transformation economy framing draws from his ongoing work following The Experience Economy (Harvard Business Review Press, 1999).</em></p><p><em>Graphic collage inspiration: </em><a href=""https://laurasullivancassidy.com/""><em>Laura Cassidy</em></a><em> from </em><a href=""https://grieversball.substack.com/""><em>Grievers Ball.</em></a></p><p><em>Very open to feedback and hearing about your own instruction-writing practice — personal or enterprise.</em></p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f0da34c445cb"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/the-craft-of-the-instruction-f0da34c445cb"">The craft of the instruction</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
