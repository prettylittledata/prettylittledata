source,domain,url,created_utc,title,text
rss,smashingmagazine.com,https://smashingmagazine.com/2025/11/css-gamepad-api-visual-debugging-css-layers/,1763125200,CSS Gamepad API Visual Debugging With CSS Layers,"CSS Gamepad API Visual Debugging With CSS Layers

Debugging controllers can be a real pain. Hereâ€™s a deep dive into how CSS helps clean it up and how to build a reusable visual debugger for your own projects."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/11/older-tech-browser-stack/,1763020800,Older Tech In The Browser Stack,"Older Tech In The Browser Stack

There are many existing web features and technologies in the wild that you may never touch directly in your day-to-day work. Perhaps youâ€™re fairly new to web development and are simply unaware of them because youâ€™re steeped in the abstraction of a specific framework that doesnâ€™t require you to know it deeply, or even at all. Bryan Rasmussen looks specifically at XPath and demonstrates how it can be used alongside CSS to query elements."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/11/effectively-monitoring-web-performance/,1762855200,Effectively Monitoring Web Performance,"Effectively Monitoring Web Performance

There are lots of tips for [improving your website performance](https://www.debugbear.com/blog/improve-website-performance?utm_campaign=sm-10). But even if you follow all of the advice, are you able to maintain an optimized site? And are you targeting the right pages? Matt Zeunert outlines an effective strategy for web performance optimization and explains the roles that different types of data play in it."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/11/smashing-animations-part-6-svgs-css-custom-properties/,1762527600,Smashing Animations Part 6: Magnificent SVGsÂ With `<use>`Â And CSS Custom Properties,"Smashing Animations Part 6: Magnificent SVGsÂ With `<use>`Â And CSS Custom Properties

SVG is one of those web technologies thatâ€™s both elegant and, at times, infuriating. In this article, pioneering author and web designer Andy Clarke explains his technique for animating SVG elements that are hidden in the Shadow DOM."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/11/practical-guide-ux-strategy/,1762347600,Six Key Components of UX Strategy,"Six Key Components of UX Strategy

Letâ€™s dive into the building blocks of UX strategy and see how it speaks the language of product and business strategy to create user value while achieving company goals. Part of the <a href=""https://measure-ux.com/"">Measure UX &amp; Design Impact</a> (use the code ğŸŸ <code>IMPACT</code> to save 20% off today)."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/11/how-leverage-component-variants-penpot/,1762250400,How To Leverage Component Variants In Penpot,"How To Leverage Component Variants In Penpot

With component variants, design systems become more flexible, letting you reuse the same component while adapting its look or state with ease. In this article, Daniel Schwarz demonstrates how design tokens can be leveraged to manage components and their variations using <a href=""https://penpot.app?utm_source=SmashingMag&amp;utm_medium=Article&amp;utm_campaign=Variants"">Penpot</a>, the open-source tool built for scalable, consistent design."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/10/desktop-wallpaper-calendars-november-2025/,1761912000,Fading Light And Falling Leaves (November 2025 Wallpapers Edition),"Fading Light And Falling Leaves (November 2025 Wallpapers Edition)

The new month is just around the corner, and that means: Itâ€™s time for some new desktop wallpapers! All of them are designed by the community for the community and can be downloaded for free. Enjoy!"
rss,smashingmagazine.com,https://smashingmagazine.com/2025/10/javascript-for-everyone-iterators/,1761570000,JavaScript For Everyone: Iterators,"JavaScript For Everyone: Iterators

Here is a lesson on Iterators. Iterables implement the iterable iteration interface, and iterators implement the iterator iteration interface. Sounds confusing? Mat breaks it all down in the article."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/10/ambient-animations-web-design-practical-applications-part2/,1761138000,Ambient Animations In Web Design: Practical Applications (Part 2),"Ambient Animations In Web Design: Practical Applications (Part 2)

Motion can be tricky: too much distracts, too little feels flat. Ambient animations sit in the middle. Theyâ€™re subtle, slow-moving details that add atmosphere without stealing the show. In part two of his series, web design pioneer Andy Clarke shows how ambient animations can add personality to any website design."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/10/ai-ux-achieve-more-with-less/,1760688000,AI In UX: Achieve More With Less,"AI In UX: Achieve More With Less

A simple but powerful mental model for working with AI: treat it like an enthusiastic intern with no real-world experience. Paul Boag shares lessons learned from real client projects across user research, design, development, and content creation."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/10/how-make-ux-research-hard-to-ignore/,1760619600,How To Make Your UX Research Hard To Ignore,"How To Make Your UX Research Hard To Ignore

Research isnâ€™t everything. Facts alone donâ€™t win arguments, but powerful stories do. Hereâ€™s how to turn your research into narratives that inspire trust and influence decisions."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/10/the-grayscale-problem/,1760349600,The Grayscale Problem,"The Grayscale Problem

From A/B tests to AI slop, the modern web is bleeding out its colour. Standardized, templated, and overoptimized, itâ€™s starting to feel like a digital Levittown. But it doesnâ€™t have to be."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/10/smashing-animations-part-5-building-adaptive-svgs/,1759755600,"Smashing Animations Part 5: Building Adaptive SVGs With `<symbol>`, `<use>`, And CSS Media Queries","Smashing Animations Part 5: Building Adaptive SVGs With `<symbol>`, `<use>`, And CSS Media Queries

SVGs, they scale, yes, but how else can you make them adapt even better to several screen sizes? Web design pioneer Andy Clarke explains how he builds what he calls â€œadaptive SVGsâ€ using ``, ``, and CSS Media Queries."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/10/intent-prototyping-practical-guide-building-clarity/,1759485600,Intent Prototyping: A Practical Guide To Building With Clarity (Part 2),"Intent Prototyping: A Practical Guide To Building With Clarity (Part 2)

Ready to move beyond static mockups? Here is a practical, step-by-step guide to Intent Prototyping &mdash; a disciplined method that uses AI to turn your design intent (UI sketches, conceptual models, and user flows) directly into a live prototype, making it your primary canvas for ideation."
rss,uxdesign.cc,https://uxdesign.cc/the-illusion-of-unmoderated-ux-testing-503137621407?source=rss----138adf9c44c---4,1763308091,The illusion of unmoderated UX testing,"The illusion of unmoderated UX testing

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/the-illusion-of-unmoderated-ux-testing-503137621407?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/1453/1*lrjUpSwINLE1yANldzaPIA.png"" width=""1453"" /></a></p><p class=""medium-feed-snippet"">I&#x2019;ve been testing the testing. Unmoderated and moderated UX tests with the same type of people on the same topic generate completely&#x2026;</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/the-illusion-of-unmoderated-ux-testing-503137621407?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/dear-llm-heres-how-my-design-system-works-b59fb9a342b7?source=rss----138adf9c44c---4,1763301560,"Dear LLM, hereâ€™s how my design system works","Dear LLM, hereâ€™s how my design system works

<h4>How to get production-ready code from AI by structuring Figma file, connecting your design system with Figma MCP, and writing betterÂ prompts.</h4><figure><img alt=""Overview of components like toggle, badges, buttons, data visualisation cards, forms and page header from Ivy Design System"" src=""https://cdn-images-1.medium.com/max/1024/1*4ecR3i5SgK488SMg3drtIA.png"" /><figcaption>Image by the author, overview of Ivy Design System components</figcaption></figure><p>Right, pull up a chair, grab your coffee. We need to talk about the new team member. Theyâ€™re brilliant, frankly, work all hours and can write boilerplate code in their sleep. Theyâ€™re also a bitâ€¦ literal. This new team member is an AI agent, and itâ€™s changing how we go from design toÂ code.</p><p>But hereâ€™s the reality check as <a href=""https://www.figma.com/blog/figma-2025-ai-report-perspectives/"">Figmaâ€™s recent AI report</a> â†— found that while 68% of developers are using AI to write code, only 32% actually trust the output. The problem isnâ€™t the AIâ€™s ability to write code, itâ€™s the AIâ€™s ability to understand context.</p><p>Now, we could spend a whole other coffee chat talking about how brilliant AI is for spinning up quick interactive prototypes for stakeholders and testing. And, indeed, itâ€™s a valid way to work. But to grow trust in AI to write real, production-ready code, we have to <strong>give it clean data with clear context to work with in the firstÂ place.</strong></p><p>So, how does this happen? Until recently, we could only give an AI a screenshot and hope for the best. Thatâ€™s changing with something called the <a href=""https://modelcontextprotocol.io/docs/getting-started/intro"">Model Context Protocol (MCP)</a> â†—. Think of the new Figma MCP server as an interpreter for your Figma files that sends rich, structured information to the LLM. The process looks simple: Figma file â†’ Figma MCP â†’ LLM. But for it to work, every step needs to be packed with meaning. <strong>Simply throwing a prompt and a Figma link at an AI agent and hoping for the best is not going toÂ work.</strong></p><h3>Build foundation inÂ Figma</h3><p>The foundation of any good design-to-code workflow, whether for a human or an AI, is a Figma file that clearly communicates its own intent. Every decision you make in how you structure your file can either bring clarity or create confusion.</p><p>First things first, master your <strong>structure hygiene</strong>. An AI reads your layer tree to understand your designâ€™s structure, so a messy tree leads to a messy DOM. Name everything with purpose. Instead of <em>Frame 74</em>, use names that describe what something <em>is</em> or what it <em>does</em>, like <em>CreateProjectModal</em> or <em>ProjectForm</em>. This directly influences the names of the components and elements the AI generates. While youâ€™re at it, try to keep your structure as flat as possible. Avoid deep, unnecessary nesting. If a group isnâ€™t serving a specific layout purpose, get rid of it. A flatter hierarchy is simply easier for everyone to understand.</p><figure><img alt=""A before-and-after comparison of a Figma layers panel, showing a messy structure with generic names like â€˜Frame 79â€™ being replaced by a clean, semantic structure with names like â€˜ProjectFormâ€™ and â€˜Group-DateRange"" src=""https://cdn-images-1.medium.com/max/1024/1*81rdmmTMHIezX1y46fuLag.png"" /><figcaption>Image by the author, showing the improvement from generic layer structure to a clean, semantically-named one</figcaption></figure><p>Next<strong> think and build components like a developer </strong>defining their API. Your layer order should reflect the visual hierarchy or, for the web, the DOM order. This is a massive help for accessibility and logical codeÂ output.</p><ul><li>Use variants for states like <em>State: Default | Hover | Disabled</em>. This maps directly to CSS pseudo-classes or stateÂ props.</li><li>Use a boolean property (Icon: True | False) to show or hide an element. Itâ€™s cleaner than separate variants and translates perfectly to a prop like <em>showIcon={true}</em>.</li><li><strong>Use the </strong><a href=""https://help.figma.com/hc/en-us/articles/35794667554839-What-s-new-from-Schema-2025#h_01K84PB569NTFMXBDS36C8DCPV""><strong>new slots</strong></a><strong> </strong>â†—<strong> feature from Figma to define flexible content areas.</strong> This is a massive improvement over simple instance swaps because it directly maps to the concept of â€œslotsâ€ or the children prop inÂ code.</li></ul><p>Letâ€™s be honest, <strong>Auto Layout</strong> should be your default for almost everything. Itâ€™s the clearest way to communicate layout intent. Use absolute positioning sparingly, keeping it for specific cases like notification badges or modal overlays. Weâ€™ve all seen the mess that older design-to-code tools made by sprinkling <em>position: absolute</em> everywhere.</p><p>Move beyond primitive names like <em>blue-500</em> and embrace <strong>semantic tokens</strong> that describe purpose. For example, <em>color-button-background-brand</em> tells the AI not just what color this is, but <em>why</em> it exists. This tells the AI <em>why</em> this color exists. <strong>For a masterclass on this, check out t</strong><a href=""https://medium.com/design-systems-collective/when-semantic-tokens-are-no-longer-semantic-d65ef16fadd7""><strong>he article series by Nate Baldwin</strong></a><strong>Â </strong>â†—</p><figure><img alt=""An infographic explaining semantic design tokens. It shows a table where raw hex codes are mapped to base tokens like â€˜brand-100â€™, which are then mapped to purpose-driven semantic tokens like â€˜bg-fill/brand/secondaryâ€™. The image also includes a diagram of the semantic naming structure and an example of the tokens applied to a UI component."" src=""https://cdn-images-1.medium.com/max/1024/1*B9GJkT08-NHbTWmM3IFquQ.png"" /><figcaption>Image by author, Visual breakdown of how to move from primitive color values to a purpose-driven semantic tokenÂ system</figcaption></figure><p>When some context canâ€™t be built, <strong>use Figmaâ€™s annotation tools</strong> or plugins like <a href=""https://www.figma.com/community/plugin/859894273811051899/annotate-it""><strong>Annotate It!</strong></a><strong> </strong><a href=""https://uxdesign.cc/feed""><strong>â†—</strong></a> to explicitly call out interaction details, accessibility requirements, or behavior. These notes will become part of the prompt eventually.</p><figure><img alt=""Screenshot of a Figma componentâ€™s annotations that includes details on interaction, development aand accessibility of â€œ_table header cellâ€"" src=""https://cdn-images-1.medium.com/max/1024/1*lnEgDXl3h46LV83M0Qy6_Q.png"" /><figcaption>Screenshot by the author, demonstrating component annotation inÂ Figma</figcaption></figure><p>While this may feel like a demanding level of detail, this very precision is a huge efficiency gain. <strong>By giving the LLM clear context from your designs, you help it use far fewer tokens to get the rightÂ answer.</strong></p><h3>Map designs toÂ codebase</h3><p>The gold standard is to explicitly map designs to code, and creating your own codebase. An AI should consume your existing components, not generate new ones. Without a direct link to your codebase, the AI relies on inaccurate searches that result in redundant code. Tools like <a href=""https://developers.figma.com/docs/figma-mcp-server/code-connect-integration/"">Figmaâ€™s Code Connect </a>â†— or third-party integrations with Storybook create this essential link, turning your Figma components into pointers to the real components in your repository.</p><figure><img alt=""A screenshot of the Code Connect UI feature linking a Figma component to its corresponding React code. Image from the official Figma developer documentation"" src=""https://cdn-images-1.medium.com/max/960/1*vDBzgtCiML2l4B5cQlmvRw.gif"" /><figcaption>A screenshot of the Code Connect UI feature linking a Figma component to its corresponding React code. Image from the <a href=""https://developers.figma.com/docs/code-connect/code-connect-ui-setup/"">official Figma developer documentation</a></figcaption></figure><h4>But what if you donâ€™t have a codebaseÂ yet?</h4><p>You can absolutely use these tools without a connected codebase. The AI will analyze your Figma file and generate new code from scratch that is great for quick prototypes or iterating on ideas. However, understand the trade-offs: you get generated code, not system code; itâ€™s less efficient; and it <strong>creates technical debt that a developer will have to refactorÂ later</strong>.</p><p>A practical middle ground is to <strong>use the AI to create a â€œscaffold.</strong>â€ Let it generate the initial components, then treat that output as the first version of your design systemâ€™s codebase. From there, you refine it, connect it back to Figma, and build a sustainable system.</p><h3>Extra: The ultimate source of truth with components asÂ data</h3><p>The ultimate source of truth is defining components as structured data (JSON), not as Figma drawings. For a deeper dive into this architectural approach, <a href=""https://medium.com/@nathanacurtis/components-as-data-2be178777f21""><strong>Nathan Curtisâ€™ article â€œComponents as Dataâ€</strong></a> â†— is an essential read. LLMs thrive on structured data, whereas the standard MCP workflow provides an <em>â€œincomplete and impreciseâ€</em> interpretation of your visual design. Tools like the <a href=""https://www.anova.design/""><strong>Anova plugin for Figma</strong></a><strong> </strong>â†— can help you get started. This data-first approach is the future of building AI-ready designÂ systems.</p><h3>Give your AI a â€œcheatÂ sheetâ€</h3><p>Whether youâ€™re connecting to code or defining components as data, you still need to guide the AIâ€™s behavior. This involves two skills: writing better prompts and creating a set of rules for the AI to follow. On moreÂ in-depth</p><h4>Write context-rich prompts</h4><p>Your prompt is your direct instruction. The more specific, theÂ better.</p><ul><li><strong>Instead of:</strong> â€œMake this a component.â€</li><li><strong>Try:</strong> â€œGenerate a React component for the selected frame using our design system library. Place the new file in src/components/ui/ and name it PricingCard.tsx.â€</li></ul><p><strong>Split your work into consumable bites</strong>: the nav bar, then the sidebar, then a content card. If you give them too much at once, AI agents can â€œchokeâ€ on the context and create a mess. Build your UI gradually.</p><h4>Set customÂ rules</h4><p>While one-off prompts are great for specific tasks, the real power comes from creating a permanent â€œcheat sheetâ€ that the AI can reference every singleÂ time.</p><p>In your project root, create a dedicated folder likeÂ <em>.docs/</em> orÂ <em>.ai/.</em> Inside, create your three core rules files: <em>README.md</em> file (for foundational rules), <em>design-system-rules.md</em> (for how to use our components), <em>figma-mcp-rules.md</em> (for the specific Figma MCP workflow).</p><figure><img alt=""Screenshot of Cursor IDE with project structure and particularly dedicated folder for AI rules that incudes README.md, design-system-rules.md, figma-mcp-rules.md files. README.md file is explicitly shown opened"" src=""https://cdn-images-1.medium.com/max/1024/1*dGI8HhOgtit-vfgQYv8LOA.png"" /><figcaption>Screenshot by the author of Cursor IDE with project structure and dedicated folder for AIÂ rules</figcaption></figure><p>In your IDE settings or at the start of every prompt, you now reference README.md file that instruct the AI on how to use them together.</p><p><strong>README.md<br /></strong>This file acts as the primary entry point. It defines the core tech stack and file structure, and most importantly, it instructs the AI to use the other two files as part of itsÂ context.</p><pre># AI Coding Guidelines: <br />This repo uses Figma MCP. For instructions, for styles, mappings and design of components you MUST read and strictly apply the rules from all three of the following files:<br />1.  This `README.md` file (for foundational rules).<br />2.  `design-system-rules.md` (for how to use our components).<br />3.  `figma-mcp-rules.md` (for the specific Figma-to-code process).---<br />## Core Principles &amp; Best Practices<br />- Expert Persona: Act as an expert senior frontend developer writing clean, accessible, and maintainable TypeScript and React.<br />- Accessibility: All components must meet WCAG 2.1 AA standards.<br />- Performance: Optimize for performance. Code should have linear time/space complexity where possible.<br />- Testing: Suggest testable code.<br />---<br />## Core Technologies (React &amp; Tailwind)<br />- Framework: React<br />- Language: TypeScript<br />- Styling:Tailwind CSS, configured via `tailwind.config.js`.<br />---<br />## File Structure &amp; Naming Conventions<br />- Components: Place all new components in `src/components/`.<br />  - Reusable UI Primitives:`src/components/ui/` (e.g., Button, Input).<br />  - Feature-Specific Components: `src/components/feature/` (e.g., `UserProfileCard`).<br />- Component Files: Use PascalCase for filenames. Each component must be in its own folder. (e.g., `src/components/ui/Button/Button.tsx`)<br />- Hooks: Custom hooks go in `src/hooks/` and should be named with the `use` prefix (e.g., `useUserData.ts`).</pre><p><strong>design-system-rules.md<br /></strong>This is the detailed guide on how to correctly use your custom components from your-design-system.</p><pre># Design System Usage Rules<br />This document outlines how to correctly implement components and styles from `your-design-system`.<br />---<br />## Component Architecture &amp; Styling<br />- Design System First: Always use existing components from the `your-design-system` package. Do not rebuild them.<br />- Layout Primitives: Always use layout components from `your-design-system` (e.g., ``, ``). Do not use raw `div`s with custom flexbox CSS.<br />- Styling with Tokens: Use Tailwind utility classes that are configured in our `tailwind.config.js`. Prefer our custom theme utilities (e.g., `bg-brand-primary`) over default Tailwind colors.<br />- Icons: Use the `` component from `your-design-system`, passing the appropriate icon name. Do not import raw SVGs.<br />- Props: Component props must be defined with a TypeScript `interface`.<br />---<br />## What to Avoid<br />- No Hardcoded Values: Do not use hardcoded strings (use translation files), URLs (use config files), or styling values (use tokens).<br />- No Inconsistent Naming: Follow the project's naming conventions.<br />- No Ignoring Errors: Do not ignore TypeScript errors.<br />- No Unnecessary DOM: Avoid unnecessary `div` wrappers.</pre><p><strong>figma-mcp-rules.md<br /></strong>This file is a specific, process-oriented set of instructions for the AI to follow whenever itâ€™s translating a design fromÂ Figma.</p><pre># Figma to Code Workflow Rules<br />When generating code from a Figma design, follow this specific process:<br />1.  Get Context First: Run `get_design_context` to fetch the structured representation of the Figma node.<br />2.  Get Visual Reference: Run `get_screenshot` for a visual reference.<br />3.  Implement: Only after you have both, begin implementation.<br />4.  Translate the MCP output (React + Tailwind) into our project's conventions, strictly following the rules defined in `README.md` and `design-system-rules.md`.<br />5.  Validate: Ensure the final UI has 1:1 visual parity with the Figma screenshot before completing.</pre><p>For a technical starting point, Figmaâ€™s developer documentation provides an excellent guide on how to <a href=""https://developers.figma.com/docs/figma-mcp-server/add-custom-rules/#example-prompt-to-generate-your-own-custom-rules""><strong>add custom rules</strong></a> â†—, including example prompts you can use to generate a baseline for theseÂ files.</p><h3>Putting it allÂ together</h3><p>Feeling overwhelmed? Donâ€™t be. Start small. Pick one component and make it â€œagent-ready.â€</p><ol><li><strong>Name everything semantically:</strong> Are your layers and components named for theirÂ purpose?</li><li><strong>Use auto layout and variables:</strong> Is your design intent baked into theÂ file?</li><li><strong>Annotate:</strong> Have you documented behaviors andÂ states?</li><li><strong>*Connect your components (if possible)Â :</strong> Link every system component to its code counterpart.</li><li><strong>Write a simple rules file:</strong> Can you create a basic README.md to guide theÂ AI?</li><li><strong>Prompt with generousÂ context</strong></li></ol><p>The teams that do this groundwork now are building the foundation for a future where design and development are in a constant, seamless loop. By treating our design systems as living sources of truth for both humans <em>and</em> machines, we can finally spend less time on tedious translation and more time building whatÂ matters.</p><p>Thanks for reading! I hope this sparked some ideas. Happy buildingÂ ğŸª´</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b59fb9a342b7"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/dear-llm-heres-how-my-design-system-works-b59fb9a342b7"">Dear LLM, hereâ€™s how my design system works</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/the-ship-of-theseus-problem-in-ai-writing-9435a4a370fa?source=rss----138adf9c44c---4,1763128228,The ship of Theseus paradox in AI-assisted writing,"The ship of Theseus paradox in AI-assisted writing

<h4>Turns out, the more feelings in your text, the weirder it feels to let AI touchÂ it.</h4><figure><img alt=""The Ship of Theseus reimagined: a vessel where each wooden plank is replaced with a silicon chip"" src=""https://cdn-images-1.medium.com/max/1024/1*MV2D6JbcFgdCKayve-w0rw.png"" /><figcaption>The Ship of Theseus reimagined: a vessel where wooden planks are being replaced with silicon chips. Generated inÂ Sora.</figcaption></figure><p>I use AI writing tools the way most people use spellcheck: casually, constantly, almost without thought. A typo here, a phrasing fix there, a quick â€œmake it flow better.â€ Over time, itâ€™s become second nature that I write <em>through</em> theÂ machine.</p><p>And thatâ€™s not unusual. Grammarly alone reports over <strong>40 million users</strong>, across <strong>50 thousand organizations</strong> and <strong>96 percent of the Fortune 500</strong> (<a href=""https://www.grammarly.com/about"">source: Grammarly.com</a>). The habit has gone mainstream, invisible, automatic, and everywhere.</p><p>But every so often, I pause over a polished paragraph and feel an odd flicker of detachment. The words look like mine, sound like mine, but theyâ€™ve passed through <strong>someone elseâ€™s hands</strong>. Or circuits. Or whatever metaphor fits. How many edits does it take before the voice that returns isnâ€™t meÂ anymore?</p><p>Itâ€™s the Ship of Theseus. Turns out Greek mythology still works in 2025. Swapping planks on a ship is just the new metaphor for rewriting yourself through syntax. Each replaced plank a synonym, each tightened sentence a substitution, until the vessel of thought sails on. The ship, and the writing, are familiar in shape butforeign inÂ soul.</p><p>And this anxiety about â€œlosing your voiceâ€ didnâ€™t arrive with ChatGPT. As others have noted, the fear of writing identity slipping away has circulated for years, long before AI became a default tool (see <a href=""https://medium.com/emma-identity/why-loss-of-writing-identity-is-a-myth-and-other-writing-debacles-85fda0b4b046"">Emma Identityâ€™s 2017 essay</a> on textual fingerprints). Whatâ€™s different now is how automated the erosionÂ feels.</p><h4>A Family-and-Friends FieldÂ Test</h4><p>I wanted to understand that uneasy drift between â€œmeâ€ and â€œmachine,â€ so I ran a small, human-scale test. A handful of volunteers, friends, family, and a few generous redditors who gave five minutes of their lives to a Qualtrics link joinedÂ in.</p><figure><img alt=""Flowchart showing participants providing three types of writing (casual text, essay paragraph, code snippet), sending each through an AI tool for ten iterative rewrites, and rating on a 1â€“7 scale how much the output still felt like â€œtheirs.â€"" src=""https://cdn-images-1.medium.com/max/1024/1*ut4rB0gbseUm7BDETSYkoQ.png"" /><figcaption>Diagram of the study procedure: participants wrote a casual text, an essay paragraph, and a code snippet, then passed each piece through an AI tool ten times and rated how much the rewritten text still felt like their own. Original illustration by theÂ author.</figcaption></figure><p>Each person worked across three familiar modes: a casual text, a short essay paragraph, and a code snippet. They wrote something genuine, then passed it through an AI writer of their choice ten times in a row. After each iteration, they rated it on a 1â€“7 scale: <strong>â€œHow much is this stillÂ mine?â€</strong></p><p>The survey itself ran on Qualtrics; I cleaned and visualized the data in R, using a simple mixed-effects model to trace how that sense of ownership decayed over time. Everyone consented, all responses were anonymized, and whatâ€™s shown here reflects only the aggregate picture.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*tsdSUozOICLkG9w_oCeT-w.png"" /><figcaption>Perceived ownership steadily declines with each AI rewrite, dropping fastest for texting and slowest forÂ code.</figcaption></figure><p>Turns out, the ownership ratings <strong>fell off a cliff</strong> around the third or fourth rewrite for texting and essay writing, but held surprisingly steady in code. A spline model (R, lme4, three-knot natural spline) showed the steepest drop early on: texting collapsed fastest, essays followed, code barely flinched.</p><p>By theÂ numbers:</p><ul><li><strong>Texting:</strong> sharpest drop of â€“1.4 points around iteration 3.</li><li><strong>Essay:</strong> a slower, smaller decline (â€“1.25 points near iteration 5).</li><li><strong>Code:</strong> almost no falloff (â€“0.3 points around iteration 4).</li></ul><p>When asked when their text felt â€œmore AI than meâ€ (below a 4 on the 1â€“7 scale), people hit that midpoint at <strong>iteration 3 for texting</strong>, <strong>iteration 7 for essays</strong>, and <strong>never</strong> forÂ code.</p><p>This aligns with a broader cultural confusion around what â€œhuman-writtenâ€ even looks like. As <a href=""https://alpace.medium.com/unless-it-was-written-by-ryan-holiday-it-was-probably-written-by-ai-0b7099516d98"">Aaron Pace</a> recently wrote, the more polished a piece of writing is, the more likely AI detectors are to label it â€œmachine-made.â€ Today we see a strange modern reversal where professionalism now reads as artificial. My data shows the inverse feeling: the messier and more intimate the writing, the more its owner wants to keep machinesÂ out.</p><h4>What the HumansÂ Said</h4><p>After the survey, I spoke with a few participants again, some over Zoom, some across a kitchen table. Their reactions lined up quite cleanly with the numbers. <strong>The more personal the writing, the more wrong it felt to let an AI touchÂ it.</strong></p><p>When it came to texting, everyone hesitated. That space is messy and intimate. Youâ€™d use lowercase shorthand, inside jokes, typos, emojis that say more than syntax ever could. Watching an AI rewrite that kind of message felt invasive, like handing your phone to someone else mid-conversation.</p><p>Essay writing was easier to surrender. Participants said it felt â€œprofessional,â€ â€œdetached,â€ something meant to be polished anyway. The AIâ€™s edits felt like a second pair of eyes rather than a replacement ofÂ self.</p><p>And code, unless you were a computer science student who lives in it, wasnâ€™t personal at all. In fact, most people said they <em>needed</em> AI to even get started, describing the help as â€œwelcome,â€ â€œexpected,â€ or â€œhonestly, a relief.â€ For them, authorship was more functional than emotional. The machine wasnâ€™t rewriting their voice, it was scaffolding theirÂ logic.</p><p>One person linked their experience to the idea of <strong>vibe coding</strong><em>, </em>which, as <a href=""https://www.ibm.com/think/topics/vibe-coding"">an IBM article described</a>, is <em>the emerging practice of writing code by talking to AI using natural language prompts instead of lines of syntax</em>. They acknowledged that software engineers often dismiss it as unserious, even lazy, but for them, it was liberating. As a nonâ€“computer science student, they said vibe coding made the process less intimidating and more creative.</p><p>It echoed something <a href=""https://medium.com/contemplate/chatgpt-grammarly-and-hemingway-walked-into-a-bar-de2fa481022f"">Jasmine McCandless</a> captured: people donâ€™t pick the â€œbestâ€ writing tool, they pick the one that demands the least effort. But effort can manifest in many ways. And one of them is emotional risk. In my study, the more personal the writing, the more people resisted AI intervention. Once the task shifted into professional or functional territory, the comfort gap widened and AI quietly became the low-effort choice.</p><figure><img alt=""A person giving a robot directions on what to write"" src=""https://cdn-images-1.medium.com/max/1024/1*7PDMmFtdCzfA4b_6caHVuA.png"" /><figcaption>Collaboration between a human writer and an AI assistant. Original illustration by theÂ author.</figcaption></figure><h4>Takeaways for UXÂ Design</h4><p>The tension here is becoming infrastructural. <a href=""https://epiren.medium.com/trust-verify-and-write-using-grammarlys-authorship-in-the-age-of-ai-1778e015dea0"">RenÃ© Najera</a> recently wrote about Grammarlyâ€™s new Authorship feature, which records your keystrokes and generates a replay of how a document came together. Their point is simple: in an age of ubiquitous AI tools, writers now feel pressure not only to produce authentic work, but to <em>prove</em> they made it. My findings speak to the same impulse from the user side: authorship isnâ€™t just who typed the words, but how much of the process still feels human, intentional, and personally owned.</p><p>Maybe the real product insight here is that Grammarly should just buy Copilot and call it a day. One interface to smooth every sentence, human or machine. Beneath the joke, though, is a deeper cue for design: peopleâ€™s comfort with AI assistance scales with emotional distance.</p><p>Texting felt intimate, messy, and human. Thatâ€™s where participants pulled back. They didnâ€™t want the machine polishing what was meant to be personal. Essay writing lived in the middle zone, where improvement feels professional rather than invasive. Code, on the other hand, was almost depersonalized; help there felt efficient, not existential.</p><p>The takeaway isnâ€™t to build less AI assistance. Itâ€™s to build <strong>context-aware</strong> assistance. In spaces where expression carries identity (e.g., messages, creative writing, voice) AI needs to step lightly, amplifying rather than rewriting. In functional spaces such as emails, documentation, and code, it can take the wheel without moral friction.</p><p>Designing for this gradient of intimacy means giving users <strong>control over how much authorship theyâ€™re willing to trade</strong>. The future of AI writing UX isnâ€™t about raw capability. Itâ€™s about sensitivity: knowing when to finish your sentence, and when to leave your typosÂ alone.</p><p>In practice, this could lookÂ like:</p><ul><li><strong>Surface adjustable â€œAI intensityâ€ controls directly in writing surfaces.</strong><br />Let users slide between light-touch suggestions (â€œfix punctuation onlyâ€) and heavier rewrites (â€œrewrite for clarityâ€).</li><li><strong>Use mode-aware defaults.</strong><br />If the system detects texting-like language (emojis, slang, rapid back-and-forth), default to minimal intervention. For structured writing (docs, reports, code blocks), default to more active assistance.</li><li><strong>Show the </strong>Î”<strong><em>delta</em>Â , not just the final rewrite.</strong><br />Display what was changed and why. Visible authorship boundaries reduce the feeling of being overwritten.</li><li><strong>Offer reversible, granular edits.</strong><br />Let users accept or reject changes at the sentence or phrase level, rather than forcing an all-or-nothing rewrite.</li><li><strong>Preserve voice automatically.</strong><br />Train style models on the userâ€™s past writing and ensure AI edits mimic that tone rather than flattening it.</li><li><strong>Avoid â€œdominant toneâ€ suggestions in intimate spaces.</strong><br />Donâ€™t push â€œprofessional tone,â€ â€œconfident tone,â€ or â€œassertive toneâ€ suggestions inside chat or messaging contexts. It reads as intrusive.</li><li><strong>Let users lock sentences or paragraphs.</strong><br />If a line feels emotionally important or identity-revealing, users can mark it: â€œdonâ€™t rewriteÂ this.â€</li><li><strong>Respect the emotional labor of writing.</strong><br />When edits touch personal content, frame them as support (â€œWant help clarifying this?â€) rather than replacement (â€œHereâ€™s a better versionâ€).</li></ul><p>Thereâ€™s an <a href=""https://medium.com/technology-hits/why-your-niche-is-your-shield-the-types-of-writing-ai-struggles-to-replace-dad50e79a01c"">argument</a> that emotionally grounded writing acts as a kind of â€œshieldâ€ that AI canâ€™t easily penetrate, the kind of work rooted in lived experience, personal history, or emotional nuance that resists mechanization by its very nature. And maybe thatâ€™s the point: the parts of writing that matter most arenâ€™t the ones AI can smooth; theyâ€™re the ones only a human canÂ feel.</p><p><em>Alice Ji is a PhD researcher at UIUCâ€™s Institute of Communications Research, studying digital persuasion, attention, and interface design. Portfolio at </em><a href=""https://alice-ji.github.io/project.html""><em>https://alice-ji.github.io/project.html</em></a></p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9435a4a370fa"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/the-ship-of-theseus-problem-in-ai-writing-9435a4a370fa"">The ship of Theseus paradox in AI-assisted writing</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/what-designers-can-learn-from-zohran-mamdanis-historical-campaign-11f92007e873?source=rss----138adf9c44c---4,1762989503,What designers can learn from Zohran Mamdani's historical campaign,"What designers can learn from Zohran Mamdani's historical campaign

<h4><em>How user-centered design principles transformed a grassroots political movement into a viral phenomenon and what it means for design practice.</em></h4><figure><img alt=""Logo with the words: Zohran for New York City in Orange with a blue background. Source: https://www.zohranfornyc.com/media-kit"" src=""https://cdn-images-1.medium.com/max/1024/1*buwhEL87lkuI_Q41zmNbBQ.jpeg"" /><figcaption><a href=""https://www.zohranfornyc.com/media-kit"">Zohran Mamdani Logo</a> designed by Aneesh Bhoopathy.</figcaption></figure><p>The year is 2025, and New York City, â€œThe Greatest City in The Worldâ€ has a 34-year-old, democratic socialist and Muslim mayor! Mamdaniâ€™s campaign has inspired millions, many of whom not New Yorkers and, most impressively, not even American or located inÂ America.</p><p>His campaign didnâ€™t just change politics, i<em>t offered a masterclass in user-centered design</em>.</p><h3>A Parks and Recreation campaign come toÂ life</h3><p>Mamdaniâ€™s campaign seemed (and continues to seem) something out of a Parks and Recreations episode. When Leslie Knope (played by Amy Poehler) decided to run for City council in season 4, she does so with her friends running every aspect of her campaign (after being let go by â€œofficialâ€ campaign managers), this gives her campaign an approachable, friendly and personal aspect that eventually led to herÂ victory.</p><p>Although specific names of the entire Mamdaniâ€™s campaign team are not extensively detailed in publicly available sources, it is known that the campaign operated with a community-driven approach characterized by deep community roots, and a decentralized yet well-coordinated volunteer network, with digital and field teams working closely to listen, test, and refine the outreach based on real-time input fromÂ voters.</p><figure><img alt=""Photograph of Zohran Mamdani speaking at a Fix the MTA press conference. Source: https://www.zohranfornyc.com/media-kit"" src=""https://cdn-images-1.medium.com/max/1024/1*mNzUIuCxNZEfjHtFn7haGw.jpeg"" /><figcaption><a href=""https://www.zohranfornyc.com/media-kit"">https://www.zohranfornyc.com/media-kit</a></figcaption></figure><h3>The team behind the viralÂ success</h3><p>The collaborators frequently mentioned in media coverage are Debbie Saslaw and Anthony DiMieri (for us Parks and Rec fans, thatâ€™s our Ben played by Adam Scott), co-founders and executive producers of <a href=""https://www.adweek.com/agencies/meet-melted-solids-the-scrappy-agency-behind-zohran-mamdanis-primary-winning-campaign/"">Melted Solids</a>, the firm behind many of the campaignâ€™s viral videos, they played a key role in shaping Mamdaniâ€™s message and managing his social media engagement, making this campaign the most viral political campaign in the past 50Â years.</p><blockquote>This lean, agile setup helped Mamdaniâ€™s campaign stay responsive and connected in a way that resonated broadly both locally and globally.</blockquote><figure><img alt=""Photo from an article in adweek: https://www.adweek.com/agencies/meet-melted-solids-the-scrappy-agency-behind-zohran-mamdanis-primary-winning-campaign/ of left to right: Anthony DiMieri, Kara McCurdy, Donald Borenstein, Zohran Mamdani and Debbie Saslaw."" src=""https://cdn-images-1.medium.com/max/1024/1*Va6OL1Z7DLivc58NtQRI8w.png"" /><figcaption>Left to right: Anthony DiMieri, Kara McCurdy, Donald Borenstein, Zohran Mamdani and Debbie Saslaw. Source: <a href=""https://www.adweek.com/agencies/meet-melted-solids-the-scrappy-agency-behind-zohran-mamdanis-primary-winning-campaign/"">https://www.adweek.com/agencies/meet-melted-solids-the-scrappy-agency-behind-zohran-mamdanis-primary-winning-campaign/</a></figcaption></figure><p><strong>Why design thinking matters in political campaigns</strong></p><blockquote>Why did Mamdaniâ€™s campaign resonate globally while better-funded opponents failed? The answer lies in principles every UX designer should understand but while keeping in mind questions of power, authenticity, and manipulation in digitalÂ spaces.</blockquote><p><a href=""https://medium.com/design-bootcamp/user-experience-politics-how-ux-can-be-applied-to-political-campaigns-4efe25435846"">Brad Oâ€™Conner</a> who transitioned into UX design after 15 years in political consulting, discovered that he had been practicing UX design principles in political campaigns long before transitioning into the field, writes &quot;The voters whom you are trying to reach are the users. The goal is to provide the voter with relevant information so that their decision to vote for your candidate or cause is made more accessible.â€</p><blockquote><strong><em>One key difference is that politics is also a lot more â€œmanipulativeâ€ thanÂ UX.</em></strong></blockquote><p>A study carried out by the <a href=""https://www.ox.ac.uk/news/2021-01-13-social-media-manipulation-political-actors-industrial-scale-problem-oxford-report"">Oxford Internet Institute</a> that documented organized social media manipulation campaigns in 81 countries, found that â€œgovernments and political parties â€œproduced misinformation on an industrial scale.â€ So as designers we must ask, <em>where does authentic transparency end and performance begin?</em></p><h3>1. Active listening as userÂ testing</h3><p>Mamdaniâ€™s campaign broke from the traditional political playbook by engaging directly with both supporters and critics on socialÂ media.</p><h4>The â€œultimatelyâ€ moment: iteration madeÂ visible</h4><p><a href=""https://www.instagram.com/zohrankmamdani/reel/DMBjabdOTPe/"">Zohran Kwame Mamdani on Instagram: &quot;I was given an ultimatum.&quot;</a></p><p><em>Alt text: Video of Zohran Mamdani catching himself mid-sentence about to say â€œultimatelyâ€ and self-correcting with humor</em> <em>Video credit: Zohran Mamdani Instagram / OriginalÂ creator</em></p><p>In one instance from July 2025, Zohran Mamdani, prompted by feedback from his communications team and voters, makes a conscious effort to stop himself from overusing the word â€œultimatelyâ€ during interviews and public appearances. In this light-hearted video, Mamdani acknowledges the feedback, shares clips illustrating his habit, and includes a segment whereâ€Šâ€”â€Šmid-sentenceâ€Šâ€”â€Šhe catches himself about to say â€œultimatelyâ€ and quickly redirects his phrasing.</p><blockquote><em>â€œI am listening, I am learning. Please keep sending me your feedback because ultimately, I will get better.â€<br />â€œIn response to my team and supporters pointing out that I say â€˜ultimatelyâ€™ a lot, I caught myself on camera correcting the habit. Keep holding me accountable. This is how we improve, together.â€ (ZohranÂ Mamdani)</em></blockquote><p>This approach mirrors what the <a href=""https://www.nngroup.com/articles/empathy-mapping/"">Nielsen Norman Group teaches about design thinking</a>: empathy mapping helps â€œdistill and categorize your knowledge of the user into one placeâ€ and â€œdiscover gaps in your current knowledge.â€ Mamdani essentially created a public empathy map of his own communication style, inviting collective feedback.</p><figure><img alt=""The User-Centered Design Loop: Mamdaniâ€™s Campaign as a Design Process. It shows a four-step circular loop labeled 1. Listen, 2. Analyze, 3. Iterate, 4. Ship, with arrows connecting each step around a central circle labeled â€œContinuous Improvement.â€ Each step includes short examples from Mamdaniâ€™s campaign, such as gathering voter feedback, reviewing patterns, adjusting speaking habits, and posting public updates. A note at the bottom reads: â€œKey Insight: Making the feedback loop visible builds&quot;"" src=""https://cdn-images-1.medium.com/max/1024/1*N1HgMHBwGYY0TvewMGXqpg.png"" /><figcaption>The User-Centered Design Loop: Mamdaniâ€™s Campaign as a DesignÂ Process.</figcaption></figure><h3>2. Building trust through transparency</h3><p>Mamdaniâ€™s response embraced feedback as a tool for self-improvement much like designers should seek out feedback and build on it in order to inspire trust in their designs. Furthermore, the transparency in documenting the process of listening and acknowledging the feedback is what inspired peopleâ€™s trust in him. This moment, in equal parts humorous and earnest, embodies the essence of user-driven iteration.</p><blockquote><strong><em>However</em></strong><em>, a question we, as designers, must ask ourselves is, how do we distinguish genuine transparency from performed authenticity? The Mamdani campaign appears to represent authentic engagement, and although their strategies are effective, they can also easily be replicated by good actors with bad intentions.</em></blockquote><blockquote><strong><em>Lessons for designers</em></strong><em>: Find the line between authentic iteration and manipulative performance and listen to your users, show them youâ€™re paying attention, and let the process of improvement happen in publicÂ view.</em></blockquote><h3>3. Small, agile teamsÂ win</h3><p>Mamdani did not have the funding for a large campaign nor was he backed by any billionaires. According to the official <a href=""https://www.nyccfb.info/vsapps/CandidateSummary.aspx?as_cand_id=2899&amp;as_election_cycle=2025&amp;cand_name=Mamdani"">campaign finance summary</a> from the New York City Campaign Finance Board for the 2025 election cycle, Zohran Mamdaniâ€™s total campaign spending was $12,794,272. His campaign raised $17,159,487 in total receipts, with over $4 million coming from private funds and significant additional support from public funds through matching programs. This is relatively lean compared to Cuomoâ€™s 40 million dollar budget which mostly came from super PACs and billionaire donors. Unlike Cuomoâ€™s traditional campaign operation, Mamdani relied on volunteers and a handful of core personnel to constantly iterate and connect at the groundÂ level.</p><h4>Hereâ€™s how an agile approach played out in practice:</h4><ul><li><strong>Culturally aware content</strong>: Mamdaniâ€™s team highlighted community through moments like salsa dancing in Bronx parks, sharing biryani with cabbies, or joining Tai Chi classes in Flushing. These resonated with the voters and quickly turnedÂ viral.</li></ul><figure><img alt=""Collage of eight campaign photos showing Zohran Mamdani at diverse community events including large crowd gatherings, group photos with supporters on steps and on motorcycles, media appearances on MSNBC Weekend, indoor community meetings, a Congresswoman Yvette Clarke endorsement poster, and candid moments speaking with constituents in outdoor settings Source: https://www.instagram.com/zohrankmamdani/"" src=""https://cdn-images-1.medium.com/max/1024/1*l4oBSYa9PKgbsRxCViYHDg.png"" /><figcaption>Collage of Mamdani at community events. Source: <a href=""https://www.instagram.com/zohrankmamdani/"">https://www.instagram.com/zohrankmamdani/</a></figcaption></figure><ul><li><strong>Collaborations born agile</strong>: Instead of focusing only on high-budget ad buys, Mamdani often teamed with social creators (like The Kid Mero and Subway Takes) and grassroots web series (â€œAre You Okay?,â€ â€œThe People Galleryâ€). These partnerships came together quickly, built on DMs and group chats rather than formal contracts.</li><li><strong>Volunteer-led feedback loops</strong>: The campaign harnessed a decentralized WhatsApp-powered network of volunteers for canvassing, â€œfriendraisers,â€ and â€œhouse parties.â€ First-time volunteers were rapidly trained and empowered to experiment, suggesting new neighborhoods to target, new stories to tell, and even filming spontaneous street interviews. This grassroots network was not only executing, but also feeding real-time insights and inspiration back to the campaignâ€™s creativeÂ leads.</li><li><strong>Prototyping at campaign speed</strong>: from valentineâ€™s day skits to a CTA in a box of chocolates, to working with rising and diverse artists like Aneesh Bhoopathy (whose branding drew from New Yorkâ€™s multicultural cityscapel), Wael Morcos of Morcos Key (who brought an Arabic inflection to the design for outreach in immigrant communities) and Rama Duwaji (Mamdaniâ€™s wife) a Syrian-American artist, all contributed creative input to the campaignâ€™s bold color palette and storytelling style. Together, these collaborators crafted a visual system that embodied the campaignâ€™s ethos: energetic, multicultural, and community-rooted.</li></ul><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/554/1*-cHRq1W7oDToc9LlXLl85g.png"" /></figure><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*qP_poKjabd5DdbvWcCKONg.png"" /></figure><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/538/1*n8JGlCWyY_n831nkV_arRQ.png"" /><figcaption>Campaign poster designs. Left to right: Arabic version (Wael Morcos/Morcos Key), Brooklyn/Staten Island/Manhattan/Queens/Bronx borough designs (Aneesh Bhoopathy), F<em>lower animation by Sarah Melawad. Source: </em><a href=""https://www.instagram.com/zohrankmamdani/"">https://www.instagram.com/zohrankmamdani/</a> &amp; <a href=""https://www.newsweek.com/inside-the-visual-strategy-that-won-nyc-for-zohran-mamdani-11023538"">https://www.newsweek.com/inside-the-visual-strategy-that-won-nyc-for-zohran-mamdani-11023538</a></figcaption></figure><p>Mamdaniâ€™s campaign proves that a small, nimble, empowered team listening, iterating, and acting in public, can beat even the biggest budget when it comes to building trust, resonance, and momentum.</p><blockquote><strong><em>Lesson for designers</em></strong><em>: Value nimbleness, rapid prototyping, and responsiveness over large, inflexible structures.</em></blockquote><blockquote><strong><em>However</em></strong><em>, While Mamdaniâ€™s agile appraoch proves effective we must acknowledge what </em><a href=""https://medium.com/@TarenSK/progressives-need-a-ux-design-revolution-195aa2c12894""><em>Taren Stinebrickner-Kauffman argues</em></a><em>, that â€œ</em>the non-profit campaigning world has overlooked the deep expertise available<em> i</em>n UX design&quot;<em>; in other words, the success of Mamdaniâ€™s campaign was possible because skilled workers chose to donate their expertise, a privilege not readily available to everyÂ team.</em></blockquote><h3>4. Social media engagement and messageÂ clarity</h3><p>If social media was meant for anything it was storytelling. This is exemplified in Mamdaniâ€™s social media campaigning, he didnâ€™t just tell a story, he listened to other peopleâ€™sÂ stories.</p><h4><strong>Near real-time responsiveness</strong></h4><p><a href=""https://www.instagram.com/zohrankmamdani/reel/DQrrenCDtrg/"">Zohran Kwame Mamdani on Instagram: &quot;Thank you, New York City. Last night we made history. Now we get to work. Comment MayorElect to learn about the transition. Comment DonateNow to make a contribution.&quot;</a></p><p><em>Alt text: Video of Mamdani announcing transition plans the day after election victory</em> <em>Video credit: Zohran Mamdani Instagram</em></p><p>Mamdaniâ€™s team developed a reputation for near real-time responsiveness, flooding platforms with multilingual and original content and directly engaging with voters, influencers and critics alike. When he won the mayoral race, influencers joked that Mamdani should â€œgo to bedâ€ as the very next day Mamdani posted a video announcing his plan for the transitional phase.</p><p>Digital experts credited his social media presence with fueling grassroots participation at levels rarely seen in local politics. Mamdani did not simply use social media he was intertwined in its roots through collaboration with influencers he knew would target his voters and his audience, thatâ€™s good user research: <em>know your audience</em>.</p><p>This engagement and real-time communication transcended boundaries and was embraced far beyond its original audience because it told a story and stories have noÂ borders.</p><blockquote><strong><em>Lessons for designers</em></strong><em>: Storytelling as a tool in UX is often overlooked and under-appreciated, yet it is at the center of good UX and UI. Tell aÂ story.</em></blockquote><blockquote><strong><em>However</em></strong><em>, how can we discuss social media political success without acknowledging that</em></blockquote><blockquote><em>the very techniques that made Mamdaniâ€™s campaign successful (rapid iteration, emotional storytelling, influencer partnerships) are the same ones </em><a href=""https://www.stimson.org/2022/social-media-misinformation-and-the-prevention-of-political-instability-and-mass-atrocities/""><em>identified by researchers</em></a><em> as enabling â€œorganized social media misinformation campaignsâ€ that threaten democracy globally.</em></blockquote><blockquote><em>How do we as designers harness the power of emotional connection and rapid iteration without contributing to the erosion of truth and democratic norms?</em></blockquote><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1008/1*VKRUtjgehtRth1msGLA3uA.png"" /></figure><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/984/1*-tz4fKlRT4hh13dnZ2z2gA.png"" /><figcaption>Two-panel screenshot of Instagram comments showing enthusiastic support for Mamdaniâ€™s campaign from users across the US and internationally, including Philadelphia, London, and Dublin, with comments praising the campaignâ€™s content strategy, authenticity, and comparing it to Obamaâ€™s 2008 campaign. Source: <a href=""https://www.instagram.com/zohrankmamdani/"">https://www.instagram.com/zohrankmamdani/</a></figcaption></figure><h3>5. Clarity as empowerment</h3><p>Other than the consistent and constant social media messaging and presence, the likes of which have never been seen in US political campaigning, Zohran Mamdaniâ€™s message was committed to clarity. On social media and in real life, whether he was explaining voting requirements, or his plans as mayor for the city, he was walking people step-by-step through the message and in multiple languages and settings thereby empowering first-time or previously disengaged voters.</p><blockquote><strong><em>Lessons for designers</em></strong><em>: Mamdaniâ€™s approach exemplified the core UX principle of affordance: making it immediately visible what action is possible and how to takeÂ it.</em></blockquote><blockquote><strong><em>However</em></strong><em>, the question then arises, </em>Why should anyone have to explain the ability and process of voting?<em> Doesnâ€™t this then allow those with the explanation the power to weaponize this knowledge? </em><a href=""https://ask.edgarallan.com/blog/a-ux-perspective-on-the-us-election""><em>Edgar Allanâ€™s UX team notes</em></a><em>, â€œIf there was ever a thing that should be unquestionably clear, understandable and accessible, it is the ability to vote. Unfortunately, the U.S. voting system suffers from a distinct lack of user-centricity.â€</em></blockquote><h3>6. Cultural pain points = empathyÂ mapping</h3><p>One of the first steps in the design thinking process is to empathize. Empathy was the main message throughout Mamdaniâ€™s campaign, it â€œgoes beyond what users explicitly state and manages to unearth hidden motivations, desires, and pain points.â€ (<a href=""https://www.interaction-design.org/literature/article/empathy-map-why-and-how-to-use-it"">Interaction Design Foundation</a>).</p><p><em>By focusing on lived experiences, cultural nuances and specific community needs his campaign resonated with voters in an unprecedented way.</em></p><p>In a historic move not seen before in American politics, Mamdani publicly asked his supporters to stop donating once the campaign reached its financial goals. He posted transparent breakdowns of fundraising progress and, upon reaching the spending cap, communicated directly:</p><p><a href=""https://www.instagram.com/zohrankmamdani/reel/DOOI8_Ijrm5/"">Zohran Kwame Mamdani on Instagram: &quot;I am once again asking you to stop sending us money. But we do need your time. Comment CanvassNow - one word, no spaces - to get a sign up link in your DMs.&quot;</a></p><p><em>Alt text: Video of Mamdani asking voters to stop donating money to his campaign. Video credit: Zohran Mamdani Instagram</em></p><blockquote>â€œWe have enough to win. Pleaseâ€¦do not give more. Redirect your generosity to mutual aid, to neighbors, to other grassroots causes.â€ (ZohranÂ Mamdani)</blockquote><h4><strong>Plain language, clearÂ needs</strong></h4><p>There were no hidden motives or complicated political lingo, just a plain-language presentation of what he needed from his supporters at each moment. Voting instructions were step-by-step. Donation limits were openly discussed. When the campaign reached its fundraising goal, supporters were told explicitly and publicly that there was no need for more underlying the empathetic line that his campaign had been running â€œFor a New York You CanÂ Affordâ€.</p><blockquote><strong><em>Lessons for designers</em></strong><em>: make empathy visible by listening, responding, and adapting your design to usersâ€™ actual lives and constraints.</em></blockquote><blockquote><strong><em>Here again we must consider the power of manipulation and exploitation.</em></strong><em> When genuine empathy certainly bridges gaps, but when performed it can manipulate vulnerable populations. Our responsibility, as designers is to ensure empathy serves usersâ€™ interests, not just campaign goals, to quote </em><a href=""https://uxpamagazine.org/political-design/""><em>Lloyd Hervey writes about Political Design</em></a><em>, designers must consider â€œwhat is possible and what is ethicalâ€¦the trade-offs of who will benefit from a particular solution versus who will be negatively affected.â€</em></blockquote><figure><img alt=""Instagram post by Zohran Mamdani promoting a â€œSavings Calculatorâ€ tool for New York City residents. The left side shows a bright blue and yellow screen asking, â€œAre you a rent-stabilized tenant?â€ with Yes/No options and a rent input field. On the right, the caption explains the calculator helps New Yorkers see how policies affect childcare, rent, and bus fare savings. Source: https://www.instagram.com/p/DPehuVCDji1/"" src=""https://cdn-images-1.medium.com/max/1024/1*UpQ25IdaLunE18QulkkNmg.png"" /><figcaption>Screenshot of the campaignâ€™s affordability agenda calculator, demonstrating the principle of UX affordance by making policy impact immediately tangible and personalized for NYC voters. Source: <a href=""https://www.instagram.com/p/DPehuVCDjt1/"">https://www.instagram.com/p/DPehuVCDjt1/</a></figcaption></figure><h3>7. Multilingual communication = inclusive design</h3><p>Some political opponents criticized Mamdani for â€œcode switchingâ€, shifting languages and communication styles to match his audience. But in UX terms, this is exactly right. Code switching isnâ€™t diluting your message; itâ€™s adapting it for different user contexts.</p><h4><strong>Authentic voices from communities</strong></h4><p><a href=""https://www.instagram.com/zohrankmamdani/reel/DQZrQz4jtFm/"">Zohran Kwame Mamdani on Instagram: &quot;Juntos vamos a construir la ciudad que merecemos! Y con tu ayuda, voy a seguir aprendiendo espaÃ±ol ğŸ˜‰ Â¡Puedes votar temprano hasta el Domingo y el dÃ­a de las elecciones es el Martes 4 de Noviembre! Â¡Comenta con la palabra VOTA y te enviaremos un enlace para ver dÃ³nde votar!&quot;</a></p><p><em>Alt text: Video of Mamdani speaking Spanish with a local Spanish speaking voter. Video credit: Zohran Mamdani Instagram</em></p><p><a href=""https://www.instagram.com/zohrankmamdani/reel/DQhue5vjlgj/"">Zohran Kwame Mamdani on Instagramâ€: &quot;â€Ø£Ù†Ø§ Ø§Ø³Ù…ÙŠ Ø²Ù‡Ø±Ø§Ù† Ù…Ù…Ø¯Ø§Ù† ÙˆØ¹Ù… Ø±Ø´Ù‘Ø­ Ø­Ø§Ù„ÙŠ Ù„Ø£ÙƒÙˆÙ† Ø§Ù„Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯ ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ù†ÙŠÙˆÙŠÙˆØ±Ùƒ&quot;â€</a></p><p><em>Alt text: Video of Mamdani speaking Arabic. Video credit: Zohran Mamdani Instagram</em></p><p>Mamdaniâ€™s socials are flooded with videos in Arabic, Urdu, Spanish, appealing to all the New York demographics and communities. He did so by enlisting the help of people from those communities, in other words he engaged with more diverse audiences thoughtfully and used locals to help make his message more authentic and relevant. This helped communities feel seen and heard and Mamdani was able to build trust with communities small andÂ large.</p><blockquote><strong><em>Lessons for designers</em></strong><em>: use language fluidity to engage broader, more diverse audiences thoughtfully and do not translate blindly, instead ellicit help from locals who speak the language. S</em></blockquote><blockquote><em>Such help from local communities should go beyond translation and requires including the diverse communioties in the design process itself and not just in the final messaging. Language fluidity is not automatically inclusive and as designers we must be weary of what researchers call â€œ</em>performative diversity<em>â€ which is looking inclusive without transferring actual power to marginalized communities</em></blockquote><h3>Conclusion</h3><h4>Hope through human-centered design</h4><p>Mamdaniâ€™s campaign gave people something special and new (perhaps also dangerous, it remains to be seen):Â Hope.</p><p>He did it by embodying core UX principles: <em>empathy</em>, <em>storytelling</em>, and <em>iteration</em>. His message was always empathetic, his statements clear and direct, and his â€œuser requirementsâ€ (what he needed from his supporters) consistently open and accessible. This was a campaign led by the people, for the people, but more importantly, Mamdani didnâ€™t position himself as someone observing New Yorkers from the outside. He is a New Yorker. He lived their stories alongside them, not aboveÂ them.</p><h4>Remembering the human element and our responsibility as designers.</h4><p>As designers, itâ€™s all too easy to get caught up in technical features or aesthetic appeal. While those things matter, the most meaningful work happens when we remember the element of humanity at the heart of every experience. As the UX world shifts rapidly, perhaps toward â€œZero UIâ€ where interfaces recede and experience comes to the foreground, we should focus on listening deeply, telling better stories, and designing platforms where usersâ€™ stories are welcomed, valued, and allowed to live andÂ breathe.</p><p>But we must also remember our ethical responsibility; feelings give way to manipulation and exploitation, and the same techniques that built hope in Mamdaniâ€™s campaign can and are also used to to spread misinformation, manipulate vulnerable populations, and erode democratic institutions. Our work as designers is never neutral, after all design for everyone is design for no one, and the question is not whether our design has impact, but whether that impact serves democratic values, human dignity, andÂ truth.</p><p>Mamdaniâ€™s campaign was a reminder that empathy, clarity, and authentic connection are what build hope and inspire change. But itâ€™s also a reminder that these tools are powerful, and that with great power comes great responsibility.</p><blockquote>The future of design in democracy depends on our willingness to hold two simultaneous truths: that connection matters deeply, and that not all connection is createdÂ equal.</blockquote><p>Every design decision is a choice. ChooseÂ wisely.</p><h3>References and Additional Readings</h3><h4><strong>Campaign Coverage:</strong></h4><ul><li>Wikipedia. <a href=""https://en.wikipedia.org/wiki/2025_New_York_City_mayoral_election"">â€œ2025 New York City mayoral election.â€</a></li><li>Adweek. <a href=""https://www.adweek.com/agencies/meet-melted-solids-the-scrappy-agency-behind-zohran-mamdanis-primary-winning-campaign/"">â€œMeet Melted Solids, the Scrappy Agency Behind Zohran Mamdaniâ€™s Primary-Winning Campaign.â€</a></li><li>New York Times. <a href=""https://www.nytimes.com/live/2025/11/05/nyregion/nyc-mayor-mamdani"">â€œMamdani, N.Y.C. Mayor-Elect, Names Transition Team of Government Veterans.â€</a> November 6,Â 2025.</li><li>CNN. <a href=""https://www.cnn.com/2025/11/04/politics/video/mamdani-nyc-mayor-democratic-socialist-vrtc"">â€œZohran Mamdani wins NYC mayorâ€™s race in historic upset.â€</a> November 5,Â 2025.</li><li>CBS News. <a href=""https://www.cbsnews.com/newyork/live-updates/nyc-election-results-2025/"">â€œZohran Mamdani claims victory in NYC mayorâ€™s race, promises â€˜relentless improvement.â€™â€</a> November 5,Â 2025.</li><li>New York City Campaign Finance Board. <a href=""https://www.nyccfb.info/vsapps/CandidateSummary.aspx?as_cand_id=2899&amp;as_election_cycle=2025&amp;cand_name=Mamdani"">â€œ2025 Campaign Finance Summary.â€</a></li><li>Mamdani, Z. K. [@zohrankmamdani]. (n.d.). <em>Instagram profile</em>. Instagram. Retrieved November 12, 2025, from <a href=""https://www.instagram.com/zohrankmamdani/"">https://www.instagram.com/zohrankmamdani/</a></li><li>Zohran for NYC. (n.d.). <em>Media kit</em>. Retrieved November 12, 2025, from <a href=""https://www.zohranfornyc.com/media-kit"">https://www.zohranfornyc.com/media-kit</a></li></ul><h4><strong>UX and Political Campaigns:</strong></h4><ul><li>Oâ€™Conner, Brad. <a href=""https://medium.com/design-bootcamp/user-experience-politics-how-ux-can-be-applied-to-political-campaigns-4efe25435846"">â€œUser experience &amp; politics: how UX can be applied to political campaigns.â€</a> Medium, JuneÂ 2021.</li><li>Key Lime Interactive. <a href=""https://info.keylimeinteractive.com/the-ux-of-politics"">â€œThe UX of Politics.â€</a></li><li>Edgar Allan. <a href=""https://ask.edgarallan.com/blog/a-ux-perspective-on-the-us-election"">â€œA UX Perspective on the US Election.â€</a></li><li>Stanford Social Innovation Review. <a href=""https://ssir.org/articles/entry/when_to_use_user_centered_design_for_public_policy"">â€œWhen to Use User-Centered Design for PublicÂ Policy.â€</a></li><li>Stinebrickner-Kauffman, Taren. <a href=""https://medium.com/@TarenSK/progressives-need-a-ux-design-revolution-195aa2c12894"">â€œProgressives need a UX design revolution.â€</a> Medium, NovemberÂ 2018.</li></ul><h4><strong>Design Thinking andÂ Empathy:</strong></h4><ul><li>Nielsen Norman Group. <a href=""https://www.nngroup.com/articles/empathy-mapping/"">â€œEmpathy Mapping: The First Step in Design Thinking.â€</a> JanuaryÂ 2024.</li><li>Interaction Design Foundation. <a href=""https://www.interaction-design.org/literature/article/empathy-map-why-and-how-to-use-it"">â€œEmpathy Mapâ€Šâ€”â€ŠWhy and How to UseÂ It.â€</a></li><li>QED42. <a href=""https://www.qed42.com/insights/how-empathy-works-in-design-thinking"">â€œThe role of empathy mapping in the process of design thinking.â€</a></li><li>Stolzoff, Alex. <a href=""https://medium.com/design-bootcamp/design-thinking-empathy-maps-journey-maps-and-how-they-are-interconnected-b145aafccdd1"">â€œDesign Thinking, Empathy Maps, Journey Maps, and how they are interconnected.â€</a> Medium, MarchÂ 2021.</li></ul><h4><strong>Agile Marketing and SmallÂ Teams:</strong></h4><ul><li>Atlassian. <a href=""https://www.atlassian.com/agile/agile-marketing/agile-marketing-team"">â€œHow to create an agile marketing team.â€</a></li><li>Teamhood. <a href=""https://teamhood.com/agile/what-is-agile-marketing/"">â€œWhat is Agile Marketing: Definition, Framework &amp; Examples.â€</a> JulyÂ 2025.</li></ul><h4><strong>Political Design andÂ Ethics:</strong></h4><ul><li>Hervey, Lloyd. <a href=""https://uxpamagazine.org/political-design/"">â€œPolitical Design.â€</a> UXP Magazine, JuneÂ 2025.</li></ul><h4><strong>Social Media Manipulation and Criticism:</strong></h4><ul><li>University of Oxford. <a href=""https://www.ox.ac.uk/news/2021-01-13-social-media-manipulation-political-actors-industrial-scale-problem-oxford-report"">â€œSocial media manipulation by political actors an industrial scale problem.â€</a> JanuaryÂ 2021.</li><li>Stimson Center. <a href=""https://www.stimson.org/2022/social-media-misinformation-and-the-prevention-of-political-instability-and-mass-atrocities/"">â€œSocial Media Misinformation and the Prevention of Political Instability and Mass Atrocities.â€</a> NovemberÂ 2022.</li><li>Yerlikaya, Turgay, and Seca Toker Aslan. <a href=""https://www.insightturkey.com/articles/social-media-and-fake-news-in-the-post-truth-era-the-manipulation-of-politics-in-the-election-process"">â€œSocial Media and Fake News in the Post-Truth Era: The Manipulation of Politics in the Election Process.â€</a> Insight Turkey, JulyÂ 2020.</li><li>Guess, Andrew, et al. <a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC6436681/"">â€œSocial mediaâ€™s contribution to political misperceptions in U.S. Presidential elections.â€</a> PLOSÂ ONE.</li><li>Bonnema, Simone. <a href=""https://www.sciencedirect.com/science/article/pii/S0377221720308249"">â€œThe responsibility of social media in times of societal and political manipulation.â€</a> European Journal of Operational Research, September 2020.</li></ul><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=11f92007e873"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/what-designers-can-learn-from-zohran-mamdanis-historical-campaign-11f92007e873"">What designers can learn from Zohran Mamdani's historical campaign</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/the-evolution-of-youtube-how-every-platform-evolves-into-an-ad-machine-9e6032ff2d20?source=rss----138adf9c44c---4,1762949937,The evolution of Youtube: How every platform evolves into an ad machine,"The evolution of Youtube: How every platform evolves into an ad machine

<h4>A guide to the extinction of user joy in the digitalÂ age.</h4><figure><img alt=""Image of a crab over a phone, symbolizing the evolution of apps into ad machines"" src=""https://cdn-images-1.medium.com/max/1024/1*Yarbt8n6ZoCN7HwRnRAJHg.png"" /><figcaption>An crap shaped ad, emerging from a cellphone.</figcaption></figure><p>I was halfway through doing the dishes while listening to a podcast when an ad cut through and interrupted one of the most interesting bits. My phone was a few meters away in the living room and I rushed over like I had forgotten a burning turkey in the oven to make sure that I got to the phone before the second ad started playing. I tapped that lower-right hand skip button and the tension in my body temporarily melted away as I made my way back to the chore and continued listening to my podcast, ironically about the slow decay of once useful apps, that was so rudely interrupted by a NissanÂ ad.</p><figure><img alt=""Popup for YouTube Premium offering ad free experience"" src=""https://cdn-images-1.medium.com/max/661/0*qVaAxIWg4nrXMgaT.jpg"" /><figcaption><a href=""https://strangershow.com/how-to-buy-youtube-premium/"">Popup for YouTube Premium offering ad free experience</a></figcaption></figure><p>As I pondered how possible it would be to find an ad blocker for my phone, the answer was slapped in my face in the form of a glorious popup! <strong>Try YouTube premium and youâ€™ll be ad free</strong>â€¦ something that only a few years ago was standard had become a premium and I asked myself why? The term I happened to be learning about that day from <a href=""https://doctorow.medium.com/https-pluralistic-net-2024-04-04-teach-me-how-to-shruggie-kagi-caaa88c221f2"">Cory Doctorow was <strong>enshittification</strong></a>, which can be loosely defined as the process by which platforms decay as they shift value away from users and toward themselves and their business customers. In other words, things start out good for us, then get worse as the companies optimize for profit. While we all have a collective <a href=""https://www.theatlantic.com/technology/archive/2014/08/advertising-is-the-internets-original-sin/376041/"">intuitive sense of platforms decaying</a> over the last few years, I wanted to look closer from a design perspective, specifically, at how that decay unfolds through the deliberate, incremental rollout ofÂ ads.</p><p>The key players in this <strong>enshittification</strong> race are the four major tech companies that dominate our digital lives: Google, Meta (Facebook and Instagram), TikTok, and YouTube. But for the sake of this article, Iâ€™m going to focus on one in particular, because itâ€™s where I spend the most time:Â YouTube.</p><p>These companiesâ€™ slow decay into ad infested dystopian nightmares reminded me of a conversation I had with my good friend and cofounder <a href=""https://charliegedeon.medium.com/"">Charlie Gedeon</a>. He told me about this theory, called <a href=""https://en.wikipedia.org/wiki/Carcinisation""><strong>carcinisation</strong></a>, which is the very hilarious scientific process which can be described as â€œ<a href=""https://brill.com/view/journals/ctoz/67/2/article-p79_1.xml"">the many attempts of Nature to evolve [into] a crab</a>â€. In simple terms, there is an evolutionary momentum that has slowly pushed a variety of unrelated species to evolve into the general behaviour pattern and shape of a crab. So given enough time and opportunity, every species will eventually become a crab. I couldnâ€™t ignore the metaphorical parallel: in our technological ecosystem, the biggest species donâ€™t turn into crabs, they turn intoÂ ads.</p><p>Before YouTube, there was TV. The TV was invented in 1939, and the <a href=""https://www.madhive.com/insights/history-of-tv-advertising"">first ads were shown in 1941</a>. In the early 1940s, ads were short, mild interruptions between programs then by the 1980s, they had become pretty much the reason every sitcom, news broadcast, and sports event existed at all. It was simply the packaging for the ad breaks that paid for it. Then streaming arrived and it felt like a breath of fresh air! A new, ad-free subspecies breaking free from its commercial ancestors. Netflix, YouTube, and early Hulu gave us hope of a new evolutionary path: content without interruption. But, it didnâ€™t stay ad free forÂ long.</p><p>So letâ€™s look at YouTube, starting with the first signs of life to crawl out of the primordial soup, we have the humble <a href=""https://www.mediapost.com/publications/article/66159/"">InVideo overlays first launched in 2007</a>, these were thin, semi-transparent banners that floated over the bottom of a playing video, usually small text or a tiny ad image that you could close with a small â€œX.â€ These early ads didnâ€™t stop the video, or call for too much attention. They quietly fed on our attention without killing us, like a parasitic symbiosis. While annoying, they didnâ€™t destabilize the delicate environment of early to mid-2000â€™s YouTube.</p><figure><img alt=""The early dismissible youtube ad banner showing at the bottom 20% of the screen"" src=""https://cdn-images-1.medium.com/max/1024/0*QMeV9fQYXmh-XYNo.png"" /><figcaption><a href=""https://setupad.com/blog/youtube-ad-formats-guide/"">Early dismissible youtube ad banner taking up the bottom 20% of aÂ video.</a></figcaption></figure><p>Then between 2010 and 2018 thereâ€™s an explosion of life, the ads grow legs, lungs, and teethâ€¦ YouTube launched <a href=""https://blog.youtube/news-and-events/trueview-video-ads-give-viewers-choice/"">TrueView ads</a>, those aforementioned and all too familiar skippable pre-rolls that appear before your chosen video. These ads demand visibility, they evolved to study their preyâ€™s habits, tracking which openings held attention longest. At first, there was just one, small but intrusive, a 5-second interaction before you could skip and move on. But as we all know, evolution rewards persistence. Soon, a single ad became two, then a pair before and after your video, and finally in 2012, <a href=""https://blog.youtube/creator-and-artist-stories/mid-roll-ads-updates-explained"">mid-roll ads appeared</a>, splitting your entertainment in bite sizeÂ chunks.</p><figure><img alt=""Showing the skipable after X number of seconds ad at the beginning of a video"" src=""https://cdn-images-1.medium.com/max/1024/0*ZSukJyjEmxwlLrTt.jpg"" /><figcaption><a href=""https://setupad.com/blog/youtube-ad-formats-guide/"">Showing the skipable after X number of seconds ad at the beginning of aÂ video</a></figcaption></figure><p>With each mutation, they became harder and harder to ignore. The variety was overwhelming, displaying banners above, overlays below, and videos within videos. The content creators themselves now, mimicking the copy of the ads. By the end of this period, the once teeming with life content creator ecosystem had turned into a chewed through rotting swamp where every spare patch of attention was colonized with a swarm ofÂ ads.</p><p>Understandably, this environment was not the most pleasant so I, like many others, used an ad-block. To put in perspective how prevalent this problem is, hereâ€™s a quote from Cory Doctorow that really struck me: <a href=""https://youtu.be/P1EKQidRooc?si=W5D9-f7NSy8VVT3I&amp;t=1372"">â€œ51% of web users have installed an ad blocker, it is literally the largest consumer boycott in human history.â€</a> The majority of people, which I imagine probably encompasses people making these decisions in the first place, probably have an ad blocker installed. This was our one tool to try and avoid the onslaught of ads that weâ€™d be subjected to on a daily basis, and while there werenâ€™t as easily accessible tools on mobile, weâ€™d at least have a reprieve on our computersâ€¦ or so weÂ thought.</p><figure><img alt=""YouTube rollout of the ad blocker warning popups, showing that Ad Blockers arenâ€™t allowed on the platform."" src=""https://cdn-images-1.medium.com/max/690/0*zH9j2L6vBUyKB43D.jpeg"" /><figcaption><a href=""https://community.brave.app/t/youtube-showing-popup-for-ad-blockers/511149"">YouTube rollout of the ad blocker warningÂ popups</a></figcaption></figure><p>In late <a href=""https://www.androidauthority.com/youtube-ad-blocker-crackdown-growing-3380809"">May of 2023</a>, YouTube began to mutate again, starting with a small subset of tests where they rolled out banners saying â€œAd Blockers are not allowed on YouTube.â€ An easily dismissable popup asking you to uninstall your Ad Blocker. It was the early signs of a dominant species that was going to make it impossible for you to avoid ads. Eventually, making it an unfeasible arms race between ad-blockers and ad-blocker-blockers (thatâ€™s a mouthful). This persisted to where weâ€™re at now where itâ€™s more of a pain to find a way to circumvent ads then to just sit through the discomfort that weâ€™ve been conditioned toÂ endure.</p><p>This whole process has been a calculated and heavily studied rollout. In some ways it is a masterful example of how to slowly introduce a feature to users that doesnâ€™t actually benefit the majority. At first users feel discomfort, but not enough to abandon the platform. The designers, researchers and tech CEOâ€™s know that theyâ€™re pushing unpleasantness and that they have to go as far as they can without causing too much pain because they understand that itâ€™s a delicate ecosystem of attention and people will find alternatives if pushed too far too quickly. If YouTube had not given notice of its intention to block ads for months prior to the rollout of the full ad-block extinction, what would have happened? I personally think the consequences would have been cataclysmic and in some ways hoped that they would have torn off the bandaid a little more violently to shock us intoÂ action.</p><p>So over the course of 25 years, theyâ€™ve increased the heat and now weâ€™re in a pot of boiling water, but in truth itâ€™s we actually donâ€™t know how much hotter it might get. Many <strong>PAID</strong> platforms now include ads, such as Prime and you have to pay additional fees to exclude them. So what comes next in this grand evolutionary chain? Maybe ads that blend seamlessly into the content, AI-generated sponsorships that mimic the creatorâ€™s voice so perfectly we canâ€™t tell where the video ends and the commercial begins. Maybe theyâ€™ll feel like features instead of intrusions. Either way, the species isnâ€™t going extinct anytime soon. Like crabs, like capitalism, ads always find aÂ way.</p><p>The hope is for the next disruption. YouTube once broke the cable monopoly with creativity, and community before slowly mutating into the very thing it replaced. It would seem, thatâ€™s the pattern of evolution, the old ecosystem collapses under its own weight, and something new crawls out of the primordial muck.</p><p>One can hope that there is a better answer gestating. Maybe itâ€™s decentralized video, maybe itâ€™s a new model of sharing that doesnâ€™t depend on monetized attention, more than likely itâ€™s something we havenâ€™t imagined yet. The godfather of the term, <a href=""https://doctorow.medium.com/"">Cory Doctorow</a>, explores this in his new book <a href=""https://www.goodreads.com/book/show/222615930-enshittification""><em>Enshittification: Why Everything Suddenly Got Worse and What to Do About It</em>Â </a>, with the most important part being that final clause: <strong>what to do aboutÂ it.</strong></p><p>As citizens, we can advocate for antitrust laws, interoperability, and policies that prevent platforms from locking in both users and creators. As users, we can choose to support the smaller, values-driven platforms that still have a soul, ones that remind us the internet doesnâ€™t have to be stinky pile of ads, but a place we actively connect. And lastly as designers, we can embed the values of portability, transparency, user control, and minimal hidden value extraction. Better than hoping for the next better thing is putting our efforts into actually designing it.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9e6032ff2d20"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/the-evolution-of-youtube-how-every-platform-evolves-into-an-ad-machine-9e6032ff2d20"">The evolution of Youtube: How every platform evolves into an ad machine</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/guiding-the-future-of-ethical-design-796e7cc3c9b1?source=rss----138adf9c44c---4,1762902322,Guiding the future of ethical design,"Guiding the future of ethical design

<h4>How to translate philosophical theory into practical design principles and responsibility.</h4><figure><img alt=""Colorful circular diagram titled â€œEthical Interface Designâ€ with sections: Inclusion, Privacy, Autonomy, Transparency, Well-Being, each linked to ethical theories."" src=""https://cdn-images-1.medium.com/max/1024/1*gm8743DS4GxpcGXJdywN2g.jpeg"" /></figure><p>The following article introduces an evolving academic framework called <em>Ethical Interface Design</em>, which examines how moral philosophy can guide interface design in the era of emerging technologies. The working site, <a href=""https://ethicalinterface.com""><strong>ethicalinterface.com</strong></a>, presents these ideas through a minimalist, text-focused layout that intentionally prioritizes thought over visualsâ€Šâ€”â€Ša meta-commentary on where design itself may be heading. The framework remains in active development, and thoughtful feedback isÂ welcome.</p><p>Society is moving beyond screens into conversational, immersive, and neural experiences, placing designers at the center of the ethical landscape of humanâ€“technology interaction. <a href=""https://ethicalinterface.com/""><em>Ethical Interface Design</em></a> helps designers understand not only the impact of their choices but also the ethical frameworks behind them, exploring:</p><ul><li>How interfaces influence behavior across visual, tactile, conversational, neural, and mixed-reality modalities</li><li>The philosophical roots of <em>Ethical Interface Design</em>â€Šâ€”â€Šinclusion, autonomy, privacy, transparency, and well-being</li><li>The trade-offs between competing moral frameworksâ€Šâ€”â€Šsuch as egalitarianism versus meritocracy, or collectivism versus libertarianismâ€Šâ€”â€Šthat determine how ethical priorities are balanced inÂ design</li><li>Practical guidance for applying ethics to real-world projects</li></ul><p><em>Ethical Interface Design</em> equips designers to make intentional, responsible choices as they shape the digital worldÂ ahead.</p><h3>Interface Modalities</h3><p>Every interface mediates the relationship between humans and machinesâ€Šâ€”â€Štranslating intention into action, and in doing so, shaping how we perceive, decide, and connect. Emerging technologies such as <a href=""https://www.ibm.com/think/topics/large-language-models"">large language models</a> and <a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC6914248/"">neural chips</a> are expanding that relationship, blurring the boundary between user and system. As interfaces grow more adaptive and conversational, the very definition of â€œuser interface designâ€ begins to dissolveâ€Šâ€”â€Šreplaced by fluid exchanges that merge cognition, language, and computation.</p><p>This evolution raises profound questions about agency, inclusion, and moral responsibilityâ€Šâ€”â€Šquestions explored further in the Ethical Foundations and <em>The Five Pillars of Ethical Interface Design</em> section of thisÂ article.</p><h4>From Command to Conversation</h4><p>Early computing relied on text-based <a href=""https://www.w3schools.com/whatis/whatis_cli.asp"">command-line interfaces</a>â€Šâ€”â€Šprecise but exclusive, accessible only to those fluent in code. The 1980s introduced the <a href=""https://www.interaction-design.org/literature/topics/graphical-user-interfaces?srsltid=AfmBOorkkBsRT2lPaE0zQed3OJt5HT3hCRkelHQF4-T-KQ_riqUiSau2"">Graphical User Interface (GUI)</a>, popularized by Xerox PARC and Apple, replacing syntax with icons and windows. This visual paradigm made computing public but also aestheticized controlâ€Šâ€”â€Šwhat you could see, you couldÂ do.</p><p>As technology spread beyond the desktop, new modalities emerged. Touchscreens redefined tactility. Voice assistants reintroduced conversation. Gesture and neural systems now blur the line between intention and execution. Each step expanded access while also raising new ethical questions about consent, surveillance, and cognitive autonomy.</p><h4>Types of Interfaces</h4><p>Interfaces are the boundaries between human intention and digital response. They manifest through the senses and combinations of themâ€Šâ€”â€Šand increasingly through language itself in conversational systems.</p><ul><li><a href=""https://www.nngroup.com/articles/visual-design-in-ux-study-guide/""><strong>Visual Interfaces</strong></a> â†’ Screens, icons, and layouts that communicate through sight. Examples include smartphone apps, operating systems, dashboards, and AI applications that blend visual and conversational interaction.</li><li><a href=""https://www.sciencedirect.com/article/pii/S2666998624001728""><strong>Tactile Interfaces</strong></a> â†’ Touchscreens, trackpads, haptic vibrations, and adaptive textures that make interaction physical. The iPhoneâ€™s multitouch screen (2007) made touch the dominant input for a generation.</li><li><a href=""https://dl.acm.org/doi/10.1016/j.cose.2023.103448""><strong>Voice Interfaces</strong></a> â†’ Spoken interaction with assistants like Alexa, Siri, and Google Assistant. Generative AI extends this to conversational agents voice mode, emphasizing tone, intent, and context over fixed commands.</li><li><a href=""https://www.hci.org.uk/article/exploring-the-role-of-gestural-interaction-in-user-interface-design-challenges-and-opportunities/""><strong>Gesture Interfaces</strong></a> â†’ Motion-based control through cameras and sensors, from the Nintendo Wii (2006) and Microsoft Kinect (2010) to AR/VR hand tracking. They extend interface design into physical space and embodiment.</li><li><a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC6914248/""><strong>Neural Interfaces</strong></a> â†’ Brainâ€“computer links such as Neuralink or medical prosthetics that translate thought into action. These collapse the boundary between user and system, making ethics inseparable from cognition itself.</li><li><a href=""https://www.mdpi.com/2079-9292/13/3/600""><strong>Mixed Reality Interfaces</strong></a> â†’ Hybrid environments such as virtual and augmented reality that merge visual, tactile, and spatial interaction. Examples include Meta Quest, Apple Vision Pro, and AI-powered immersive designÂ spaces.</li></ul><p>Every modality encodes a worldviewâ€Šâ€”â€Šabout who controls interaction, what is considered intuitive, and where agency resides. <em>Ethical Interface Design</em> asks not only how interfaces function, but what kind of human theyÂ imagine.</p><h3>Ethical Foundations</h3><p>Ethics has always asked how we ought to act. From <a href=""https://plato.stanford.edu/entries/aristotle-ethics/"">Aristotleâ€™s pursuit of virtue</a> to <a href=""https://plato.stanford.edu/entries/kant-moral/"">Kantâ€™s universal duty</a> and <a href=""https://plato.stanford.edu/entries/utilitarianism-history/"">Millâ€™s utilitarian calculus</a>, moral philosophy has sought to balance intention, outcome, and character. In design, these same questions take form in pixels, code, and policies. Every layout, algorithm, and feedback loop implies an answer to an ethical questionâ€Šâ€”â€Šsometimes implicitly, sometimes withÂ intent.</p><p>Yet the foundations of ethics are not absolute. Thinkers like <a href=""https://plato.stanford.edu/entries/nietzsche-moral-political/"">Nietzsche</a>, <a href=""https://plato.stanford.edu/entries/foucault/"">Foucault</a>, and <a href=""https://plato.stanford.edu/entries/derrida/"">Derrida</a> challenged the idea of universal morality, arguing that ethical systems are shaped by culture, power, and interpretation. From this view, ethics is not a fixed code but a living discourseâ€Šâ€”â€Šone that evolves with our technologies and values. What counts as â€œgood designâ€ may therefore reflect not timeless truth, but the shifting moral architectures of anÂ age.</p><h4>Philosophical Frameworks</h4><p><em>Ethical Interface Design</em> draws from a range of philosophical frameworks that form the foundation of <a href=""https://ethicalinterface.com/pillars.html""><em>The Five Pillars of Ethical Interface Design</em></a>. While ethics is contextual, these pillars rest on moral principles widely recognized across contemporary society.</p><p>These principles act as guiding coordinates rather than fixed rules, reflecting a shared yet evolving moral landscape. The <a href=""https://ethicalinterface.com/pillars.html"">Five Pillars</a> reveal how ethical tensions emerge when values intersectâ€Šâ€”â€Šthe goal is not to eliminate these contradictions but to make them visible, allowing designers to act with clarity andÂ intent.</p><p>History shows how one value can override anotherâ€Šâ€”â€Šprivacy may yield to collective well-being during crises, or inclusion may challenge merit-based systems. <em>Ethical Interface Design</em> highlights these trade-offs, guiding designers toward more transparent and deliberate choices.</p><h4>Core Ethical and Philosophical Principles</h4><p>The following list outlines major philosophical frameworks relevant to ethical design. While many other schools of thought exist, these principles form the foundational basis for <a href=""https://ethicalinterface.com/pillars.html""><em>The Five Pillars of Ethical Interface Design</em></a>.</p><ul><li><a href=""https://www.britannica.com/topic/collectivism""><strong>Collectivism</strong></a> â†’ A moral and political philosophy that prioritizes the needs, goals, and well-being of the group over individual interests. It emphasizes cooperation, shared responsibility, and social interdependence as essential to human flourishing.</li><li><a href=""https://plato.stanford.edu/entries/communitarianism/""><strong>Communitarianism</strong></a> â†’ A perspective emphasizing community, shared values, and social responsibility in shaping moral and political life. It critiques excessive individualism in liberalÂ thought.</li><li><a href=""https://plato.stanford.edu/entries/egalitarianism/""><strong>Egalitarianism</strong></a> â†’ The view that all individuals deserve equal moral consideration and that social and economic inequalities require justification. It emphasizes fairness, equal opportunity, and the reduction of arbitrary privilege.</li><li><a href=""https://plato.stanford.edu/entries/kant-moral/""><strong>Kantian Ethics</strong></a> â†’ A deontological moral theory grounded in reason and autonomy. It holds that moral action arises from adherence to universal moral laws derived from rationalÂ duty.</li><li><a href=""https://plato.stanford.edu/entries/liberalism/""><strong>Liberalism</strong></a> â†’ A political and moral philosophy that upholds individual rights, freedom of choice, and consent as the basis of legitimacy. It limits interference by the state or others in personalÂ domains.</li><li><a href=""https://plato.stanford.edu/entries/libertarianism/""><strong>Libertarianism</strong></a> â†’ A philosophy centered on individual liberty and minimal external control. It values personal autonomy, voluntary exchange, and limited government intervention, often resisting collective mandates or welfare-oriented design.</li><li><a href=""https://plato.stanford.edu/entries/meritocracy/""><strong>Meritocracy</strong></a> â†’ The belief that rewards and positions should reflect individual talent, effort, and achievement rather than social status or inherited privilege. It values ability and contribution as the basis for distribution.</li><li><a href=""https://plato.stanford.edu/entries/pragmatism/""><strong>Pragmatism</strong></a> â†’ A philosophical movement that evaluates ideas and actions by their practical effects. It privileges usefulness and results over strict adherence to principles or absoluteÂ truth.</li><li><a href=""https://plato.stanford.edu/entries/utilitarianism/""><strong>Utilitarianism</strong></a> â†’ A consequentialist theory asserting that moral value depends on outcomes, with the best action being the one that maximizes overall happiness or minimizes suffering for the greatestÂ number.</li><li><a href=""https://plato.stanford.edu/entries/ethics-virtue/""><strong>Virtue Ethics</strong></a> â†’ A moral framework emphasizing the cultivation of moral character and the pursuit of virtue. It focuses on traits such as honesty, integrity, and wisdom as the foundation of ethical behavior.</li></ul><h4>Toward an Ethical Interface</h4><p>As we move into an era defined by artificial intelligence, automation, and neural integration, ethics becomes both more complex and more urgent. Interfaces now operate at cognitive and emotional levels once considered private. The goal of <em>Ethical Interface Design</em> is to preserve human dignity within this expanding field of influence.</p><h3>The Five Pillars of <em>Ethical Interface Design</em></h3><p><a href=""https://ethicalinterface.com/pillars.html""><em>The Five Pillars of Ethical Interface Design</em></a> are built on established <a href=""https://ethicalinterface.com/ethics.html"">ethical principles</a> and common and emerging <a href=""https://ethicalinterface.com/modalities.html"">interface modalities</a>. Together, they reveal how moral values manifest across different modes of interactionâ€Šâ€”â€Šand how every ethical choice involves trade-offs that designers must navigate responsibly.</p><p>To learn more, review the principles in detail below. For a concise overview, refer to the <a href=""https://ethicalinterface.com/img/five-pillars-ethical-design-2025.pdf"">summaryÂ table</a>.</p><h4>Inclusion</h4><p>Inclusion ensures that interfaces welcome, represent, and empower people across abilities, cultures, and contexts. It rejects design that privileges one type of user at the expense of others and treats accessibility not as accommodation but as design integrity. An inclusive interface assumes difference as a constant, not an exception.</p><p><strong>Examples Across Modalities</strong></p><ul><li><strong>Visual Interfaces</strong> â†’ Support diverse visual and linguistic literacies through adjustable type scales, high-contrast modes, and culturally inclusive iconography.</li><li><strong>Tactile Interfaces</strong> â†’ Design haptic feedback and physical controls that accommodate varied dexterity and sensory abilities, ensuring equitable interaction.</li><li><strong>Voice Interfaces</strong> â†’ Train recognition systems on global accents, dialects, and speech patterns to prevent bias toward dominant languages.</li><li><strong>Gesture Interfaces</strong> â†’ Calibrate motion detection for diverse ranges of movement, body types, and physical capabilities instead of a single â€œidealâ€ gestureÂ model.</li><li><strong>Neural Interfaces</strong> â†’ Build adaptive systems that account for neurodiversity, cognitive variation, and comfort levels in signal interpretation.</li><li><strong>Mixed Reality Interfaces</strong> â†’ Configure AR and VR environments to support varied sensory sensitivities, spatial perception, and physical comfort acrossÂ users.</li></ul><blockquote><strong>Underlying Philosophy</strong></blockquote><blockquote>Rooted in <a href=""https://plato.stanford.edu/entries/egalitarianism/"">Egalitarianism</a>. The core claim is moral equalityâ€Šâ€”â€Šlike cases should be treated alike unless a relevant difference justifies unequal treatment. In design terms, capability differences, cultural background, and context are relevant inputs to equalize effective opportunity, not reasons toÂ exclude.</blockquote><blockquote><strong>Contrasting Philosophy</strong></blockquote><blockquote><a href=""https://plato.stanford.edu/entries/meritocracy/"">Meritocracy</a> links reward to demonstrated talent, effort, or productivity, appealing to fairness through performance. In design, this logic supports optimizing for â€œpower usersâ€ or â€œhigh-value segmentsâ€ to maximize efficiency andÂ impact.</blockquote><p><strong>Ethical Tension: Equality vs.Â Merit</strong></p><p><strong>Design Focus</strong> â†’ Design for accessibility and difference as defaults while recognizing merit, balancing equality with performance.</p><ul><li>Adopt accessibility guidelines (<a href=""https://www.w3.org/WAI/standards-guidelines/wcag/"">WCAG</a>, <a href=""https://www.w3.org/WAI/standards-guidelines/aria/"">WAI-ARIA</a>) as creative constraints.</li><li>Use participatory methods to include marginalized users in testing and feedback.</li><li>Offer personalizationâ€Šâ€”â€Šcontrast, input method, motion sensitivity, voice settingsâ€Šâ€”â€ŠwithoutÂ stigma.</li><li>Audit datasets, prompts, and imagery for cultural or algorithmic bias.</li><li>Frame inclusivity as an innovation driver that benefits allÂ users.</li><li>Provide â€œpower featuresâ€ as additive layers, not the defaultÂ path.</li><li>Evaluate efficiency metrics for the whole user base, not just the fastest quartile.</li></ul><h4>Autonomy</h4><p>Autonomy protects the userâ€™s capacity to think, choose, and act without manipulation or coercion. In interface design, it means preserving agencyâ€Šâ€”â€Šgiving users real control over what they see, share, andÂ decide.</p><p><strong>Examples Across Modalities</strong></p><ul><li><strong>Visual Interfaces</strong> â†’ Empower users to manage visibility and consent through clear privacy controls, opt-in dialogs, and transparent data-use indicators.</li><li><strong>Tactile Interfaces</strong> â†’ Provide physical controls, such as undo buttons and emergency stops, that let users reverse or interrupt actions on their ownÂ terms.</li><li><strong>Voice Interfaces</strong> â†’ Require explicit verbal control over actions, ensuring conscious intent over automated interpretation.</li><li><strong>Gesture Interfaces</strong> â†’ Design gestures that distinguish deliberate intent from incidental movement, preventing unintended triggers.</li><li><strong>Neural Interfaces</strong> â†’ Secure informed consent and allow opt-out before interpreting or predicting user intent from neuralÂ signals.</li><li><strong>Mixed Reality Interfaces</strong> â†’ Give users control over spatial permissions, tracking boundaries, and data sharing within AR and VR environments.</li></ul><blockquote><strong>Underlying Philosophy</strong></blockquote><blockquote>Grounded in <a href=""https://plato.stanford.edu/entries/kant-moral/"">Kantian Ethics</a>. People must be treated as ends in themselves, not as means to collective goals. Ethical action respects rational self-rule, even when doing so conflicts with collective well-being. Favor transparent choices, meaningful consent, and reversibility over efficiency or massÂ benefit.</blockquote><blockquote><strong>Contrasting Philosophy</strong></blockquote><blockquote><a href=""https://plato.stanford.edu/entries/utilitarianism/"">Utilitarianism</a> values the greatest good for the greatest number. This approach may favor streamlined decisions, the removal of risky options, or persuasive defaults to maximize collective well-being and system efficiency.</blockquote><p><strong>Ethical Tension: Freedom vs.Â Welfare</strong></p><p><strong>Design Focus</strong> â†’ Preserve informed choice while allowing limited, transparent guidance that prevents harm without coercion.</p><ul><li>Treat informed consent as ongoing and contextual.</li><li>Provide granular settings for automation and dataÂ sharing.</li><li>Default to privacy-preserving choices.</li><li>State clearly where automation begins andÂ ends.</li><li>Eliminate dark patterns and manipulative urgencyÂ cues.</li><li>When nudging for welfare, disclose the nudge, show the rationale, and offer a one-tapÂ opt-out.</li><li>Provide escape hatches (undo, exit, manual override).</li></ul><h4>Transparency</h4><p>Transparency concerns the integrity of information shared between humans and systems. It requires honesty, disclosure, and authenticity so that what is shown aligns with what is real. It provides the truthful foundation necessary for understanding, accountability, and rationalÂ choice.</p><p><strong>Examples Across Modalities</strong></p><ul><li><strong>Visual Interfaces</strong> â†’ Present data truthfully through accurate charts, clear sponsorship labels, and visible indicators of AI-generated or manipulated media.</li><li><strong>Tactile Interfaces</strong> â†’ Provide feedback that reflects the true system state, distinguishing between successful actions, errors, and pending responses.</li><li><strong>Voice Interfaces</strong> â†’ Disclose when users are interacting with non-human agents and summarize what information is recorded or retained.</li><li><strong>Gesture Interfaces</strong> â†’ Notify users when motion data is being tracked, interpreted, or stored to maintain awareness of system observation.</li><li><strong>Neural Interfaces</strong> â†’ Distinguish which neural signals can be measured versus which should be, clarifying how thought-related data is interpreted andÂ used.</li><li><strong>Mixed Reality Interfaces</strong> â†’ Display clear boundaries between simulated and real elements, including visibility into what is recorded, shared, or AI-generated within AR/VR environments.</li></ul><blockquote><strong>Underlying Philosophy</strong></blockquote><blockquote>Draws on <a href=""https://plato.stanford.edu/entries/ethics-virtue/"">Virtue Ethics</a>. Honesty and truthfulness appear as reliable presentation, avoidance of deception, and willingness to disclose limits. Interfaces operationalize these traits via faithful visualizations, provenance signals, and candid explanation of uncertainty.</blockquote><blockquote><strong>Contrasting Philosophy</strong></blockquote><blockquote><a href=""https://plato.stanford.edu/entries/pragmatism/"">Pragmatism</a> values what works over what is strictly true. It treats clarity and usefulness as higher goods than full disclosure, accepting that some truths may be simplified or deferred when they hinder progress.</blockquote><p><strong>Ethical Tension: Truth vs.Â Utility</strong></p><p><strong>Design Focus</strong> â†’ Reveal truth and uncertainty clearly and in context, maintaining usability andÂ trust.</p><ul><li>Disclose algorithmic involvement in rankings andÂ outputs.</li><li>Make consent and data flows comprehensible inÂ context.</li><li>Expose uncertainty and known limitations.</li><li>Align copy and feedback with actual system behavior.</li><li>Design clarity as a user benefit, not just compliance.</li><li>Use layered explanations: short, then expandable detailâ€Šâ€”â€Šnever hide conflicts of interest.</li><li>Avoid â€œspinâ€â€Šâ€”â€Šsimplify presentation without sacrificing truth.</li></ul><h4>Privacy</h4><p>Privacy safeguards the boundary between the individual and the system. It upholds user control over information, attention, and identityâ€Šâ€”â€Šdefining the conditions under which data is observed, shared, orÂ stored.</p><p><strong>Examples Across Modalities</strong></p><ul><li><strong>Visual Interfaces</strong> â†’ Provide clear permission settings, visible tracking indicators, and accessible private modes that let users manage visibility and data exposure.</li><li><strong>Tactile Interfaces</strong> â†’ Incorporate hardware shutters, physical switches, and tactile indicators to give users direct control over sensors and device activity.</li><li><strong>Voice Interfaces</strong> â†’ Include audible or visual cues when recording begins, and enable quick, in-flow deletion or redaction of capturedÂ speech.</li><li><strong>Gesture Interfaces</strong> â†’ Capture movement data only upon explicit initiation, ensuring gestures are recorded intentionally rather than passively monitored.</li><li><strong>Neural Interfaces</strong> â†’ Protect sensitive neural data through local processing, encryption, and consent-based access to signal interpretation.</li><li><strong>Mixed Reality Interfaces</strong> â†’ Allow users to define spatial privacy zones and control when spatial mapping, camera feeds, or biometric data areÂ shared.</li></ul><blockquote><strong>Underlying Philosophy</strong></blockquote><blockquote>Informed by <a href=""https://plato.stanford.edu/entries/liberalism/"">Liberalism</a>. Individuals possess rights to personal domain and consent that constrain collective or commercial interests. Privacy functions as a precondition of autonomy by preserving mental space for deliberation, identity formation, andÂ dissent.</blockquote><blockquote><strong>Contrasting Philosophy</strong></blockquote><blockquote><a href=""https://plato.stanford.edu/entries/communitarianism/"">Communitarianism</a> prioritizes common goods and may endorse broader sharing for safety, coordination, or solidarity. Legitimate community aims must be balanced against the standing claim of individuals to control access and use of theirÂ data.</blockquote><p><strong>Ethical Tension: Autonomy vs. Solidarity</strong></p><p><strong>Design Focus</strong> â†’ Protect dignity through consent and control while permitting ethical data sharing for genuine collective benefit.</p><ul><li>Practice data minimization.</li><li>Show whatâ€™s shared, why, and for how longâ€Šâ€”â€ŠwithoutÂ fatigue.</li><li>Make controls accessible, contextual, and reversible.</li><li>Process data on the userâ€™s device whenever possibleâ€Šâ€”â€Šuse cloud storage or computation only when essential and clearly justified.</li><li>Frame privacy as trust that enables participation.</li><li>For â€œcommon goodâ€ uses, require aggregation, anonymization, purpose binding, and expiry byÂ default.</li><li>Provide opt-out mechanisms for any data pooling or sharing when feasibleâ€Šâ€”â€Šdocument non-optional cases.</li></ul><h4>Well-Being</h4><p>Well-being asks whether technology supports human and ecological flourishingâ€Šâ€”â€Špsychological, social, physical, and environmental. It favors sustained clarity, balance, and restoration over compulsive engagement, overuse, or unsustainable resourceÂ demand.</p><p><strong>Examples Across Modalities</strong></p><ul><li><strong>Visual Interfaces</strong> â†’ Design calm visual hierarchies, offer dark or focus modes, and adjust color temperature or brightness over time to reduce eye strain andÂ fatigue.</li><li><strong>Tactile Interfaces</strong> â†’ Use haptic cues to encourage rest, posture shifts, or mindful breaks, supporting physical comfort and recovery.</li><li><strong>Voice Interfaces</strong> â†’ Employ tone and pacing that convey calm support without emotional manipulation or artificial empathy.</li><li><strong>Gesture Interfaces</strong> â†’ Encourage healthy movement through interaction patterns that counter sedentary use and promote active engagement.</li><li><strong>Neural Interfaces</strong> â†’ Adapt cognitive pacing and information density to match user comprehension and prevent overload orÂ fatigue.</li><li><strong>Mixed Reality Interfaces</strong> â†’ Balance immersion with grounding cues, allowing users to pause, recalibrate, or exit experiences to protect mental and physical well-being.</li></ul><blockquote><strong>Underlying Philosophy</strong></blockquote><blockquote>Anchored in <a href=""https://www.britannica.com/topic/collectivism"">Collectivism</a><strong>.</strong> Well-being is measured by the health of the collectiveâ€Šâ€”â€Šhow design supports shared welfare, mutual dependence, and the long-term sustainability of both human and ecological systems. Design choices emphasize cooperation, community benefit, and responsibility for collective outcomes over purely individual gain.</blockquote><blockquote><strong>Contrasting Philosophy</strong></blockquote><blockquote><a href=""https://plato.stanford.edu/entries/libertarianism/"">Libertarianism</a> prioritizes individual freedom and self-determination, often resisting design constraints that guide or limit user behavior for the sake of collective good. From this view, interventions that promote well-beingâ€Šâ€”â€Šsuch as limiting usage or prompting restâ€Šâ€”â€Šcan be seen as paternalistic, infringing on personal autonomy even when intended for protection orÂ benefit.</blockquote><p><strong>Ethical Tension: Welfare vs.Â Freedom</strong></p><p><strong>Design Focus</strong> â†’ Foster flourishing and reduce harm without violating autonomy or moral boundaries.</p><ul><li>Replace engagement KPIs with well-being metrics: satisfaction, balance, trust, recovery.</li><li>Use persuasion for healthy habits, not compulsion.</li><li>Add protective friction: time limits, focus modes, optionalÂ pauses.</li><li>Assess long-term emotional and cognitive effects of tone, color, andÂ loops.</li><li>Test for calm, clarity, and self-perception, not only task completion.</li><li>Offer well-being prompts as configurable aids with clear off switches.</li><li>Decouple core utility from compulsive loops (infinite scroll, variable rewards).</li></ul><h3>Design for theÂ Future</h3><p>Design for the future means translating moral philosophy into tangible choices that shape how people live, work, and connect. <a href=""https://ethicalinterface.com/index.html""><em>Ethical Interface Design</em></a> and the <a href=""https://ethicalinterface.com/pillars.html"">Five Pillars</a> framework were developed by <a href=""https://www.linkedin.com/in/micbuckcreative/"">Michael Buckley</a> to help designers approach emerging technologies with awareness and accountability. The framework offers a structured method for aligning design decisions with ethical intentâ€Šâ€”â€Šbridging creativity, usability, and humanÂ values.</p><h3>Collaborate &amp;Â Connect</h3><p>If youâ€™re interested in exploring <em>Ethical Interface Design</em> furtherâ€Šâ€”â€Šor would like to discuss a speaking engagement, workshop, or collaborationâ€Šâ€”â€Šplease get in touch. Reach out at <a href=""mailto:hello@ethicalinterface.com"">hello@ethicalinterface.com</a></p><h4>Services</h4><ul><li><strong>Speaking &amp; Workshops:</strong> Presentations on <em>Ethical Interface Design</em>, media literacy, and design philosophy.</li><li><strong>Consulting:</strong> Guidance for teams creating transparent, inclusive, and autonomy-driven digital experiences.</li><li><strong>Collaboration:</strong> Academic, research, and publication partnerships on ethics and emerging interface systems.</li></ul><p>Together, we can shape technology that serves humanÂ values.</p><h3>About theÂ Author</h3><p><a href=""https://www.linkedin.com/in/micbuckcreative/"">Michael Buckley</a> is a Professor at <a href=""https://www.shu.edu/profiles/bucklemi.html"">Seton Hall University</a>, where he teaches UX/UI design, web coding, and graphic design. With nearly two decades of experience as a creative director, brand strategist, and UX/UI engineer, his work has been recognized across healthcare, publishing, and technology for its innovation and socialÂ impact.</p><p><strong><em>Donâ€™t miss out! </em></strong><a href=""https://micbuckcreative.medium.com/subscribe""><strong><em>Join my email list</em></strong></a><strong><em> and receive the latestÂ content.</em></strong></p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=796e7cc3c9b1"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/guiding-the-future-of-ethical-design-796e7cc3c9b1"">Guiding the future of ethical design</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/why-its-okay-to-break-a-fundamental-piece-of-design-advice-a161799f55d8?source=rss----138adf9c44c---4,1762870891,Why itâ€™s okay to break a fundamental piece of design advice,"Why itâ€™s okay to break a fundamental piece of design advice

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/why-its-okay-to-break-a-fundamental-piece-of-design-advice-a161799f55d8?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/1979/1*3PPQqqusAHLz3--NFWvqTQ.jpeg"" width=""1979"" /></a></p><p class=""medium-feed-snippet"">Showing a high-fidelity version of the future isn&#x2019;t as taboo anymore</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/why-its-okay-to-break-a-fundamental-piece-of-design-advice-a161799f55d8?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/the-new-professional-core-the-future-of-work-is-sensemaking-cb6e2dfecf6d?source=rss----138adf9c44c---4,1762870818,The future of work is sensemaking,"The future of work is sensemaking

<h4><strong>AI can automate frameworks, but not their interpretation orÂ meaning.</strong></h4><figure><img alt=""Minimal composition of neutral-coloured material samples arranged in geometric blocks on a soft beige background, suggesting structure, relationships and design frameworks."" src=""https://cdn-images-1.medium.com/max/1024/1*b8DzJdpcGkM2SfZcyrbI2Q.jpeg"" /></figure><p>AI can now generate design artefacts in seconds. It can fill in a Value Proposition Canvas, a journey map, or a jobs-to-be-done structure with impressive speed and coherence. For designers and other professionals, this raises an uncomfortable question: if machines can apply the tools of our disciplines, where does that leave human expertise?</p><p>The answer lies not in what AI can produce, but in what humans can <em>decide</em>. The real transformation is not about replacing expertise, but about separating the visible outputs of design and strategy from the judgement that gives those outputs meaning. The part of professional work being automated is not the expertise itself. It is the formatting. The model doesnâ€™t replace human judgement; it replicates its surface patterns.</p><p>This distinction matters because in many disciplines, frameworks frameworksincluding design systems, journey maps and strategy canvasesâ€Šâ€”â€Šhave gradually become confused with expertise. In reality, frameworks were never the work. They were simply a way to show the thinking behind theÂ work.</p><blockquote>A canvas, a storyboard, a strategic quadrant or a journey map is a visible output of reasoning. It is not the reasoning itself.</blockquote><p>Designers and professionals are trusted not because they can draw boxes in the correct configuration, but because they can decide what those boxes should mean in this particular context, and why certain trade-offs deserve priority.</p><p>This becomes especially clear in a phrase that appears frequently in AI technical discussions: â€œonce you have a knowledge graph.â€ It sounds like a neutral implementation detail, but it conceals something fundamental. A knowledge-graph structure implies that the relevant concepts have already been chosen, that the relationships between them have already been defined, and that the worldview has already been interpreted. The graph does not emerge automatically from data. It is the result of decisions about relevance and meaning. In other words, the interpretive stepâ€Šâ€”â€Šthe step that decides what something really meansâ€Šâ€”â€Šis already done by humans before the model can do anything intelligent with the structure.</p><p>Once meaning is structured, AI can format it into any template shape you like: a canvas, a SWOT, a JTBD card, a â€œnorth starâ€ narrative or a product-positioning document. These are all just containers for meaning. The automation is real. And designers and professionals who ignore these new capabilities will eventually feel pressure from those who adopt them. But the layer that becomes automated is not the layer where value originates. It is the layer where value becomesÂ visible.</p><p>This leads to a larger shift taking place across design and the professions more broadly. The next decade is unlikely to be defined by whether AI can perform technical tasks. It is already clear that it can. The future of work will be shaped by where professional value migrates in response. Designers and professionals who have defined themselves primarily by their ability to apply tools will see their contribution begin to shrink, not because they are being replaced by machines, but because the level of work they have specialised in is now becoming commoditised.</p><p>Where value increases is in the upstream layer: the ability to define the context, shape the problem, discern what matters and design the meaning architecture within which decisions will operate. In product development, for instance, the meaning architecture is not the journey map or the sprint cadence itself, but the decision to prioritise long-term ethical design over short-term growth, or to define â€˜customer valueâ€™ in a way that includes accessibility and inclusion. These are structural choices about what the organisation believes to be valuable, and they shape every subsequent framework or metric thatÂ follows.</p><p>These are not technical choices; they are leadership ones. And because AI accelerates the downstream tasks so dramatically, the upstream space becomes more valuable. The gravitational pull of automation is downward toward execution. The centre of human advantage moves upward toward sensemaking.</p><p>In reality, the relationship between AI and human professionals is less a division of labour than a feedback loop. Automated outputs can expose hidden assumptions or new correlations, prompting humans to revisit and refine the structures of meaning themselves. The speed of automation accelerates the human capacity for interpretation by providing more â€œfirst draftsâ€ to react to, question and evolve. In this sense, AI doesnâ€™t just apply frameworks fasterâ€Šâ€”â€Šit multiplies the opportunities for deeper sensemaking.</p><p>This is why, in strategic practice, the deepest leverage is not in the artefacts themselves but in the operating logic behind them. For example, in customer-centric strategy work, it is not the journey map or the value-proposition template that drives transformation, but the underlying ontology of the business. Frameworks such as the Customer Centricity Strategy Framework, used in strategic design practice, operate at this level, defining how customer value is understood, related and governed. This is the layer where meaningÂ lives.</p><p>Deciding what matters is not only a rational act but also a deeply human one. It involves reading the emotional climate of an organisation, understanding unspoken cultural dynamics, and sensing how ideas will land with people. Frameworks may help clarify choices, but it is empathy, discretion and moral awareness that allow those choices to take root. The human advantage lies not only in deciding what matters, but in helping others believe in and act onÂ it.</p><figure><img alt=""A personâ€™s hand arranges natural and crafted materialsâ€Šâ€”â€Šwood, ceramic, shells, and small objectsâ€Šâ€”â€Šon a marble surface beside a cup of tea, suggesting human curation, reflection, and creativity."" src=""https://cdn-images-1.medium.com/max/1024/1*S48RRgrQ5v4gyW_93HEkZQ.jpeg"" /><figcaption>Human sensemaking in actionâ€Šâ€”â€Šarranging, selecting and interpreting elements to create coherence andÂ meaning.</figcaption></figure><p>So the real future-of-work question is not â€œWill AI replace me?â€ The more accurate question is: â€œAt what level am I operating?â€ If your identity is tied to applying templates, AI will apply them faster. If your identity is tied to interpreting reality, your value not only remains, it increases. Designers and professionals do not become obsolete because their tools become automated. They become obsolete when they equate their professional identity with theÂ tools.</p><p>AI can apply any framework. But only humans can decide what matters. The future of work will not be defined by competition between human and machine capability, but by the reallocation of valueâ€Šâ€”â€Šfrom tool application to meaning architectureâ€Šâ€”â€Šand that is where real transformation in organisations is most likely to happenÂ next.</p><p><em>For further reading on the deeper architecture of customer-centric meaning, see</em> <a href=""https://a.co/d/5vceae5""><strong><em>Designing Customer Experiences with Soul: How to Build Products, Services and Brands that People Genuinely Love</em></strong></a>, <em>which explores the ontology behind customer value and strategic coherence.</em></p><h4><strong>Author</strong></h4><p>Simon Robinson is CEO (Worldwide) of Holonomics and a global thought leader on customer experience, systems thinking and strategic transformation.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=cb6e2dfecf6d"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/the-new-professional-core-the-future-of-work-is-sensemaking-cb6e2dfecf6d"">The future of work is sensemaking</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/learning-by-doing-an-essential-method-in-a-design-course-d275f6e5b974?source=rss----138adf9c44c---4,1762870774,Learning by doing: an essential method in a design course,"Learning by doing: an essential method in a design course

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/learning-by-doing-an-essential-method-in-a-design-course-d275f6e5b974?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/2600/1*RRYaDkBbKJnBfk_z3FVK_g.jpeg"" width=""2803"" /></a></p><p class=""medium-feed-snippet"">Project work is a crucial experience in our UI Design master&#x2019;s educational process. Here is how we structured and refined it over time to&#x2026;</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/learning-by-doing-an-essential-method-in-a-design-course-d275f6e5b974?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/it-seems-anyone-can-build-software-now-how-do-you-build-the-right-software-182057dfa122?source=rss----138adf9c44c---4,1762857334,"When building software became easier with AI, deciding became harder","When building software became easier with AI, deciding became harder

<h4>When you can build a working prototype in an afternoon, the question is no longer <em>can you make it</em>â€Šâ€”â€Šitâ€™s <em>should you build it</em>, and <em>what should youÂ build</em>.</h4><figure><img alt=""Central target with four coral hands reaching toward it, connected by teal circuit board lines and technology icons"" src=""https://cdn-images-1.medium.com/max/1024/1*ja7QeYOYZ4XrSSk5eA3opw.jpeg"" /></figure><p>Most founders and teams struggle with those questions, not because they lack conviction, but because they test that conviction too late. Resources get committed before customer needs are validated. Confirmation bias filters what gets heard. They often fall in love with solutions before fully understanding the problems they need toÂ solve.</p><p>AI-accelerated development affects everyoneâ€Šâ€”â€Šfrom solo founders building alone to small startup teams to larger software organizations at scaleups and enterprises. Whether youâ€™re a team of one or one hundred, the challenge is the same: deciding what deserves toÂ exist.</p><p>The CHAOS Report (Standish Group, 2020) indicates that more than two-thirds of software projects fail to meet their intended outcomes. While execution failures play a role, the problems often begin earlierâ€Šâ€”â€Šin the discovery phase. A focus on delivery velocity has replaced the judgment time needed to uncover what trulyÂ matters.</p><h3>Co-Invention: Redesigning Discovery</h3><p>I had spent six weeks conducting discovery and design on an AI-powered toolkit to accelerate product discovery when I came across Ajay Agrawalâ€™s YouTube interview, &quot;<a href=""https://www.youtube.com/watch?v=UhfpHwcrx6c"">The AI Economist: The Skill You Need to Stay Employed in the Age of AI</a>&quot; (Full interview is 1 hour and 5 minutes). I was researching AI capabilities for the toolkit when his insight stopped me in my tracks. In my experience, software teams often skipped proper discovery, claiming they didnâ€™t have time. I saw AI as the solution to a productivity problem: automate tasks, integrate workflows, and make discovery faster so software teams would have fewer excuses not to do it properly.</p><p>In their book <em>Power and Prediction</em>, Agrawal, Gans, and Goldfarb (2022) describe what happened when factories first adopted electricity. Manufacturers replaced steam engines with electric motors, but retained the same layoutâ€Šâ€”â€Šone large motor, complex belts and pulleys, and machines clustered around a central power source. Productivity barely improved. The breakthrough came when they redesigned around electricityâ€™s potential: machines could go anywhere, organized by workflow rather than powerÂ source.</p><p>This is what they call co-invention: the new technology is only part of the solution. The real value lies in inventing complementary systems, processes, and organizational structures that leverage the technology's strengths.</p><p>Today weâ€™re making the same mistake with AIâ€Šâ€”â€Šbolting it onto existing workflows, increasing speed without improving outcomes. An effective AI-human partnership requires co-inventing new systems that leverage the strengths of each. The processes that guide discovery need fundamental redesign, not around speed, but around judgment.</p><p>Speed without judgment means building the wrong thingsÂ faster.</p><h3>Prediction vsÂ Judgment</h3><p>Agrawalâ€™s deeper insight reframed my thinking. In the human-AI partnership, AI makes predictions radically fasterâ€Šâ€”â€Šfilling in missing information, recognizing patterns, generating options, and forecasting outcomes. What remains scarce is judgment: the ability to interpret meaning, weigh trade-offs, and decide whatâ€™s worth pursuing.</p><p><a href=""https://youtube.com/clip/UgkxkhxVv155CAE29w9F2Dal4uOtXrRKFuEZ?si=iZTAjfaqrrJrD4o2""><strong><em>Watch economist Ajay Agrawal explain this distinction</em></strong><em>(1 minute)</em></a></p><p>Decision-making is the interplay of prediction and judgment. AI handles what is likely to happen. Humans decide what should be done aboutÂ it.</p><blockquote><strong>â€œAI handles what is likely to happen. Humans decide what should be done aboutÂ it.â€</strong></blockquote><figure><img alt=""Diagram showing AI prediction capabilities (pattern recognition, synthesis, generating options) on the left converging with human judgment capabilities (determining purpose, interpreting meaning, weighing tradeoffs) on the right into decision-making at the center"" src=""https://cdn-images-1.medium.com/max/784/1*_uav4SS4tk8EzfaiTJkeQQ.png"" /><figcaption>Diagram created by author using Google Gemini AI text-to-image creator</figcaption></figure><p>In discovery, this plays out constantly. AI can analyze customer-interview transcripts and identify patterns across dozens of conversationsâ€Šâ€”â€Šthatâ€™s prediction. But we must decide whether those patterns represent real problems worth solving or simply interesting noiseâ€Šâ€”â€Šthatâ€™s judgment.</p><p>AI can forecast which features might increase engagementâ€Šâ€”â€Šthatâ€™s prediction. Only humans can decide how well they align with customersâ€™ needs and the productâ€™s vision, purpose and principlesâ€Šâ€”â€Šthatâ€™s judgment.</p><p>Brandon Harwood (2025) describes this distinction in his framework for AI-powered product design, noting that specific tasks must remain human-centric due to the need for judgment, intuition, and emotional intelligence. What AI accelerates is synthesis; what humans provide is meaning-making.</p><p>Judgment develops by making decisions under uncertainty and owning the consequences. When you pursue an opportunity based on limited evidence, ship it, and learn it was the wrong bet, that failure can grow your judgment. When you kill a beloved idea because evidence doesnâ€™t support it and that decision proves correct, that success strengthens your judgment. Learning comes from the whole cycle: decide â†’ act â†’ observe â†’Â reflect.</p><p>Discovery judgment is the practice of developing judgment about what deserves to exist. Itâ€™s not a single activity but a set of practices: identifying opportunities, talking to customers about their needs, mapping assumptions about what would make a solution valuable, testing those assumptions with minimal investment, and deciding when evidence is strong enough to commit resources.</p><p>Sinekâ€™s (2009) Golden Circle, explored in <em>Start with Why</em> and his TED Talk <a href=""https://www.ted.com/talks/simon_sinek_how_great_leaders_inspire_action"">â€œHow Great Leaders Inspire Actionâ€</a>, provides a valuable lens for understanding why judgment remains a human trait. The Golden Circle shows that inspiring action starts with <em>why</em>â€Šâ€”â€Špurpose and beliefâ€Šâ€”â€Šthen moves to <em>how</em>, and finally <em>what</em> you produce. This order is often reversed, starting with what or how toÂ build.</p><p>AI can handle the how and even suggest the what. Only humans can determine the why, which makes it worthwhile to build. Judgment is the capacity to reason about purpose, meaning, and valueâ€Šâ€”â€Šcapabilities that belong uniquely toÂ us.</p><h3>Cross-Functional Discovery Judgment</h3><p>If discovery judgment is scarce, everyone involved in bringing software to market needs to develop it. Discovery emerges from judgment calls across multiple perspectivesâ€Šâ€”â€Štechnical feasibility, user needs, market fit, operational constraints, and strategy.</p><p>Solo founders and small teams make all these judgment calls themselves, constantly switching between perspectives. Youâ€™re the product person AND the engineer AND the designerâ€Šâ€”â€Šoften feeling overwhelmed by the need to think across domains that donâ€™t come naturally.</p><p>Larger teams distribute these perspectives across various roles, including product managers, engineers, designers, user researchers, and customer success. When these perspectives stay siloed, teams consistently build the wrong things. The â€œthatâ€™s not my jobâ€ mindset creates blind spots that AI acceleration only amplifies.</p><p>Tutti Taygerly (2020) learned this at Facebook: true collaboration isnâ€™t about stakeholders handing work to each otherâ€Šâ€”â€Šitâ€™s about establishing trust and reasoning together from the start. Whether developing multi-perspective judgment as a solo founder or learning to reason together as a team, the principle is the same: cross-functional discovery judgment determines whether you build the rightÂ things.</p><p>For solo builders, this means developing judgment across domains you might not naturally think inâ€Šâ€”â€Šand creating systematic practices to compensate for working alone. For teams, it means different perspectives combining to form a better collective judgment. Over time, this capability compoundsâ€Šâ€”â€Šsolo founders develop intuition across multiple domains, and teams learn to reason together effectively.</p><p>For organizations, siloed discovery wastes insight and resources. For individuals working alone, a lack of systematic practices wastes time and money. For professionals staying narrow in their function, they become replaceable.</p><h3>Discovery as a Continuous Loop</h3><p>Discovery isnâ€™t a pre-build phaseâ€Šâ€”â€Šitâ€™s a continuous practice that runs alongside delivery in an ongoing learning loop. Whether working solo or in teams, you discover what to build, deliver it, learn from what happens, and feed those insights back into the next round of discovery.</p><blockquote><strong>â€œDiscovery isnâ€™t a pre-build phaseâ€Šâ€”â€Šitâ€™s a continuous practice running alongside delivery in an ongoing learningÂ loop.â€</strong></blockquote><figure><img alt=""Circular diagram illustrating the continuous discovery-delivery-learning cycle with arrows showing how builders discover what to build, deliver it, learn from outcomes, and feed insights back into the next discovery cycle"" src=""https://cdn-images-1.medium.com/max/1024/1*mgTX7TtZVj4kuIf97UINzQ.png"" /><figcaption>Diagram created by author using Google Gemini AI text-to-image creator</figcaption></figure><p>Discovery requires double-loop learning: not only asking Did this feature succeed? but examining the assumptions and reasoning behind it. Single-loop learning adjusts what you do; double-loop learning improves how you think about what toÂ do.</p><p>Matthew Godfrey (2020) explores this in his work on the critical role of discovery, noting that the best product teams recognize that discovery and delivery should run concurrently and continuously, rather than as separate, sequential phases. This dual-track approach enables teams to address problems as they arise throughout the product development process.</p><p>As builders move through the discoveryâ€“deliveryâ€“learning loop, theyâ€™re not just refining the productâ€Šâ€”â€Štheyâ€™re refining their judgment. Each cycle sharpens your sense of what matters, which assumptions to test first, and what evidence is sufficient to act on. For teams, when this capability becomes shared, organizations learn to reason togetherâ€Šâ€”â€Šand that capacity compounds. For solo founders, systematic practice creates the judgment that teams might develop through diverse perspectives.</p><p>Solo founders often think they can skip systematic practices because â€œI am the product person AND the engineer AND the designer.â€ But thatâ€™s why the loop matters moreâ€Šâ€”â€Šyou need systematic practices to compensate for not having diverse perspectives catching your blind spots in real-time.</p><h3>Why This MattersÂ Now</h3><p>When spreadsheets arrived, accounting firms hired fewer accountantsâ€Šâ€”â€Šbut those who remained became more strategic and better compensated. Arithmetic specialists moved on; judgment-oriented accountants thrived.</p><p>Software builders face the same shift. The ratio of value to people is changing dramatically. Your value isnâ€™t in what AI can generateâ€Šâ€”â€Šcode, mock-ups, or transcripts. Itâ€™s in developing discovery judgment: framing the right problems, interpreting evidence, weighing trade-offs, and helping others do theÂ same.</p><p>Iâ€™ve seen this pattern repeatedly throughout my career. Teams would spend months building technically impressive solutionsâ€Šâ€”â€Šonly to discover theyâ€™d solved problems no customer actually had. The breakthrough came when we started mapping assumptions systematically before buildingâ€”the same talented people, but different discovery judgment practices.</p><p>The market is splitting: professionals who use AI merely to execute faster versus those who use AI to amplify their judgment. The latter will become increasingly valuable precisely because AI canâ€™t automate judgment. As Teixeira and Braga (2025) document in <em>The State of UX in 2025</em>, algorithms and automated tools are taking over execution tasks, making human judgment about purpose and meaning increasingly valuable.</p><p>For solo founders and small teams, this shift is even more pronounced. You can now build a complete product yourself using AIâ€Šâ€”â€Šbut your judgment quality determines whether that product solves a real problem. The democratization of building capability means your competitive advantage is 100% in judgment, not execution speed.</p><p>If this all feels like too much change, thatâ€™s natural. AI is rewriting how we work; uncertainty is real; change feels relentless. But youâ€™re already making judgment calls every dayâ€Šâ€”â€Šdeciding priorities, interpreting feedback, choosing when to proceed or pause. It isnâ€™t new work; itâ€™s becoming deliberate about the work that mattersÂ most.</p><blockquote><strong>â€œSpeed and judgment used to be a trade-off. Now theyâ€™re both requiredâ€Šâ€”â€Šand only one can be automated.â€</strong></blockquote><h3>Key Takeaways</h3><ul><li>When AI makes building and delivering software easier, faster and more cost-effective, judgment becomes the scarce resource.</li><li>Judgment develops through decisions under uncertainty and reflection onÂ results.</li><li>Cross-functional discovery judgment is the ability to interpret evidence and decide what deserves to be built and whyâ€Šâ€”â€Šwhether youâ€™re building solo or as aÂ team.</li><li>Discovery judgment is a continuous, double-loop learning practice, not aÂ phase.</li><li>The software market is segmenting: those who focus on delivering faster versus those who compete on wisdom and strategic value.</li></ul><h3>References</h3><p>Agrawal, A., Gans, J., &amp; Goldfarb, A. (2022). <em>Power and prediction: The disruptive economics of artificial intelligence</em>. Harvard Business ReviewÂ Press.</p><p>Agrawal, A. (2023, June 15). <em>The AI economist: The skill you need to stay employed in the age of AI</em> [Video]. YouTube. <a href=""https://www.youtube.com/watch?v=UhfpHwcrx6c"">https://www.youtube.com/watch?v=UhfpHwcrx6c</a></p><p>Godfrey, M. (2020, June 12). The critical role of discovery in product development. <em>UX Collective</em>. <a href=""https://uxdesign.cc/the-critical-role-of-discovery-in-product-development-6f50bf196722"">https://uxdesign.cc/the-critical-role-of-discovery-in-product-development-6f50bf196722</a></p><p>Harwood, B. (2025, February 8). Collected consciousness: AI product design for empowering human creativity. <em>UX Collective</em>. <a href=""https://uxdesign.cc/collected-consciousness-ai-product-design-for-empowering-human-creativity-dcc62100a7a4"">https://uxdesign.cc/collected-consciousness-ai-product-design-for-empowering-human-creativity-dcc62100a7a4</a></p><p>Sinek, S. (2009). <em>Start with why: How great leaders inspire everyone to take action</em>. Portfolio.</p><p>Sinek, S. (2009, September). <em>How great leaders inspire action</em> [Video]. TED Conferences. <a href=""https://www.ted.com/talks/simon_sinek_how_great_leaders_inspire_action"">https://www.ted.com/talks/simon_sinek_how_great_leaders_inspire_action</a></p><p>Standish Group. (2020). <em>CHAOS Report 2020</em>. The Standish Group International.</p><p>Taygerly, T. (2020, January 16). True collaboration with cross-functional peers (Engineering + PM + Design). <em>UX Collective</em>. <a href=""https://uxdesign.cc/true-collaboration-with-cross-functional-peers-engineering-product-management-design-7be163369bb0"">https://uxdesign.cc/true-collaboration-with-cross-functional-peers-engineering-product-management-design-7be163369bb0</a></p><p>Teixeira, F., &amp; Braga, C. (2025). <em>The State of UX in 2025</em>. UX Collective. <a href=""https://trends.uxdesign.cc"">https://trends.uxdesign.cc</a></p><h3>About GaleÂ Robins</h3><p>I help software teams and solo founders develop discovery judgmentâ€Šâ€”â€Šthe ability to decide whatâ€™s worth building when AI makes building easier and faster. My approach combines methods such as Jobs-to-Be-Done, assumption mapping, and double-loop learning with evidence-based reasoning to make judgment development systematic rather than accidental.</p><p><strong>Connect:</strong> <a href=""http://www.linkedin.com/in/galerobins"">www.linkedin.com/in/galerobins</a></p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=182057dfa122"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/it-seems-anyone-can-build-software-now-how-do-you-build-the-right-software-182057dfa122"">When building software became easier with AI, deciding became harder</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
