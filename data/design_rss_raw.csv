source,domain,url,created_utc,title,text
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/designing-streak-system-ux-psychology/,1771426800,Designing A Streak System: The UX And Psychology Of Streaks,"Designing A Streak System: The UX And Psychology Of Streaks

What makes streaks so powerful and addictive? To design them well, you need to understand how they align with human psychology. Victor Ayomipo breaks down the UX and design principles behind effective streak systems."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/building-empathy-centred-ux-framework-mental-health-apps/,1770994800,Building Digital Trust: An Empathy-Centred UX Framework For Mental Health Apps,"Building Digital Trust: An Empathy-Centred UX Framework For Mental Health Apps

Designing for mental health means designing for vulnerability. Empathy-Centred UX becomes not a “nice to have” but a fundamental design requirement. Here’s a practical framework for building trust-first mental health products."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/designing-agentic-ai-practical-ux-patterns/,1770814800,"Designing For Agentic AI: Practical UX Patterns For Control, Consent, And Accountability","Designing For Agentic AI: Practical UX Patterns For Control, Consent, And Accountability

Autonomy is an output of a technical system. Trustworthiness is an output of a design process. Here are concrete design patterns, operational frameworks, and organizational practices for building agentic systems that are not only powerful but also transparent, controllable, and trustworthy."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/css-scope-alternative-naming-conventions/,1770278400,CSS <code>@scope</code>: An Alternative To Naming Conventions And Heavy Abstractions,"CSS <code>@scope</code>: An Alternative To Naming Conventions And Heavy Abstractions

Prescriptive class name conventions are no longer enough to keep CSS maintainable in a world of increasingly complex interfaces. Can the new `@scope` rule finally give developers the confidence to write CSS that can keep up with modern front ends?"
rss,smashingmagazine.com,https://smashingmagazine.com/2026/02/combobox-vs-multiselect-vs-listbox/,1770112800,Combobox vs. Multiselect vs. Listbox: How To Choose The Right One,"Combobox vs. Multiselect vs. Listbox: How To Choose The Right One

Combobox vs. Multi-Select vs. Listbox vs. Dual Listbox? How they are different, what purpose they serve, and how to choose the right one. Brought to you by <a href=""https://ai-design-patterns.com"">Design Patterns For AI Interfaces</a>, **friendly video courses on UX** and design patterns by Vitaly."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/desktop-wallpaper-calendars-february-2026/,1769850000,"Short Month, Big Ideas (February 2026 Wallpapers Edition)","Short Month, Big Ideas (February 2026 Wallpapers Edition)

Let’s make the most of the shortest month of the year with a new collection of desktop wallpapers that are sure to bring a smile to your face — and maybe spark your creativity, too. All of them were designed with love by the community for the community and can be downloaded for free. Happy February!"
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/practical-use-ai-coding-tools-responsible-developer/,1769778000,Practical Use Of AI Coding Tools For The Responsible Developer,"Practical Use Of AI Coding Tools For The Responsible Developer

AI coding tools like agents can be valuable allies in everyday development work. They help handle time-consuming grunt work, guide you through large legacy codebases, and offer low-risk ways to implement features in previously unfamiliar programming languages. Here are practical, easy-to-apply techniques to help you use these tools to improve your workflow."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/unstacking-css-stacking-contexts/,1769508000,Unstacking CSS Stacking Contexts,"Unstacking CSS Stacking Contexts

In CSS, we can create “stacking contexts” where elements are visually placed one on top of the next in a three-dimensional sense that creates the perception of depth. Stacking contexts are incredibly useful, but they’re also widely misunderstood and often mistakenly created, leading to a slew of layout issues that can be tricky to solve."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/beyond-generative-rise-agentic-ai-user-centric-design/,1769086800,Beyond Generative: The Rise Of Agentic AI And User-Centric Design,"Beyond Generative: The Rise Of Agentic AI And User-Centric Design

Developing effective agentic AI requires a new research playbook. When systems plan, decide, and act on our behalf, UX moves beyond usability testing into the realm of trust, consent, and accountability. Victor Yocco outlines the research methods needed to design agentic AI systems responsibly."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/rethinking-pixel-perfect-web-design/,1768903200,Rethinking “Pixel Perfect” Web Design,"Rethinking “Pixel Perfect” Web Design

Amit Sheen takes a hard look at the “Pixel Perfect” legacy concept, explaining why it’s failing us and redefining what “perfection” actually looks like in a multi-device, fluid world."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/smashing-animations-part-8-css-relative-colour/,1768384800,Smashing Animations Part 8: Theming Animations Using CSS Relative Colour,"Smashing Animations Part 8: Theming Animations Using CSS Relative Colour

CSS relative colour values are now widely supported. In this article, pioneering author and web designer [Andy Clarke](https://stuffandnonsense.co.uk/) shares practical techniques for using them to theme and animate SVG graphics."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/ux-product-designer-career-paths/,1768212000,UX And Product Designer’s Career Paths In 2026,"UX And Product Designer’s Career Paths In 2026

How to shape your career path for 2026, with decision trees for designers and a UX skills self-assessment matrix. The only limits for tomorrow are the doubts we have today. Brought to you by <a href=""https://smart-interface-design-patterns.com/"">Smart Interface Design Patterns</a>, a **friendly video course on UX** and design patterns by Vitaly."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/penpot-experimenting-mcp-servers-ai-powered-design-workflows/,1767859200,Penpot Is Experimenting With MCP Servers For AI-Powered Design Workflows,"Penpot Is Experimenting With MCP Servers For AI-Powered Design Workflows

[Penpot](https://penpot.app/?utm_source=SmashingMagazine&amp;utm_medium=Article&amp;utm_campaign=MCPserver) is experimenting with MCP (Model Context Protocol) servers, which could lead to designers and developers being able to perform tasks in Penpot using AI that’s able to understand and interact with Penpot design files. Daniel Schwarz explains how [Penpot MCP](https://github.com/penpot/penpot-mcp) servers work, what they could mean for creating and managing designs in Penpot, and what you can do to help shape their development."
rss,uxdesign.cc,https://uxdesign.cc/field-study-prototypes-over-mockups-8581f20102ff?source=rss----138adf9c44c---4,1771589634,Field study: prototypes over mockups,"Field study: prototypes over mockups

<h4>A practical guide to designing with code in 2026</h4><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*KRmA-I3ubGIeZ53xNAH8lA.png"" /></figure><p>Someone drops a link in a thread — not a deck, not a Figma file — something you can click through, interact with. The conversation shifts from opinions to behavior. This keeps happening <a href=""https://dust.tt/"">at Dust</a>. We’ve been experimenting with making prototypes our default design artifact. The question driving us:</p><blockquote><strong>What should designers produce to help teams decide faster while raising the quality bar and reducing implementation waste?</strong></blockquote><p>We don’t have a final answer yet. This article is a field report: what we’ve tried so far, what seems to work, and what still feels unresolved.</p><h3>Running a design project at Dust</h3><p>I’ll describe the workflow in the order it usually happens. It’s not a rigid process — more a default path we’ve been trying.</p><h4>From “thinking” to “making it real”</h4><p>After the initial analysis and quick sketchbook phase, when I need to give the idea shape and pressure-test it, I don’t open Figma.</p><h4>Create a playground</h4><p>I open my development environment pull the latest version of our repo, and create a branch. Then I ask an agent to scaffold a new prototype, and I describe what I’m trying to make.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*DRbolDACpMR1V_In70LESw.png"" /></figure><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*DOcoZx7cEzBj2DpKU2GhqQ.png"" /><figcaption>Our playground environment. Nothing fancy. Each entry is a story.</figcaption></figure><h4>Iterate on behavior, not pictures</h4><p>At this point I mostly care about trying the idea and seeing whether the interaction holds. I’ll build small flows, prototype the transitions, and sanity-check the parts that static screens often hide (state changes, error cases, motion, empty states, keyboard/navigation/accessibility basics).</p><p>I can very easily use realistic fake data. We’ve pre-generated a bunch of it (people profiles, pictures, names, emails, lists of files and folders, entire conversations…). If I need new ones, it’s a matter of seconds to generate them.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*tON3pEqQar-RiXwxitNaIg.png"" /></figure><p>Everything runs in the browser, so I’m always looking at the experience at 1:1 scale — at user level. What I see is what user gets.</p><h4>Use the design system by default, bend it when needed</h4><p>During that phase, I don’t optimize for visual polish. I start with behavior and structure; the styling can catch up once the flow holds. The agent naturally builds on our design system (Sparkle), so the prototype reuses our components and tokens from the start.</p><p>If I need a new component or a tweak, I’ll do it on the branch — and only later decide what should become “real” design-system work vs. what was just local exploration.</p><a href=""https://medium.com/media/56ec6445356d38c3fe402d4ed71afcff/href"">https://medium.com/media/56ec6445356d38c3fe402d4ed71afcff/href</a><h4>Share early</h4><p>Sharing is simple: I push the branch to GitHub and open a PR. My prototype auto-deploys, I can share an easy-to-open URL. GitHub is a natural environment for engineers, so feedback happens where the work lives.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*DrhjiRCYmzZAGm4-Ph7BWw.png"" /><figcaption>Automatically generated deploys of storybook and playground.</figcaption></figure><h4>Converge: reduce the diff</h4><p>As iteration loops tightens and I get closer to something final, I start paying attention to:</p><ul><li>introducing just the right new component (and no more)</li><li>making the smallest necessary change in Sparkle</li><li>cleaning up accidental modifications</li></ul><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*YJRrxMU_oCIH4gt-B8WfTQ.png"" /></figure><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*zDHU4GLC-2xydHqT0D8OfA.png"" /><figcaption>Component generated, added to the design system, documented in storybook.</figcaption></figure><h4>Handoff through the PR</h4><p>Engineering handoff is straightforward: it happens through the PR.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*mJY1WVDqYtld8WpZcJLHBA.png"" /><figcaption>Design system updates and prototypes.</figcaption></figure><p>I can point to what changed, what needs refining in the design system, and what is “prototype-only”. Engineers can reuse components directly, and use the prototype code as reference — because it runs on the same stack, with the same components.</p><h3>Shortcomings</h3><p>Before going into the setup, a few honest trade-offs we’ve run into:</p><ul><li><strong>It’s still a sandbox.</strong> Unless we simulate it, it won’t naturally reproduce real latency, loading states, and backend weirdness.</li><li><strong>Feedback isn’t perfectly smooth</strong>. Vercel’s preview comments can help, but it’s been a bit flaky for us — screenshots + Slack are still the default.</li><li><strong>You occasionally pay the “code tax.”</strong> Most of the time this is faster than mockups, but occasionally you lose 30 minutes to a silly environment or tooling issue. Quite often, the prototype reveals the brief itself is still fuzzy — or that what we asked for isn’t feasible yet.</li><li><strong>It can invite premature polish</strong>. Because it looks real quickly, it’s easy to get dragged into nitpicks before the core interaction is settled.</li><li><strong>Hand-off clarity</strong>. It’s not always clear with engineers what they should directly re-use and what is prototyping.</li></ul><h3>The Setup</h3><p>Dust runs on React, in a monorepo. Our design system (Sparkle) lives there too. Sparkle gives us two complementary places to work:</p><ul><li><a href=""https://storybook.js.org/""><strong>Storybook</strong></a><strong> </strong>for building and documenting components in isolation (variants, interactions, visual regression). It’s our component catalog.</li><li><strong>Playground</strong> is just a small Vite app nested inside Sparkle. It is a fast development server, it starts quickly, refreshes instantly as you edit, and stays lightweight.</li></ul><p><strong>The key point</strong>: both environments consume the same Sparkle source and Tailwind styles, so prototypes reuse real components and tokens by default.</p><h4>Vercel for easy sharing</h4><p>Vercel is a cloud platform that automatically deploys a GitHub branch as a shareable, live URL. For us, if the branch name contains sparkle, Vercel auto-deploys both Storybook and the Playground and posts preview links in the PR — so sharing a runnable prototype is basically instant.</p><p>One thing we haven’t tried yet: bringing real AI interactions into prototypes — streaming responses, agent behavior, conversational flows. Since Dust is an AI product, that’s a meaningful gap in our current setup. Vercel’s serverless functions could be a way to close it.</p><h4>Coding tools</h4><p>The beauty of this setup is that almost any AI coding tool will work. <strong>Claude Code</strong>, <strong>Antigravity</strong>, <strong>Codex</strong> . My tool of choice is often <a href=""https://cursor.com/agents""><strong>Cursor</strong></a><strong> </strong>(for prototyping), but I move between them freely.</p><p>Notable gap: <strong>Lovable</strong>, which I heavily use and love for personal projects, isn’t a good match here — yet. For good simplicity reasons, it doesn’t handle repos as complex as ours or support the kind of professional-grade git collaboration we need.</p><h4>What about Figma?</h4><p>Don’t believe the catchy “RIP Figma” headlines. Figma hasn’t gone anywhere for us. We still use it upstream for mapping journeys, sketching flows, and running brainstorms; downstream for managing visual assets and illustrations.</p><p>It’s taken a back seat in the middle: the step where you translate an idea into something concrete enough to evaluate. It helps that we never treated Figma as the source of truth for product or design system — <em>truth is in the code</em>. That’s a huge potential efficiency win when you flip to prototyping first.</p><p>A note on Figma Make: we haven’t been convinced by it compared to our own setup. Make is designed to generate code from mockups — which makes sense if mockups are your starting point. But if you flip to prototyping first, it doesn’t play well with an existing codebase.</p><h3>The question underneath: should designers work in code?</h3><p>Behind all the talking is the old debate of technical literacy in design. Should designers code? To what extent? Here’s my current take.</p><h4>Belief #1 — Technical literacy is a design skill</h4><p>I’m an industrial designer by training. In industrial design, understanding your materials is not a “nice to have”. If you don’t know how plastic bends, how aluminum behaves, or what manufacturing constraints do to a shape, you design objects that look right in 3D but fail in the hand.</p><p>Digital products are built with a material too: code. Technical literacy does not necessarily mean coding, but it means understanding how a digital system behaves and why.</p><h4>Belief #2 — Designers working in code can improve quality and speed</h4><p>Having designers work in code can be a real accelerator: tighter feedback loops, fewer handoffs, and less drift between what we design and what ships.</p><p>The catch is that this approach doesn’t scale by default. As organizations grow, the product surface expands, the codebase gets more complex, and the friction adds up. Recruiting designers who can both design and contribute significantly to production code — at scale — is hard to rely on as a core strategy.</p><h4>Belief #3 — “Designers coding” cost-benefit has flipped</h4><p>AI is changing the game on three axes:</p><ol><li><strong>It’s easier.</strong> The “need to know” is shrinking and the cost of learning is going down fast.</li><li><strong>It’s faster.</strong> What used to take hours takes minutes. Once you have a runnable baseline, you can explore variations, states, and interactions at a speed that makes static mockups feel like overhead.</li><li><strong>It’s more powerful.</strong> The amount you can build with limited ability is remarkable — full interaction flows, working components, realistic prototypes.</li></ol><p>I won’t pretend this requires zero knowledge. It doesn’t. But the cost-benefit has flipped: what you can do, and how fast you can do it, makes it hard to justify not putting in the effort to acquire the minimal skills required.</p><h3>What does the future look like?</h3><p>The next step for us is bridging the gap between prototypes and implementation. Today, designers rarely make changes directly in production code . Running a local instance of Dust is heavy, and pushing to a test environment is too slow for tight iteration.</p><p>This is changing extremely fast. More on this soon — but if you’re curious, here’s <a href=""https://dust.tt/blog/engineering-at-the-speed-of-ai"">a technical preview of what we’re working on</a>.</p><h3>Closing</h3><p>I keep coming back to a longer historical loop. Design as a distinct practice really took shape when mass production separated roles that used to live in a single person: the craftsperson who could think, make, and sell the object. Once manufacturing scaled beyond the workshop and those roles split, you needed something that could travel between people and disciplines.</p><p>That something wasn’t just a pretty shape. It was a <em>dessein</em>: a drawing, a plan, a set of intentions constrained by technical reality. In other words: a model of the object, expressed in a format compatible with production.</p><p>Digital product design inherited that logic. Static mockups, specs, flows — they’re all versions of the same move: translating intent into a plan that others can implement.</p><p>But when coding (and prototyping) becomes more accessible we’re not only getting faster at producing plans. <strong>We’re regaining the ability to model directly</strong>.</p><p>More like clay than drafting: you shape, you test, you feel, you adjust — with an instantaneous feedback loop. The artifact is no longer a description of the thing. It starts to become the thing, or at least a runnable slice of it.</p><p>Design is moving from planning experiences to modeling them — closer to reality, earlier, and with tighter feedback loops.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8581f20102ff"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/field-study-prototypes-over-mockups-8581f20102ff"">Field study: prototypes over mockups</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/are-we-performing-ourselves-into-exhaustion-0e41da7d7652?source=rss----138adf9c44c---4,1771436232,Are we performing ourselves into exhaustion?,"Are we performing ourselves into exhaustion?

<h4>On self-surveillance, professional performance, and the cost of forgetting that we built the cage.</h4><figure><img alt=""A translucent mask held in hands with a blurred figure in the background, suggesting the separation between performance and identity."" src=""https://cdn-images-1.medium.com/max/1024/1*_gP_nYArWNMnQl8l2rAojQ.jpeg"" /><figcaption>Is this one me? (All conceptual images in this article were generated by the author using AI.)</figcaption></figure><p>As designers, we live performance twice over: as users who perform constantly, and as the creators of the systems that encourage that performance. This article is about the first, but it inevitably speaks to the second.</p><p>I spent forty minutes choosing a profile picture. Forty minutes taking selfies, testing angles, adjusting light, cropping, thinking about how I wanted to be seen. My professional dignity is somehow correlated to how well I can camouflage my aging. For people I’ve never met. Who may never see it.</p><p>While testing angles, I thought: When did this become normal? Then I did something even worse. Or more practical. Or both — I can’t tell the difference anymore.</p><p>I used an AI tool to “enhance” the photo. Adjust the lighting. Add a studio background. Give it that more “professional” look.</p><blockquote>I literally asked an algorithm to tell me what my face should look like.</blockquote><p>And the most depressing part wasn’t doing it. It was that it worked. The photo was “better.” More presentable. More performative.</p><p>I learned this from an influencer with millions of followers. He shows it with data: posts featuring his face generate 40% more engagement. More likes, more comments, more shares. Translation: more visibility, more clients, more revenue. Except you don’t have time to produce 30 different photos a week. So he uses AI to generate headshots of himself in any context. Very practical.</p><p>If you don’t do it, you’re losing reach. If you’re not doing it now, your competitors already are.</p><p>The logic is impeccable.</p><p>And I tested it. Because the data was clear. And anyone who seriously studies metrics has been doing this for months.</p><p>We went from practical to practical. And we distorted ourselves along the way.</p><h3>The promise that worked</h3><p>To be fair: personal branding was born as a promise of freedom.<br />Before, you depended on gatekeepers. Editors deciding whether your writing deserved to exist. HR deciding whether your résumé was “adequate.” Agencies deciding whether you were “relevant enough.” Your competence only mattered if someone with power gave you permission to show it.</p><p>The internet changed that.</p><p>Designers sharing their process on Behance and landing clients on the other side of the world. Writers publishing directly, without a publisher, building real audiences. That brilliant but shy professional who never shone at in-person networking is finally able to let the work speak for itself.<br />And it worked.</p><p>You wouldn’t be reading this if that weren’t true. A substantial part of those without a platform finally gained a voice. Many voices can now be heard, and that is a humanitarian gain. Opportunities that simply wouldn’t have existed 15 years ago became possible.</p><p>Personal branding was empowerment: controlling your own narrative and deciding who you are, instead of accepting the box they put you in.</p><p>But then the market noticed it was working.</p><blockquote>And did what markets do best:<br />systematized, optimized, monetized, and turned it into an obligation.</blockquote><figure><img alt=""Hand adjusting a translucent mask against a soft-focused background, depicting the continuous refinement of professional performance."" src=""https://cdn-images-1.medium.com/max/1024/1*PFmLv0kt5vVpF3k0pPh7_A.jpeg"" /><figcaption>Almost natural. Just one more adjustment.</figcaption></figure><h3>When empowerment became compulsory performance</h3><p>“Show your work” became “perform expertise 24/7.”</p><p>“Be authentic” became “authenticity as a brand strategy — here are 10 templates.”</p><p>“Build an audience” became “monetize every second of attention or you’re wasting your potential.”</p><p>And “follow these steps” became the promise that you might finally be seen in an ocean of people competing for attention.</p><p>What was optional became mandatory to survive.</p><p>And again we bought in. Because it works. Those who perform better grow faster. Those who aren’t seen aren’t remembered. Those without a personal brand have no market.</p><p>So you and I performed. And we performed well.</p><h4>We became the product</h4><p>We became a Coca-Cola that needs a logo, a tagline, an editorial calendar, a conversion funnel, alignment with causes, and constant optimization.<br />And now, an increasingly common strategy: turn on the camera. Reels. Stories. Short videos. For some, text is no longer enough. Show your face in motion. Speak directly. Rehearse spontaneity twelve times until it looks natural.</p><p>That uncomfortable task of selling yourself publicly. Of talking alone to a camera. Of feigning naturalness while repeating the same hook three times until it sounds “authentic.”</p><p>The logic is clear: the algorithm prioritizes video, “people trust people.” Showing your face has become synonymous with credibility.</p><p>So they record. Edit. Re-record. Post. And hope those fifteen seconds of filmed discomfort generate the numbers that justify the effort.</p><blockquote>They called it “personal branding.” As if you were a product. We objectified ourselves.</blockquote><p>And what does refusing mean? Becoming invisible in a market where visibility has become a condition of existence.</p><p>We show the quality of our work and what makes us unique — but through so many layers of optimization and strategic performance that the line between substance and packaging begins to disappear. We sell edited versions of ourselves — myself included — for the market to consume.</p><h4>It’s a silent escalation</h4><p>If only this were just a problem of photos.</p><p>It has migrated into everyday communication. That email with eight people copied? You know exactly what happens. Heightened attention. Every word calculated. You’re not writing to communicate — you’re performing competence for an audience you imagine judging every comma.</p><p>And we find ourselves wondering: How will so-and-so read this? What if someone takes it the wrong way? Does this sentence sound confident or arrogant? Is this tone collaborative or passive?</p><p>We edit.</p><p>Re-edit.</p><p>Save the draft.</p><p>Come back.</p><p>Change one word.</p><p>Send.</p><p>And breathe.</p><p>One more theater of efficiency.</p><figure><img alt=""A sharp, in-focus mask in the foreground with a blurred human figure behind it, showing the mask becoming clearer than the person."" src=""https://cdn-images-1.medium.com/max/1024/1*aTGFWrN1Hsbecodms47WEg.jpeg"" /><figcaption>I definitely look better this way.</figcaption></figure><h3>The mask that became a face</h3><p>At work, the performance is even more explicit. We have to put on the mask of “I’m the most incredible person you have.” Nothing more than structural survival.</p><p>If you don’t show up, you’re not remembered when promotions come around. Visible performance matters more than silent work. The clarity of your reports ends up defining your perceived reliability.</p><p>So we send weekly reports that are 70% real work, 30% personal marketing. “Key achievements this week,” we write, as if we’d won the space race. That presentation where you rehearsed your “spontaneous contribution” multiple times to impress the people who matter. That calculated moment of waiting a few minutes before responding on Slack, fast enough to seem available, but not so instant that you look desperate or like you have nothing real to do.</p><h4>And it works. Of course it works.</h4><p>Managers love employees who are clear, proactive, communicative. Organizational efficiency improves. Engagement goes up. Everyone wins.</p><p><strong>Does it, though? What’s the cost?</strong></p><p>The cost is invisible but real. Time spent editing messages instead of thinking through the problem that actually needs solving. Mental energy wasted anticipating imaginary judgments. The constant anxiety of being misunderstood or forgotten.</p><p>Professional decisions made based on “what will look good,” until you forget what actually matters.</p><p>Because beneath all this performative efficiency, there’s exhaustion. A permanent sense of inadequacy. You never sent all the updates you should have. Never participated in all the strategic meetings. Never read all the messages or the endless email threads. Never took enough initiative in company-wide meetings. Never had enough visibility to be remembered when the promotion came.</p><p>And the more you try, the more you confirm that <strong>you’re not enough</strong>.</p><figure><img alt=""Two hands pressed against a translucent barrier from inside, palms open, depicting confinement in a self-built prison."" src=""https://cdn-images-1.medium.com/max/1024/1*c0DUYalI8FDA_Lcx8JcVGA.jpeg"" /><figcaption>What was the way out again?</figcaption></figure><h3>Voluntary prisoners</h3><p>We built our own prison. There is no captor. No explicit coercion. But there are invisible bars, internalized, real. Bars made of constant visibility, of public metrics, of social validation as the currency of professional survival.<br />And we built them. Post by post.</p><p>It’s a practical necessity. Your career and market relevance depend on it. You perform without even noticing. Until you lose the ability to distinguish what you want from what you need to demonstrate.</p><p>I say this as someone who has already built a platform. Portfolio, clients, margin. What about those trying to enter? There’s no “before performing.” You start already performing. The prison has floors. Some think about leaving. Others, about getting in. That doesn’t invalidate the critique. It just situates who has the margin to make it.</p><h4>Nothing worse than becoming a voluntary enforcer</h4><p>That colleague who delivers exceptional work but doesn’t post their process? “They need to learn to communicate better.”</p><p>That designer who doesn’t update their portfolio every month?<br />“Doesn’t know how to sell themselves.”</p><p>You’ve internalized the logic so thoroughly that you now police it for free.<br />And when someone criticizes the system, we feel indignation. Almost a personal attack.</p><p>“Well, I grew because of it. You’re being naive; this is just how the world works now.”</p><p>Someone pointed at the prison, and we explained why the bars are necessary.</p><figure><img alt=""A heart shape drawn on a foggy glass barrier with hands visible below, representing affection directed toward one’s own confinement."" src=""https://cdn-images-1.medium.com/max/1024/1*mCfzlDWITl7GOUI6JwHReQ.jpeg"" /><figcaption>Privileged to be here.</figcaption></figure><h3>And we’re still grateful</h3><p>The results are real. That client from networking, that promotion, they happened. Visibility works. For those who need to build material security, this isn’t superficial.</p><p>The problem arises when we lose the ability to distinguish: am I doing this because I chose to, or because I can no longer imagine any other way of existing professionally?</p><p>And when we achieve something?</p><p><strong>Genuine gratitude, or a performance of contentment?</strong></p><p>I don’t even know anymore.</p><p>So we post about gratitude. About the “privilege of doing what you love.” About lessons from the journey. About how our team is “the most incredible family in the world,” how the company culture “transforms lives.”</p><p>#grateful.</p><h4>The moment identity disappears</h4><p>But there comes a moment, sooner or later, when that thought surfaces.<br />“That opinion I posted… do I actually believe it, or was it because it would generate engagement?”</p><p>“That project I chose… did it interest me, or did it just make better content?”</p><p>“That moment of introspection…</p><p>Did I need to think?</p><p>Or was I already performing introspection?”</p><p>And maybe we no longer know. We’ve performed for so long that we don’t know who we’d be without it.</p><figure><img alt=""A blurred interface or document visible through translucent glass with a human figure observing it, symbolizing designers viewing their own creations."" src=""https://cdn-images-1.medium.com/max/1024/1*tnPAz5xqu5ndwIUuZIv8Ug.jpeg"" /><figcaption>Ready for personal use.</figcaption></figure><h3>And who designs all this?</h3><p>Us. Designers, UX, product managers.</p><p>I’m not saying we do it in bad faith. Most genuinely believe they’re “improving the experience.” But let’s be honest about what that means:<br />Public validation metrics — views, likes, shares — so you know in real time exactly how well you’re performing.</p><ul><li><strong>The “seen” receipt <br /></strong>It obligates you to respond immediately, or perform an excuse.</li><li><strong>“Typing…”<br /></strong>It makes you rewrite your message knowing they can see you typing.</li><li><strong>Who viewed your profile<br /></strong>Sold as a premium feature. Monetizing paranoia.</li><li><strong>Your profile is 73% complete<br /></strong>Amanufactured sense of being unfinished.</li><li><strong>Seen by 47 people<br /></strong>Paranoia about who saw it and said nothing.</li><li><strong>Active now</strong> <br />Logging in as a public declaration of presence.</li></ul><p>Interfaces designed to manipulate decisions. Features that create behavioral dependency. Architectures that hijack your time.</p><p>All of this has a name. <a href=""https://www.deceptive.design/""><strong>Dark patterns</strong></a>. <strong>Addictive design</strong>. <strong>Attention capture</strong></p><p>Every discomfort you feel was engineered.</p><h4>And it worked. Better than we imagined.</h4><p>We built systems so efficient that they now face litigation. <a href=""https://www.npr.org/2026/01/27/nx-s1-5684196/social-media-kids-addiction-mental-health-trial"">Meta and YouTube for addictive design</a>. <a href=""https://www.ftc.gov/news-events/news/press-releases/2025/09/ftc-secures-historic-25-billion-settlement-against-amazon"">Amazon fined for impossible cancellation flows</a>. <a href=""https://www.digital-fairness-act.com/"">The European Union classified 97% of the most popular App Store applications as containing dark patterns</a> and began regulating the sector.<br />But when you’re on the product team, the metrics make sense. Engagement up 40%. Retention improved. We celebrate. Bonuses. A success case for the portfolio.</p><p><strong>We are, unfortunately, optimizing addiction.</strong></p><figure><img alt=""A hand against a barrier with intersecting lines or wires overlaid, representing interconnected surveillance and tracking systems."" src=""https://cdn-images-1.medium.com/max/1024/1*s7-C6NFcoyM8HzULNmyTyA.jpeg"" /><figcaption>I have bad news.</figcaption></figure><h3>The inevitable convergence</h3><p>This constant performance, this voluntary self-optimization, this prison we built… The corporate market is systematizing all of it. Developing tools that don’t just observe your performance — they help you perform better. With data, insights, algorithmic empathy.</p><blockquote>And the worst part: we’ll accept it. Because refusing means falling behind.</blockquote><p>The pieces already exist. <a href=""https://www.bbc.com/news/business-68067022"">Amazon uses tracking systems — wristbands and scanners that monitor warehouse workers’ productivity every second</a>, generating “idle time” metrics. Microsoft Viva analyzes your communication patterns and productivity. Humanyze monitors tone of voice and body language in interactions. Insurance companies offer discounts in exchange for health data.</p><p>In isolation, each seems reasonable. But when they talk to each other?<br />You stay late working on an urgent proposal. Your laptop logs nighttime productivity. Your smartwatch logs insufficient sleep. Your calendar shows six meetings in one day.</p><p>The system infers: “Problematic time management. Procrastination during working hours. Burnout risk.”</p><p>You probably won’t be consulted. The system will build a behavioral profile you’ve never seen, but that your manager can access. That interface is likely already being built by talented designers and engineers, perhaps even well-intentioned ones, who believe they’re solving real problems.</p><h4>And how will it be sold?</h4><p>“Empowerment through data.”</p><p>“Enhanced self-awareness.”</p><p>“Personalized support.”</p><p>The messaging will be empathetic, careful, almost moving.<br />But you didn’t define what “healthy” looks like. They did. You didn’t choose to be monitored. You just accepted it, because refusing would mean “not caring about your own development.”</p><p>It’s the same logic as the profile picture.</p><p>Every step is perfectly rational. Supremely practical.</p><p><strong>And we keep distorting ourselves.</strong></p><figure><img alt=""A person’s hands held together in contemplation, clearly visible without barriers, suggesting awareness and presence outside of performance."" src=""https://cdn-images-1.medium.com/max/1024/1*Y5ddbSG-7Q71-zTZ8h_vrA.jpeg"" /><figcaption>Here I am.</figcaption></figure><h3>There any redemption?</h3><p>I don’t believe the solution is deleting your account or retreating to the woods. And you probably won’t stop performing — I won’t. The game exists. Refusing to play is a privilege most people don’t have.<br />But there is a difference between performing with the awareness that it’s theater, and believing that the theater is you.</p><p>If you need to edit the photo to get in the game, that’s fine. A diplomat wears a suit, a doctor wears a white coat; you wear a profile picture. We put on masks because the world demands it. This is ritual, contingent on the moment we’re living in. It is neither eternal nor natural.</p><p>The problem was never the mask. It was forgetting you were wearing one.<br />I write this knowing that this very text participates in the same structure, it will be shared, and it will be performed. Awareness of the theater doesn’t take me off the stage. It only reminds me that I’m on it too.</p><p>But what makes us sick is not the performance itself, it’s the sustained lie. The manager who posts about human-centered culture while expecting replies at 11pm. The designer who celebrates inclusivity but never questions the product’s exclusionary metrics. The professional who performs gratitude while feeling nothing but exhaustion.</p><p>Awareness of the theater is what prevents moral dissonance. When you know it’s performance, you preserve mental energy for what truly matters. Your family. Your values. Your actual competence.</p><figure><img alt=""A person with hands in reflective pose, with a blurred screen or interface visible in the background, depicting conscious work and agency."" src=""https://cdn-images-1.medium.com/max/1024/1*uAr7q8SqhJ6CiUlqhZZeVA.jpeg"" /><figcaption>What if I changed this?</figcaption></figure><h4>And us, as designers?</h4><p>For us, creators of systems, designers, UX, product managers, there is always a gap.</p><p>A gap between “what the metric demands” and “what I know is right.”</p><p>A gap between “what will perform better in the A/B test” and “what won’t leave the user feeling manipulated afterward.”</p><p>The gap isn’t always navigable. There are moments when the system leaves no room, when the choice is between compromising the design or compromising the paycheck.</p><h4>But the gap exists.</h4><p>And culture is never static. It is always transforming. What holds value today becomes irrelevant tomorrow. What seems inevitable today, five years from now, is embarrassing.</p><p>Remember when smoking on planes was normal? When doctors recommended cigarettes? When asbestos was the “material of the future”? When tobacco advertising sponsored Formula 1? When children’s advertising had no limits? When drunk driving was socially tolerated? When no one wore a seatbelt?</p><p>I know, the damage of the attention economy is different. It’s diffuse, psychological, subjective. You can’t prove in court that the infinite scroll “caused” your anxiety the way cigarettes caused cancer.</p><p>So the change will be slower. More fragmented. It won’t be “we banned dark patterns.” It will be “some companies realize that trust is worth more than engagement, and that becomes a competitive advantage.”</p><p>Change doesn’t come overnight. It comes from accumulated small acts of resistance. From designers who, when they can, choose not to add that manipulative feature. From PMs who advocate for satisfaction metrics, not just time-on-platform. From companies that realize — late, but realize — that trust is worth more than predatory engagement.</p><h4>The market transforms.</h4><p>Fines, lawsuits, and regulations have already begun to appear. Companies are changing because it’s become too costly to stay the same. The market is extremely fluid and hates losing money.</p><h4>Perception transforms.</h4><p>Users already know they’re being manipulated. Teenagers already say “I know this is addictive, but I can’t stop.” Parents are already looking for basic phones for their children. Entire generations growing up distrustful of likes. Books like Jonathan Haidt’s The Anxious Generation and Johann Hari’s Stolen Focus are worldwide bestsellers.</p><p>None of this comes close to solving everything. But it creates space.<br />It creates conditions for products to function without depending on addiction. For professionals to build a presence without constant theatricality. For the process — not just the polished result — to hold recognized value.</p><p>And when designers, creators, and builders of systems begin to see our own complicity, the tools we create begin to change too.</p><p>Because we realize that systems that consume people eventually consume us as well.</p><p>Redemption, if it exists, is not individual. It is not “I saved myself, good luck to the rest.” It is collective, far slower than it should be, and full of setbacks.</p><p>But it is possible.</p><p>Small cracks in the system. Minimal gestures, repeated until they become culture. We stay in the game, but play it a little differently. A little more consciously.</p><p>And when we have the choice, we can choose to lie a little less.</p><p>Maybe it won’t change the world. But it keeps a piece of you intact.</p><p>And that, in itself, is already a beginning.</p><h4>To continue the conversation</h4><p>Thank you for following me through this reflection. This text is part of an ongoing inquiry into design, digital behavior, and identity, one that spills into the practice at my studio and the tools I build. If these ideas resonated, I’d love to continue the conversation.</p><p><strong>Where to find me</strong>: Substack · LinkedIn · <a href=""mailto:pedro@reinostudio.com"">pedro@reinostudio.com</a><br /><strong>Projects</strong>: Reino Studio · Talk to Amia</p><h3>References and further reading</h3><h4>I. The structure of self-surveillance</h4><p><a href=""https://www.amazon.com.br/Discipline-Punish-Prison-Michel-Foucault/dp/014013722X""><strong>FOUCAULT, Michel. Discipline and Punish: The Birth of the Prison (</strong>1975)</a><br />The concept of panopticism: when the prisoner doesn’t know if they’re being watched, they begin to watch themselves permanently. The warden becomes unnecessary because they have been internalized.</p><p><a href=""https://en.wikipedia.org/wiki/Byung-Chul_Han""><strong>HAN, Byung-Chul</strong></a><strong>. </strong><a href=""https://www.amazon.com.br/Burnout-Society-Byung-Chul-Han/dp/0804795096""><strong>The Burnout Society</strong> (2015)</a><br />The achievement-subject is not oppressed by another — they oppress themselves with greater efficiency than any external system could manage.</p><h4>II. Performance, theater, and identity</h4><p><a href=""https://www.amazon.com/Presentation-Self-Everyday-Life/dp/0385094027""><strong>GOFFMAN, Erving. The presentation of self in everyday life</strong> (1959)</a> Goffman proposes that all social interaction is performance, with front stage and back stage. He wrote this decades before social media — and described with precision exactly what they would do once they eliminated the backstage.</p><p><a href=""https://www.amazon.com/Society-Spectacle-Guy-DEBORD/dp/0934868077/ref=sr_1_1?crid=3QQH81X8H5F6&amp;dib=eyJ2IjoiMSJ9.ipVuVSR-KZ75gnFYEPz4oy8dlinEN1agXzPeL93S7w61QxfcdJ0LG8UdT4_vraJJq5kTaUQiuAf62R8IdpBAVynDw4TXpGZePXWifSEzqsGm5aE0rkOSmMLDglVFIrBrN7QfFiNYXSAMs-IbQscX28kah6_JwZ7vuQfFsx3FgFTrO6mEayQQ0YXkhhNk5xdbMoSGDS48iCRFC4hxIqyyfX92VeQTuSv2S6jc4jkCGoc.LG_PSMvRD0Zk-cRLIntj_mJCrZs9n6K2cIxnFd5-8b4&amp;dib_tag=se&amp;keywords=The+Society+of+the+Spectacle+%281967%29&amp;qid=1771373734&amp;s=books&amp;sprefix=the+society+of+the+spectacle+1967+%2Cstripbooks%2C196&amp;sr=1-1""><strong>DEBORD, Guy. The Society of the Spectacle (1967)</strong></a> <br />The spectacle is not a set of images: it is a social relation between people mediated by images.</p><h4>III. The design of attention and its mechanisms</h4><p><a href=""https://pt.wikipedia.org/wiki/Shoshana_Zuboff""><strong>ZUBOFF, Shoshana</strong></a><strong>. </strong><a href=""https://pt.wikipedia.org/wiki/Shoshana_Zuboff""><strong>The Age of Surveillance Capitalism</strong> (2019)</a> <br />How platforms transformed human behavior into raw material for prediction and modification. The economic logic behind every feature designed to create anxiety.</p><p><a href=""https://www.deceptive.design/""><strong>BRIGNULL, Harry. Deceptive Design</strong> </a><br />The researcher who named and catalogued dark patterns in 2010. The site maintains a living archive of deceptive patterns identified in real products.</p><p><a href=""https://www.edpb.europa.eu/system/files/2023-02/edpb_03-2022_guidelines_on_deceptive_design_patterns_in_social_media_platform_interfaces_v2_en_0.pdf""><strong>EUROPEAN DATA PROTECTION BOARD. Guidelines 03/2022 on Deceptive Design Patterns in Social Media Platforms</strong></a><strong><br /></strong>The European regulatory document that classified manipulation techniques across major platforms.</p><h4>IV. The cost of metrics</h4><p><a href=""https://www.amazon.com.br/Tyranny-Metrics-Jerry-Z-Muller/dp/0691174954""><strong>MULLER, Jerry Z. The Tyranny of Metrics </strong>(2018)</a><br />When the metric becomes the goal, behavior reorganizes itself around appearing — not being.</p><p><a href=""https://www.amazon.com.br/Weariness-Self-Diagnosing-Depression-Contemporary/dp/0773536256""><strong>EHRENBERG, Alain. The Weariness of the Self</strong> (2010)</a><br />The contemporary anguish is not that of someone who has failed — it is that of someone who never finishes constructing themselves.</p><h4>V. For further investigation</h4><p><a href=""https://uxdesign.cc/the-snake-that-eats-its-tail-3656b31fd0f9""><strong>MURRAY, Rachel M. The snake that eats its tail</strong></a><strong><br /></strong>UX Collective. The tech ecosystem as a self-feeding feedback loop: attention generates data, data refines design, design captures more attention.</p><p><a href=""https://uxdesign.cc/the-snake-that-eats-its-tail-3656b31fd0f9""><strong>FABRIZIA. The design of shallow thinking</strong></a><strong><br /></strong>UX Collective, 2025. How the internet’s design choices have reconfigured not just what we consume, but the cognitive architecture with which we process anything at all.</p><p><a href=""https://uxdesign.cc/are-we-creating-brain-rot-dad9e947ba5c""><strong>WILHELM, Daley. Are we designing for brain rot?</strong></a><strong><br /></strong>UX Collective, November 2025. The consequences of creating products designed to form compulsive habits — and the designer’s responsibility in that process.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=0e41da7d7652"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/are-we-performing-ourselves-into-exhaustion-0e41da7d7652"">Are we performing ourselves into exhaustion?</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/openai-ads-as-content-708666ddab2b?source=rss----138adf9c44c---4,1771436145,OpenAI: from ads to content,"OpenAI: from ads to content

<h4>The future of digital advertising should feel less like a distraction and more like recommendations from a friend.</h4><figure><img alt=""A delicate watercolor and graphite sketch of a smartphone screen showing a chat interface where one specific message bubble is glowing with a warm, golden light, symbolizing a high-utility, relevant answer"" src=""https://cdn-images-1.medium.com/max/1024/1*3gimrvTxVY4VhPIvGKaU5A.png"" /><figcaption><em>High relevance ads could enhance the quality of the content.</em></figcaption></figure><p>I’m a <strong>product designer</strong> and for the last four years I have been working in digital advertising for tech. I wouldn’t have imagined building part of my career in advertising, but here I am, and I have found myself honestly enjoying the challenge. Through this experience, I have learned that <strong>healthy and fair advertising</strong> is actually a good thing for the industry. It helps brands reach people, and it helps people find what they are looking for. When ads are well-executed, they are great content. There have been amazing pieces of advertisement across history, and when you pair great ads with great products, magic happens.</p><p>A few days ago, ChatGPT started <a href=""https://openai.com/index/testing-ads-in-chatgpt/"">testing ads</a> within its conversations. The first screenshots show a safe and expected approach: a user starts a conversation around a specific vertical, the model gives a standard AI answer, and then it shows an ad related to the conversation keywords. This replicates the model used in other media platforms by capturing signals and showing inventory that matches them. It’s a safe starting point, but it feels simplistic considering the possibilities of AI. There should be a more sophisticated way to do this. We need a model that treats ads not as an interruption, but as <strong>high-utility content</strong> that actually improves the experience.</p><figure><img alt=""A screenshot of a ChatGPT interface showing a ‘Potluck power tips’ response followed by a sponsored ad from ‘Heirloom Groceries’ for an Enchilada Kit, featuring price, stock status, and a small product image."" src=""https://cdn-images-1.medium.com/max/1024/1*KHxGNphfpnapvof7EIbXog.jpeg"" /><figcaption><em>OpenAI’s initial test displays ads as sponsored links following a conversational response.</em></figcaption></figure><p>The immediate reaction from many is that ads in ChatGPT will harm the user experience. I think that if the strategy is well executed, it could <strong>improve the quality of recommendations</strong>. My rationale is based on the <strong>data source</strong>. When platforms create an ad ecosystem, they have a real incentive to digest merchant catalogs, local service listings, and travel databases. Once this content is set up for advertisers, it can be used to power general responses, too.</p><p>Currently, when you ask ChatGPT about a product or local service, it likely scrapes multiple websites. This doesn’t guarantee the metadata is right or updated. A <strong>direct integration</strong> with a business’s database ensures freshness and reliability. It is the technical difference between scraping a UI and communicating directly through an <strong>API</strong>. I don’t doubt that advertisers, from global retailers to local barber shops, are waiting for the moment they can upload their catalogs and service lists directly to OpenAI. As <a href=""https://www.roidna.com/blog/advertising-in-the-age-of-llms-what-happens-when-ai-becomes-the-ad-platform/"">industry reports from <strong>ROI·DNA</strong></a> suggest we are entering an era where brand-verified catalogs become a fundamental part of the reasoning layer, allowing the model to ground its advice in high-quality, real-time data rather than just scraped noise.</p><p>Think of it this way: if you ask about running shoes today, the model analyzes your context and scrapes the web. It reads reviews and influencer videos, but it is unlikely to find professional, unbiased benchmarking. Most of these sources already have sponsored rankings, so the content is already biased by <strong>SEO games</strong>. The model might show you an organized summary, but it’s coming from already <strong>biased and probably sponsored sources</strong>. The products might also be out of stock or the technical specs might be outdated.</p><p>In a new model, the AI would bypass this noise by directly integrating with <strong>rich catalogs</strong> of shoes, boutique hotels, or specialized medical services. These databases include real-time price, stock, and local availability. When doing research, the model could cross-reference this first-hand data with community reviews or scientific research. This also <strong>levels the playing field</strong>, as smaller, local brands could be part of that same inventory.</p><p>By leaning into this infrastructure now, OpenAI is pulling ahead of most competitors. They are building a better recommendation experience under the guise of an ad network, establishing the <strong>technical pipes required for commerce </strong>before others even begin. <strong>Google</strong> remains a unique incumbent here. Through its shopping features, it has spent years perfecting an ecosystem of real-time merchant data. While Google may be better positioned to activate these features when they decide to fully commit to AI ads, OpenAI’s moves are defining a new standard for what a helpful recommendation engine in the AI world looks like.</p><p>My take on the recent <a href=""https://www.youtube.com/watch?v=De-_wQpKw0s"">Anthropic ad campaign</a>, which brilliantly mocks the scenario where ads become intruders in the conversation, is that while it is a great piece of marketing, it only addresses the “noisy” version of advertising. In a scenario where ads are treated as high-quality, verified content, this satirical picture no longer fits. More importantly, LLMs that fail to integrate these APIs and catalogs soon risk falling behind in the race to provide the best commercial recommendations because that is a primary use case for AI users today.</p><p>I often think of the analogy of a friend recommending a product. If you tell a friend who is passionate about bikes that you’re looking for a weekend ride, they don’t just shout a brand name at you. They ask about your taste, your budget, and your local terrain. They are a friend first, understanding your context before suggesting a solution. This is the difference between <strong>advertising as noise and advertising as advice.</strong></p><figure><img alt=""A minimalist watercolor study of two friends sitting at a cafe table; one is gesturing while a faint, sketched bicycle appears as a shared thought between them."" src=""https://cdn-images-1.medium.com/max/1024/1*QNSAdwJd_H2s30QaPRdENg.png"" /><figcaption><em>AI recommendations should feel like a conversation between friends.</em></figcaption></figure><p>This analogy brings us to a <strong>“neutrality paradox.”</strong> A good AI analysis should rank options based on what is actually best for the user, rather than who paid the most. Critics will rightly ask: how can the model remain an unbiased advisor if it’s also the salesperson? To get this right, OpenAI must maintain a <strong>strict wall</strong> between the model’s reasoning and the ad auction. The AI should first determine the ideal solution for the user, and only then see if there is an ad that matches that specific intent. If the best suggestion happens to be sponsored content, everyone wins. If the model starts nudging users toward an ad just to hit a revenue target, the trust is gone.</p><p>For this neutrality to feel real, it has to be visible. This means following the rules of digital advertising, like <strong>clear labeling</strong>. If a recommendation is sponsored, the user should know exactly why it’s there and have the power to opt out or mute specific ads entirely. Trust isn’t built by camouflaging the ads, it’s built by giving the user <strong>visibility and control</strong>.</p><p>This new paradigm also has massive implications for <strong>user agency</strong>. Think of a scenario where, when planning a trip, the model surfaces a tiny local coffee shop that just opened in your destination: a business that otherwise could never afford to compete with big brands for global SEO. Perhaps because the user specifically asked for small local businesses. For this to work, OpenAI has to solve the <strong>“pay to play”</strong> barrier. If only the giants can afford to bid, the user’s preference for local shops becomes a dead end. A fair game requires a model where relevance and values can occasionally outrank a large ad budget.</p><figure><img alt=""A warm, glowing watercolor of a small local coffee shop storefront standing out in vibrant color against a muted, grey-sketched city background."" src=""https://cdn-images-1.medium.com/max/1024/1*3ylFl0Cshsq07DBHsYxJFg.png"" /><figcaption><em>Find local businesses that matter to you.</em></figcaption></figure><p>This leads to a shift away from one-way communication. Most ads today are static broadcast. In this new model, an ad becomes a <strong>two-way dialogue</strong>. Instead of a generic banner, the ad can become an interactive experience that answers specific questions about a product’s features. It moves from interruption to <strong>consultation</strong>, surfacing only the parts of a product relevant to your current need. The model becomes more like a great salesperson that answers your questions, one who knows when to say, “I have nothing to offer you right now, but it was great having a conversation with you.”</p><p>This approach also enables better <strong>discovery</strong>. Traditional search is limited by what we know to ask for. In a conversation, an AI can identify a need that the user hasn’t yet put a name to. This is a shift toward a discovery-based experience, similar to what platforms like <strong>Pinterest</strong> have spent years perfecting. As <a href=""https://medium.com/pinterest-engineering/ads-candidate-generation-using-behavioral-sequence-modeling-f9077ee1325d"">Pinterest Engineering notes</a>, ads are most effective when they are designed to inspire and connect users with ideas they genuinely love rather than acting as a simple distraction.</p><p>Of course, this level of personalization raises the stakes for privacy. To be the “friend” in the analogy, the AI needs to understand your context deeply, but that data must remain a private secret. The real opportunity is in <strong>zero-knowledge advertising</strong>, where an advertiser knows their ad was shown to the right person, but never knows who that person is or what they said to the AI. The proof of success will be found in ad performance, not data harvesting.</p><p>Ultimately, the success of this shift depends on one thing: OpenAI’s willingness to be rigorous enough to <strong>decide when not to show an ad</strong>. The business hunger for growth often leads platforms to flood every empty space with sponsored content. If OpenAI shows an ad just to collect the incentive, even when it doesn’t truly help the user, they become just another search engine. If they make it right, they can leverage the power of the ads as content.</p><p>There are broader challenges in the AI landscape that need to be addressed. Like any powerful technology, AI must be governed by priorities like safety and ethics. <strong>Safety concerns must always come first.</strong> It is only when all necessary guardrails are met, and the model identifies a clear and safe shopping intent, that a healthy advertisement ecosystem can exist.</p><p>This new paradigm opens a significant opportunity for ads to evolve from a <strong>tax on our attention</strong> into a <strong>service for our needs</strong>. A well-executed advertisement could be a win for everyone involved. We’re jumping into the <strong>next phase of human history</strong>, and yes, there will be ads. I don’t think anything will stop this change, so the best thing we can do is get it right by ensuring the value of the answer is never traded for the price of the ad. That’s what I’m hoping to happen.</p><p><strong><em>Rodrigo Osornio</em></strong><em> is a San Francisco-based product designer with training in visual arts and industrial design. He is passionate about shaping the future of human-technology interaction.</em></p><p><em>Illustrations created by the author using mixed media. The screenshot of the OpenAI ads test was taken from their announcement.</em></p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=708666ddab2b"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/openai-ads-as-content-708666ddab2b"">OpenAI: from ads to content</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/5-reasons-why-ar-glasses-are-inevitable-69fb46013816?source=rss----138adf9c44c---4,1771417176,5 Reasons why AR glasses are inevitable,"5 Reasons why AR glasses are inevitable

<figure><img alt=""A glasses with floating UI on it. AI-Generated image"" src=""https://cdn-images-1.medium.com/max/1024/1*RDWf03gEFMyyNtIVGJpa2g.jpeg"" /><figcaption>AI-generated image based on the original from this article <a href=""https://www.queppelin.com/ar-glasses-for-navigation/"">https://www.queppelin.com/ar-glasses-for-navigation/</a></figcaption></figure><p>When I talk about AR glasses, I don’t mean bulky headsets. I mean devices that look like regular eyewear capable of whispering information into your ear and displaying 3D content or flat panels overlaid on the real world.</p><p>This article is neither a validation of the technology nor a criticism, it is an objective personal look at what I see coming. Here are five reasons why I believe AR glasses will succeed and become mainstream, along with the consequences designers and users must face.</p><h3>1. Solving the “Kidnapped Body” problem</h3><figure><img alt=""An illustration of a woman looking at the phone. AI-Generated image"" src=""https://cdn-images-1.medium.com/max/1024/1*Kv1QO5wUplFqomrm1QmdGg.jpeg"" /></figure><p>Mobile devices demand a high physical price. They force us to look down, occupy our hands, and monopolize our attention. I call this the “kidnapped body” problem. We pay this price without thinking, but the implications are profound. It severs us from the physical world, blocking out views, people, and genuine connection.</p><p>We have even developed medical terms for this. “<a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC11856789/"">Tech Neck</a>” describes the repetitive stress injury caused by constantly looking down at a screen. AR glasses may succeed because they liberate the body. They return our hands to us and, more importantly, they lift our chins. By allowing us to look up and ahead, they reintegrate us into the world. While they will for sure introduce new distractions, the shift from “head-down” to “heads-up” computing is a fundamental ergonomic correction that users will gravitate toward.</p><h3>2. The new, no-new device</h3><figure><img alt=""A woman smiling grabbing her glasses. AI-Generated image"" src=""https://cdn-images-1.medium.com/max/1024/1*zGXPyqrNH8iLmzgZrqIbKw.jpeg"" /></figure><p>In a gadget-saturated world, introducing a completely new device is risky. However, AR glasses have a secret weapon. They aren’t “new.” They are just glasses.</p><p>Global estimates suggest around <a href=""https://www.overnightglasses.com/eyewear-industry-statistics/#:~:text=Globally%2C%20at%20least%202.2%20billion,for%202023%20was%20$65.6%20billion."">4 billion people already wear glasses</a> for various reasons, with over <a href=""https://www.who.int/news-room/fact-sheets/detail/blindness-and-visual-impairment"">2.2 billion people having a near or distance vision impairment</a> according to the World Health Organization. The form factor is familiar, socially accepted, and worn daily. The friction to adoption is incredibly low compared to a VR headset. Much like the Apple Watch which sold 12 million units in its breakout year by leveraging the centuries-old habit of wearing wristwatches. As long as they look like iconic eyewear rather than tech hardware, they are primed for mass adoption.</p><h3>3. The Spatial Canvas (Escaping the Screen)</h3><figure><img alt=""An illustration of a busy street seen from the perspective of a person with AR glasses. The street is full of floating user interface. AI-Generated image"" src=""https://cdn-images-1.medium.com/max/1024/1*a-BfGo5NIZp_uzQbQHmxCg.jpeg"" /><figcaption>AI-generated image based on the original from this article <a href=""https://www.crunchfish.com/how-does-the-world-look-through-smart-glasses/"">https://www.crunchfish.com/how-does-the-world-look-through-smart-glasses/</a></figcaption></figure><p>There is a multi-million-dollar industry currently trapped behind 6-inch glass rectangles. Tech giants from phone manufacturers to app developers are hitting the limits of what can be done on a 2D flat screen.</p><p>The physical world is a “green field” for these companies, a massive, unclaimed canvas for digital overlay. Whether we like it or not, the push to colonize our surroundings with digital content is the next logical growth engine for the tech sector.</p><p>For product designers, this represents the most significant paradigm shift ever seen. We are moving from designing within a constrained frame to designing without borders. Eventually, we will no longer design apps that “trap” users, we will design apps that enhance the environment. This shift to “Spatial Computing” will unlock entirely new economies and user behaviors that flat screens simply cannot support.</p><h3>4. Strong Use Cases</h3><figure><img alt=""A floating user interface seen from inside a car. AI-Generated image"" src=""https://cdn-images-1.medium.com/max/1024/1*dDpTz4QCVcDT2OR8tmirpA.jpeg"" /><figcaption>AI-generated image based on the original from this article <a href=""https://www.digitaltrends.com/cars/envisics-ar-windshield-technology/"">https://www.digitaltrends.com/cars/envisics-ar-windshield-technology/</a></figcaption></figure><p>The utility of placing digital objects into the physical world is undeniable. While current iterations act as a Heads-Up Display (HUD) rather than fully immersive AR, they are the necessary bridge to the future.</p><p>Apps designed for physical interaction will thrive first:</p><ul><li><strong>Exploration and Learning:</strong> This is perhaps the strongest long-term use case. You could stand in front of a historical monument and instantly see contextual history overlaid on the stone. You could look at a complex car engine and see step-by-step repair labels pointing to the exact parts you need to touch. It turns the entire world into an interactive learning manual.</li><li><strong>Navigation:</strong> Arrows overlaid on the street as you walk, allowing you to converse with friends without checking a phone map.</li><li><strong>Cooking:</strong> Hands-free recipes floating above the counter.</li><li><strong>Safety:</strong> The ability to livestream to emergency services instantly during dangerous situations could be a significant deterrent to crime.</li><li><strong>Accessibility:</strong> For users with disabilities, these glasses can provide real-time context, enhance visual contrast, or narrate the environment instantly empowering users without requiring active intervention.</li></ul><p>As product designers we can already start exploring how to design for AR glasses in Google’s documentation for instance, in this <a href=""https://design.google/library/transparent-screens"">article</a>, David Allan Reese talks about how to design with daylight, UI components sizes, contrast, space awareness and more. You can also take a look at how Google’s AR UI kit, <a href=""https://developer.android.com/develop/xr/jetpack-xr-sdk/jetpack-compose-glimmer"">Jetpack Compose Glimmer</a>, looks like. On the other hand, Meta has also released its <a href=""https://wearables.developer.meta.com/docs/develop"">documentation </a>about developing and designing for Ray-Ban Meta Glasses.</p><h3>5. Voice Interaction Made Natural</h3><figure><img alt=""An illustration of a woman with an AR glasses touching the right side of them simulating touching the volume. Some sound player UI floating next to the hand. AI-Generated image"" src=""https://cdn-images-1.medium.com/max/1024/1*mEhRbXCbvv-XrjkMYjF2kg.jpeg"" /></figure><p>Historically, conversational interfaces have suffered from poor reputations. They were often frustrating and limited. However, the rise of Large Language Models (LLMs) has fundamentally changed this.</p><p>Conversational AI is becoming natural, instant, and context-aware very fast. When you combine competent AI with the “hands-free” nature of glasses, voice becomes a crucial input system. The social awkwardness of talking to a device will likely fade, much like how wearing wireless earbuds 24/7 went from “rude” to “normal.”</p><h3>The Consequences</h3><p>If this technology succeeds, we must be prepared for two major friction points, particularly from a design perspective:</p><p><strong>1. The Privacy Paradox</strong> Privacy concerns run in two directions. First, what are manufacturers capturing about our daily lives? Second, how do we manage peer-to-peer privacy? The “always-on” camera creates uneasiness in social environments. How will society regulate recording in public? Will hacking the “recording LED” become common? These social contracts will need to be rewritten, and many users may reject the tech simply to opt out of this surveillance state.</p><p><strong>2. Spatial Invasion and New UX Patterns</strong> If the world becomes a digital canvas, who owns the rights to that space? We risk a future of “spatial spam” ads anchored to physical storefronts, landmarks, or even sidewalks. Without strict regulation, the tranquility of the real world could be interrupted by pop-ups and notifications that we cannot swipe away.</p><p>For designers, this raises critical ethical questions. What is the “dark pattern” of spatial design? How do we design a safe mode mechanism for the real world? We may see the rise of aggressive attention-grabbing patterns that intentionally block your view. Designers will have the heavy responsibility of defining the etiquette of this new layer. Will we design respectful interfaces that blend in, or intrusive billboards that demand attention? We may not see the regulations put in place until after the intrusion has already begun.</p><h3>To wrap up</h3><p>AR glasses introduce a new reality, literally. They’ll bring with them as many opportunities as there are threads of concern. For product designers, the challenge is not just to be ahead of the trend, but to deeply understand what this technology adds to or subtracts from human life.</p><p>Personally, I am fascinated by a future enhanced by this technology, a life where reality is augmented to help and delight us. However, we have seen how initial excitement can turn into unforeseen consequences. Social media reminds us of how a tool for connection can evolve into a mental health crisis. By the time we realized the damage, it was already too late for some generations.</p><p>I believe in the power of individual common sense, but I also know that mass adoption shifts behavior in unpredictable ways. I hope this article has sparked your interest and encouraged you to investigate further. We are facing a fascinating technological horizon, and it is up to us, designers and users, to ensure it remains a tool for human enhancement rather than distraction.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=69fb46013816"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/5-reasons-why-ar-glasses-are-inevitable-69fb46013816"">5 Reasons why AR glasses are inevitable</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/bad-model-behaviour-by-design-af0b514a1314?source=rss----138adf9c44c---4,1771417119,Bad (model) behaviour by design,"Bad (model) behaviour by design

<h4>AI doesn’t just reflect human bias. It amplifies it through everyday use, quietly shaping judgement, trust, and decisions.</h4><figure><img alt=""Illustration of a person from behind, holding a pen thoughtfully while reviewing two stacks of CVs on a wooden desk. One stack is labelled “AI” and the other “Colleague.”"" src=""https://cdn-images-1.medium.com/max/1024/1*tIC7DvEr9qL6ZqaiHrcYcw.png"" /><figcaption>Image generated by author</figcaption></figure><p>You’re halfway through a stack of job applications, AI assistant humming along beside you, flagging promising candidates like an eager intern. It disagrees with your gut feeling about one applicant. You pause. Reconsider. A few screens later, it disagrees again. You change your mind once more. By the end of the session, you’ve deferred to its judgment on nearly a third of the decisions where you initially felt confident.</p><p>Now swap the algorithm for a colleague. Same stack, same disagreements. How often do you actually budge? Much less often, as it turns out.</p><p>This isn’t hypothetical hand-wringing. <a href=""https://www.nature.com/articles/s41562-024-02077-2"">Recent research</a> involving 1,401 participants found that <strong>when people disagree with AI, they change their minds 32% of the time. With other humans, that figure drops to 11%.</strong> That’s almost three times more influence from a machine than from another person. Quite something, given we’ve spent millennia learning to be sceptical of each other.</p><p>We like to think of ourselves as rational decision-makers who simply use these tools as helpful aids. The evidence tells a different story. They do more than support our thinking; they reshape it. And far more powerfully than human opinions ever could.</p><p>This raises a question that should keep designers up at night: if users are this susceptible to algorithmic persuasion, what happens when the model itself is biased?</p><p>The answer isn’t just that users make biased choices. The effect compounds. Through repeated interactions, small errors snowball into meaningful distortions. And in most cases, users never notice it happening.</p><figure><img alt=""An infographic showing two stacks of CVs: a tall stack labelled “AI Recommended” and a shorter stack labelled “Your Gut Feeling.” An arrow shows 32% of decisions shifting toward the AI pile. A hand with a magnifying glass reviews the documents."" src=""https://cdn-images-1.medium.com/max/1024/1*yvNAnP9ZZFCbxgWnaz2X4A.png"" /><figcaption>Image by author</figcaption></figure><h3>The amplification machine</h3><p>To understand why AI influence packs such a punch, we need to look at what’s actually happening in the interaction itself. The problem isn’t one-sided. It’s a two-way street, with both the AI and the human feeding into a loop that amplifies bias at every turn.</p><p>On the AI side, these systems don’t simply reflect the patterns in their training data. They tend to exaggerate them. Machine learning models are optimised to detect and reinforce patterns, which means subtle skews in the data often become stronger in the output.</p><p>A hiring algorithm trained on historical data where 70% of successful hires were men doesn’t just reproduce that 70/30 split. It learns to weight male candidates more heavily, nudging the bias further along. The system is simply doing what it was built to do: find patterns and double down on them.</p><p>But that’s only half of the picture.</p><h3>The human half of the equation</h3><p>We don’t treat AI recommendations the same way we treat advice from another person. We tend to see automated systems as more objective, more analytical. Somehow immune to the messy biases that plague human thinking. This perception gets reinforced by how AI is marketed (“trained on the sum of human knowledge”) and by our long-standing trust in technology for high-stakes decisions in healthcare and finance.</p><p><a href=""https://www.nature.com/articles/s41562-024-02077-2"">Research from University College London and MIT</a> shows just how potent this combination can be. Across a series of experiments, participants interpreted facial expressions, motion perception, and other people’s performance. In each case, they interacted with AI systems that had been deliberately programmed with biases similar to those found in many real-world models.</p><p><strong>What happened next wasn’t just a momentary nudge.</strong> The distortion deepened. Through repeated interaction, small initial errors grew into significant skews. Participants who started with relatively minor biases ended up with significantly stronger ones. Largely because the AI amplified existing patterns while participants remained unusually receptive to its guidance.</p><p>Here’s where it gets uncomfortable: most participants had no idea how much the AI was shaping their judgement. When researchers told participants they were interacting with another person (while secretly using a machine), the effect weakened. Simply believing the source was human reduced how deeply the bias took hold.</p><figure><img alt=""Three-phase diagram showing bias persistence. Phase 1: Human and AI robot making decisions together, producing a skewed pattern of outputs. Phase 2: AI is removed, human continues working alone. Phase 3: Human alone still making the same skewed decisions, with a “bias stuck” label and a broken gear icon indicating the pattern has transferred to the human even without the AI present."" src=""https://cdn-images-1.medium.com/max/1024/1*I7183KuMBJ1-HEdXNNmGnQ.png"" /><figcaption>Image by author</figcaption></figure><p>Awareness mattered, but not in the way we might expect. Knowing that the advice came from an algorithm made people <em>more</em> susceptible, not less. <strong>We expect AI to be right, so we lower our guard.</strong> It’s the automation equivalent of assuming the smoke alarm would have gone off if there were a real fire.</p><p>Even more troubling: this effect doesn’t vanish when the AI does. <a href=""https://www.nature.com/articles/s41598-023-42384-8"">A 2023 study</a> had participants assess medical images (specifically, deciding whether skin spots were benign or potentially cancerous) using a deliberately biased algorithm.</p><p>When the system was removed, the bias stuck around. People continued making the same skewed calls on their own, having unconsciously absorbed its patterns into their decision-making. The higher someone’s self-reported trust in automation, the more their independent conclusions mimicked the machine’s errors.</p><h3>The path of least resistance</h3><p>Our vulnerability to AI isn’t just about trust in technology. It’s rooted in something rather unflattering about how our brains handle effort.</p><p><strong>Psychologists call this the <em>cognitive miser</em> effect. </strong>Thinking is expensive (metabolically, emotionally, and in terms of time) so when we’re handed an opportunity to reduce mental effort, we tend to grab it. These systems offer a particularly seductive shortcut: offloading complex judgement to something that appears faster, smarter, and more thorough than we could manage ourselves.</p><p>This helps explain why automation bias shows up so consistently. <a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC3240751/"">Studies show</a> that when people work with automated decision-support systems, they dial back their own monitoring and verification. <strong>The presence of AI doesn’t just influence decisions; it quietly changes how much effort we put into thinking at all.</strong></p><p>We see the recommendation, and it feels complete. The work appears to be done. On to the next thing.</p><figure><img alt=""Split illustration comparing cognitive load. Left side labelled “Without AI: High Cognitive Load” shows an overheating brain filled with gears, surrounded by data, documents, and question marks. Right side labelled “With AI: Reduced Cognitive Effort” shows a calm, glowing brain with a “Recommendation Accepted” checkmark and a small robot below."" src=""https://cdn-images-1.medium.com/max/1024/1*YdaGvpezSIlnOGjNRow9JQ.png"" /><figcaption>Image by author</figcaption></figure><p>But this isn’t purely about conserving effort. We also tend to believe that these tools have analytical abilities that surpass our own. In some domains, that belief is justified. They can crunch enormous datasets, spot patterns at scale, and perform calculations far beyond human capability.</p><p>The problem is that we extend this assumption into contexts where it doesn’t hold.</p><p>The fact that an algorithm can analyse thousands of medical images doesn’t mean its assessment about an individual patient is infallible. Just because a system can process every CV in a database doesn’t mean it grasps what makes a good hire in a specific team. The leap from “impressive at scale” to “trustworthy in specifics” is one we make far too readily.</p><h3>When trust spirals</h3><p><a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC3240751/"">The evidence on automation bias</a> shows that inexperience and low confidence amplify this effect. When people feel uncertain about their own abilities, or when they’re working under time pressure, they’re more likely to defer to the machine.</p><p>This creates a rather unhelpful loop. <strong>The more we rely on AI, the less we exercise our own judgement. The less confident we become, the more we rely on it.</strong> Keep that cycle spinning long enough, and your decision-making instincts start to gather dust.</p><p>Branding matters too. Studies have found that when users are told they’re working with an “expert system” rather than a basic algorithm, they’re more likely to follow its recommendations — even when those recommendations are wrong. How we label AI directly shapes how much trust users place in it.</p><p>There’s also what researchers call <a href=""https://en.wikipedia.org/wiki/Automation_bias"">“learned carelessness”</a>. When these systems perform well over time, users gradually reduce their vigilance. High reliability leads to complacency. If the system has been right 95% of the time, we stop checking as carefully. And when it stumbles on that remaining 5%, we’re rarely prepared.</p><figure><img alt=""Diagram of a downward spiral with four stages: “Defer to AI” at the top, followed by “Practice Own Judgement Less”, then “Confidence in Own Ability Drops”, and finally “Defer to AI Even More.” The spiral tightens and darkens as it descends into a vortex."" src=""https://cdn-images-1.medium.com/max/1024/1*dUJ8BO0UjkYFStYuUgdqOA.png"" /><figcaption>Image by author</figcaption></figure><h3>More than the sum of its parts</h3><p>For a long time, AI bias and human bias have been treated as separate problems with separate fixes. We try to clean up training data and fine-tune models. We train users to recognise their own blind spots and think more critically. Both approaches matter, but they miss something essential. What emerges from human–AI interaction isn’t simply one plus the other. It’s compound bias. Something new that arises specifically from the interplay between the two.</p><p>Think of it less like an equation and more like a chemical reaction. When certain elements combine, they don’t just sit alongside one another. They interact, producing something neither had before they met.</p><p><a href=""https://arxiv.org/html/2504.18759v1"">A recent paper</a> makes exactly this argument. <strong>Bias doesn’t live solely in the model or in the user. It emerges dynamically through feedback loops that evolve over time, each party making the other a little worse with every exchange.</strong></p><p>Medical AI studies illustrate this clearly. In a <a href=""https://arxiv.org/html/2511.14591v2"">2025 study</a>, researchers examined what happens when class imbalance in AI systems meets human base rate neglect.</p><p>Class imbalance is a technical bias where rare conditions are underrepresented in training data. Base rate neglect is a human cognitive bias where we struggle to reason accurately about low-probability events. Individually, each causes predictable problems.</p><p>Together, they produce something worse.</p><p><strong>Class imbalance throws off people’s ability to gauge how much they should trust the AI. Base rate neglect makes users more vulnerable to skewed outputs.</strong> Each bias reinforces the other, creating feedback loops that lead to poorer outcomes than either would cause alone.</p><p>The errors don’t add up. They multiply.</p><h3>Fixing the wrong problem</h3><p>This is why traditional debiasing strategies often fall short. You can scrub training data and tune algorithms all you like. But if the interaction pattern itself amplifies bias, you haven’t solved the problem. You’ve just trimmed one input to a system that compounds errors regardless.</p><p>The same goes for human-focused interventions. <strong>Teaching users about cognitive bias is valuable, but if the interface encourages offloading and discourages verification, that knowledge stays theoretical.</strong> Filed under “good to know” and rarely retrieved.</p><p>The question shifts. It’s not “is the AI biased?” Or even “are the users biased?” Instead, it goes: “does this interaction create bias amplification over time?”</p><p>That’s a fundamentally different design challenge.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*WTzPaQG-Fs7yjFKMekYRLg.png"" /><figcaption>Image based on Caravaggio’s Narcissus (1597–1599)</figcaption></figure><h3>Mirror, mirror in the interface</h3><p>AI doesn’t just mirror our biases. It amplifies them. Quietly, incrementally, and often invisibly. The problem isn’t confined to training data or human psychology alone. It lives in the space between humans and machines, shaped by how each influences the other over repeated interaction.</p><p>For designers, this reframes what we’re solving for. We can’t simply debias algorithms and dust off our hands. We can’t rely solely on user education. <strong>The bias emerges from the interaction itself. From patterns of trust, deference, and cognitive offloading that compound with every exchange.</strong></p><p>Which brings us to the real question.</p><p>If this amplification happens through everyday interactions with AI, where is it already showing up most strongly in the products we use today?</p><p>The research points to a particular type of interface. One that feels natural, helpful, and conversational. In these interactions, users don’t just follow biased recommendations. They begin to internalise them, carrying those biases forward even after the interaction ends.</p><p>And that’s precisely what makes it so hard to see.</p><blockquote><strong><em>Thanks for reading! 📖</em></strong></blockquote><blockquote><em>If you liked this post, </em><a href=""https://medium.com/@doracee""><em>follow me on Medium</em></a><em> for more.</em></blockquote><h3>References &amp; Credits</h3><p>Glickman, M., &amp; Sharot, T. (2024). <em>How human–AI feedback loops alter human perceptual, emotional and social judgements</em>. <strong>Nature Human Behaviour</strong>. <a href=""https://www.nature.com/articles/s41562-024-02077-2"">https://www.nature.com/articles/s41562-024-02077-2</a></p><p>Sánchez-Amaro, A., &amp; Matute, H. (2023). <em>Humans inherit artificial intelligence biases</em>. <strong>Scientific Reports</strong>, 13, 15737. <a href=""https://www.nature.com/articles/s41598-023-42384-8"">https://www.nature.com/articles/s41598-023-42384-8</a></p><p>Goddard, K., Roudsari, A., &amp; Wyatt, J. C. (2012). <em>Automation bias: a systematic review of frequency, effect mediators, and mitigators</em>. <strong>Journal of the American Medical Informatics Association</strong>, 19(1), 121–127. <a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC3240751/"">https://pmc.ncbi.nlm.nih.gov/articles/PMC3240751/</a></p><p>von Felten, N. (2025). <em>Beyond Isolation: Towards an Interactionist Perspective on Human Cognitive Bias and AI Bias</em>. <strong>CHI 2025</strong>. <a href=""https://arxiv.org/html/2504.18759v1"">https://arxiv.org/html/2504.18759v1</a></p><p><em>Biased Minds Meet Biased AI: How Class Imbalance Shapes Appropriate Reliance and Interacts with Human Base Rate Neglect</em>. (2025). <a href=""https://arxiv.org/html/2511.14591v2"">https://arxiv.org/html/2511.14591v2</a></p><p><em>Automation bias</em>. (2025). <strong>Wikipedia</strong>. <a href=""https://en.wikipedia.org/wiki/Automation_bias"">https://en.wikipedia.org/wiki/Automation_bias</a></p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=af0b514a1314"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/bad-model-behaviour-by-design-af0b514a1314"">Bad (model) behaviour by design</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/the-problem-with-best-practices-in-the-age-of-ai-642987dc9ec9?source=rss----138adf9c44c---4,1771416915,The problem with best practices in the age of AI,"The problem with best practices in the age of AI

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/the-problem-with-best-practices-in-the-age-of-ai-642987dc9ec9?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/642/0*e0rMolDP61o_MdGx"" width=""642"" /></a></p><p class=""medium-feed-snippet"">I showed my team an AI-generated design. Two senior designers called it &#x2018;solid.&#x2019; None of them questioned where it came from.</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/the-problem-with-best-practices-in-the-age-of-ai-642987dc9ec9?source=rss----138adf9c44c---4"">Continue reading on UX Collective »</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/dinosaurs-and-designers-are-underrated-120137cc0221?source=rss----138adf9c44c---4,1771416863,Dinosaurs and designers are underrated,"Dinosaurs and designers are underrated

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/dinosaurs-and-designers-are-underrated-120137cc0221?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/1200/1*4SwGsnldv7l7R1uOOTX0Jw.png"" width=""1200"" /></a></p><p class=""medium-feed-snippet"">A myth from paleontology explains almost everything that hurts.</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/dinosaurs-and-designers-are-underrated-120137cc0221?source=rss----138adf9c44c---4"">Continue reading on UX Collective »</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/why-code-is-not-the-source-of-truth-df456a8d12de?source=rss----138adf9c44c---4,1771357096,Why code is not the source of truth,"Why code is not the source of truth

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/why-code-is-not-the-source-of-truth-df456a8d12de?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/2600/0*kQ8ryjt21G-fcrl7"" width=""4255"" /></a></p><p class=""medium-feed-snippet"">And why AI just made it urgent.</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/why-code-is-not-the-source-of-truth-df456a8d12de?source=rss----138adf9c44c---4"">Continue reading on UX Collective »</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/how-user-segmentation-rather-than-personas-helps-you-get-design-buy-in-6662d0a9a3ec?source=rss----138adf9c44c---4,1771330713,"How user segmentation, rather than personas, helps you get design buy-in","How user segmentation, rather than personas, helps you get design buy-in

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/how-user-segmentation-rather-than-personas-helps-you-get-design-buy-in-6662d0a9a3ec?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/2276/1*p_SpLi1JX-xKYeV1V5rsJA.jpeg"" width=""2276"" /></a></p><p class=""medium-feed-snippet"">Businesses need to know who their user is, and how many there are</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/how-user-segmentation-rather-than-personas-helps-you-get-design-buy-in-6662d0a9a3ec?source=rss----138adf9c44c---4"">Continue reading on UX Collective »</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/getting-carried-away-when-intelligence-is-replaced-by-compliance-f6c63585f7af?source=rss----138adf9c44c---4,1771330652,Getting carried away: When intelligence is replaced by compliance,"Getting carried away: When intelligence is replaced by compliance

<p>Exploring navigation as something bigger than moving through physical and digital spaces — as a fundamental cognitive act</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*X6jNJk6M9uS348ftxZqXGQ.png"" /><figcaption>Shipping the package that’s yourself (Image generated using Procreate and Nanobanana)</figcaption></figure><p>A few weeks ago, I read an article — <a href=""https://www.doc.cc/articles/ai-navigation"">A new navigation paradigm</a> — that felt relatable, yet unsettling in a way I couldn’t fully articulate. I eventually stopped thinking about it, but the ideas lingered in the background. They then surfaced in the most mundane and seemingly unrelated places.</p><p>A few weeks later, as I struggled to submit a loan application online, I realized that my frustration stemmed from more than a technical problem. I had lost the ability to <em>navigate </em>the system.</p><p>Shortly after, while taking an Uber ride to a friend’s place, I caught myself in a moment of reflection. I was moving in the physical space with no attention to my surroundings, completely at the mercy of the driver, who was in turn following the instructions from his phone.</p><p>The kind of navigation I was doing in an Uber didn’t seem all that different from my attempts to navigate the banking system. Navigation suddenly felt bigger — like a fundamental cognitive act.</p><p>In this essay, I examine <strong>navigation in four different realms: institutional, physical, digital, and semantic</strong>. By exploring the commonalities, I hope to convey what’s at stake when we eliminate the <em>friction</em> of navigation, and how to deliberately regain some of it.</p><h3>Navigating the system</h3><p>I am currently attempting to fund a home loan through a major Indian bank that is committed to the idea of a “digital-first” India. As a sign of this commitment, they are now “digital-only” — they now process home loan applications online exclusively, or so I’m told. But the portal is a never-ending obstacle course, almost as if designed to test the user’s patience and persistence. It succeeds phenomenally.</p><p>I’m prompted for identification proofs as PDFs under 1MB or JPEGs under 5MB, but when I provide them, I get an error: <em>“Oops! Something went wrong.”</em> Of course, many things could go wrong. Underneath the digital interfaces is the cross-continent network that makes them all tick — the hundreds of hardware devices the data has to traverse, the tens of thousands of miles of wire crossing land and oceans. Distributed systems are built assuming ephemeral failures. But when they happen consistently, it signals something systemic.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*caGp-0y1osvEHv4av2lxqg.png"" /><figcaption>“Oops! Something went wrong.” Error while submitting documents on a banking portal</figcaption></figure><p>You know the problem is prevalent when it becomes material for comedy. Biswas Kalyan Rath, an Indian stand-up comedian, <a href=""https://youtu.be/PK_zuAuI9x0?si=szB908spPScog_k6&amp;t=64"">has a good bit about the state of “digital-first” marriage registration in India</a>.</p><blockquote>“Marriage, no problem. Very simple. Getting marriage registered.. Big problem. It was always a problem, now there is the illusion of Digital India. There is a website. You can go and register. No problem. Fill the entire form. The “Submit” button does not work.”</blockquote><p>I cracked up when I watched it. I’m not laughing now. I’m three weeks into the loan application, and I need it approved.</p><p>The deeper problem isn’t the bad website. It’s what it does to its users. I understand technology, but the bank’s application is a black box to me. The loan agent is an expert in the process but has proven useless. He’s as much a spectator in this as I am. I’m not merely dependent on the website; I’m trapped between technology that’s not done well and a human who doesn’t understand it well. We’ve both lost the ability to <em>navigate</em> the system.</p><h3>Navigating the physical world</h3><p>During my childhood, when I traveled by bus to my maternal hometown — Ichalkaranji, Maharashtra’s textile hub — we could tell we had arrived by the pungent smell and rhythmic din of the cotton mills. We could tell the time remaining by the landmarks we passed — the restaurants, the trees, the market, the temples, the stadium. Knowing a destination meant knowing its spatial signature. We had to read the surroundings to know when we had arrived. Navigation was an act of observation. Even the act of describing a destination meant being deeply aware of its characteristics. We saw a place as a point in space, situated relative to other places.</p><p>Then came paper maps, which provided a way to abstract the physical reality and capture its essence in symbols. It was a leap forward, but still required us to compare the map against the territory — active interpretation. The map acted as external memory. The processing still happened in your brain. In software terms, this was the separation of storage and compute. The maps only told you the layout. You had to orient yourself against it.</p><p>Then Google Maps outsourced “compute” too.</p><p>It wasn’t just paper maps on a screen. It offered a fundamentally different technology: <em>active navigation</em>. Active interpretation on the user’s part gave way to the step-by-step guidance from the software. It not only knew the landscape; it knew your place in it. You just had to trust it. Spatial navigation became a skill you no longer needed. When Google Maps tells me to “take the next exit and stay on the second to the right lane”, I mindlessly execute.</p><p>What navigation demands from me now is not intelligence but compliance.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*2hg4yjIZHevidm3IrPtAhw.png"" /><figcaption>Navigating by compliance (Image generated using Procreate and Nanobanana)</figcaption></figure><p>I’m as much a victim as a culprit of this de-skilling that has happened at scale. Due to my overreliance on technology, I don’t remember the street names in my neighborhood anymore, despite having lived in the same place for over two years. It speaks to how little I pay attention to the world around me when the blue arrow and the friendly voice are leading the way.</p><p><a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC7156656/"">A 2020 longitudinal study</a> tracked regular drivers over three years and found that people with greater lifetime GPS experience demonstrated reduced spatial memory during self-guided navigation. It found not just correlation, but causality of GPS use leading to decline in spatial memory.</p><h3>Getting carried away</h3><p>Compare this to the Uber model.</p><p>You set the destination, a driver accepts, you accept, and off you go. One of the “benefits” of Uber is that it relieves you of the mundane act of driving. Your attentional resources, the logic goes, can be better spent on “more important” needs — like scrolling social media on your phone instead of greeting the human three feet away. Thanks to self-driving cars, we’ve even eliminated that guilt, if there ever was.</p><p>Somewhere in that scroll, you lose track of time, of the route, of your environment. You’re physically there, but mentally elsewhere. The only active role you’ve played is scheduling the pick-up and the drop-off. You’ve shipped the package that is yourself.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*k0y481sV4ABUh7-lWm-1RA.png"" /><figcaption>Uber pickup and drop-off</figcaption></figure><h3>Navigating the digital world</h3><p>Consider a typical day using digital interfaces. You log in to your computer or your phone, you find the application you need, navigate through its menus and screens, do what you want to do, close it, move on to the next app. It’s essentially an endless loop of navigation until you log off and go back to the real world.</p><p>This is akin to navigation in the pre-GPS world. There is a certain map of the world — the software’s user interface — and you have to learn it. You “onboard” yourself, and navigate it by paying attention. You are spatially aware of the world that the software has created for you. You know what capabilities it has, or where and how to find them.</p><p>We are increasingly moving towards an intent-based navigation system, or as I think of it, “Uber-ification” of the digital world. Intent-based software comes in two flavors: search-first, and AI-first.</p><p>In search-first applications, you search for keywords, say, “expense report” and it’ll show you potential actions like “View your expense reports” or “File a new expense report”. You select the one you want without knowing where it lives in the application. The geography doesn’t matter.</p><p>In AI-first applications, it’s even more extreme. You declare the intent — “File my expenses” — and the agent handles the rest. You’ve achieved your goal without ever knowing where the expense module is housed in the software, or even how to even file one yourself.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/727/1*Z3AwXnIZWn1Ceh_Qq9795Q.png"" /><figcaption>Intent-driven: Search-first</figcaption></figure><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/743/1*0Z9bFsL-7NjcpvzCWKIg1Q.png"" /><figcaption>Intent-driven: AI-first</figcaption></figure><p>There’s a plethora of intent-based software now, with AI embedded in them, eager to translate your intent into outcomes. Traditional navigation is gradually converging into a ubiquitous conversational interface.</p><p>Notion AI promises to write your docs for you. Cursor and Claude promise to write your code. Glean <a href=""https://outofdesk.blog/blog/performance-reviews-llms"">writes your performance reviews</a> — the very document that requires sitting with the uncomfortable and hard work of assessing your own year, your team’s contributions, their strengths and weaknesses in your own words. Gemini writes your emails.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/799/1*aJHyzWcesP9jtTOHoRDX8g.png"" /><figcaption>Performance review agent — Generates self-review with a mere prompt. No reflection required</figcaption></figure><p>Each of these <em>smart</em> apps makes you more productive. No navigation required. No blank page to face. No empty file with a cursor blinking. Information architecture — the design of where things live and how they connect — no longer matters. Not to you, at least. The agent navigates for you.</p><h3>Beyond movement: Navigation as a metacognitive skill</h3><p>Consider what happens when you navigate without GPS in an unfamiliar city. You <strong>assess</strong> your current location, <strong>identify</strong> your destination, <strong>survey</strong> available information — landmarks, street signs, smell, sounds, <strong>hypothesize</strong> about which direction to go, <strong>make decisions</strong> with incomplete information, <strong>monitor</strong> feedback as you move, <strong>adjust</strong>, when you encounter obstacles, wrong turns, or dead ends, and <strong>learn</strong> from the experience, building a richer understanding of the territory.</p><p>Now consider what you do when you’re trying to understand a topic, say, climate change. You <strong>assess</strong> what you currently know or understand, <strong>identify</strong> what you need to know, <strong>survey</strong> available information — documentation, courses, articles, real world case-studies, papers, mental models, <strong>hypothesize</strong> about how it might work, <strong>make decisions</strong> about which path to explore first with incomplete information, <strong>monitor</strong> feedback as you learn, <strong>adjust</strong> your mental model when you hit obstacles, and <strong>learn</strong>, building a richer, more accurate understanding of the system.</p><p>Problem-solving is navigation. Learning is navigation. Decision-making is navigation. You’re moving through space — either real or conceptual — through uncertainty, towards clarity, understanding, destination, or outcomes.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*d711JP8zbOaFo9j4j8lQUQ.png"" /><figcaption>Navigating to clarity (Image generated using Procreate and Nanobanana)</figcaption></figure><p>This isn’t a metaphor. <a href=""https://iiif.library.cmu.edu/file/Simon_box00065_fld05031_bdl0001_doc0001/Simon_box00065_fld05031_bdl0001_doc0001.pdf"">Herbert Simon’s work</a> describes problem-solving as “search through a problem-space.” <a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC5248972/"">Neuroscience has also shown that the same neural systems(the hippocampus, place, and grid cells)</a> support both physical and conceptual navigation. What makes something navigation isn’t whether it moves through a physical space, but rather that it requires relational mapping, position assessment, route planning, and dynamic reorganization.</p><p>So we are navigating pretty much all the time. And it’s becoming easier with technology. So what?</p><p>Just as GPS use causes a measurable decline in our spatial memory, AI tools that navigate conceptual spaces for us pose the same risk in a different, arguably more critical, domain.</p><h3>The Pharmakon and the problem with frictionless</h3><p>Looking at technological innovations through the lens of <a href=""https://aeon.co/essays/bernard-stieglers-philosophy-on-how-technology-shapes-our-world""><em>pharmakon</em></a> — both remedy and poison — remedies are clearly visible: speed, efficiency, productivity, convenience. Even when they’re not, organizations are happy to sell us on them.</p><p>The poisons are more subtle: dependency, agency, losing the cognitive friction that maintains our capacity to navigate. The agent absorbs the cost of navigation, but it also becomes the locus of agency.</p><p>Two risks that stand out, and are worth thinking about:</p><p><strong>The transfer of agency</strong>: By shifting the agency of navigation to the software, you end up being at the mercy of the navigator. <a href=""https://outofdesk.blog/performance-reviews-llms"">If the AI routes you, you stop thinking about alternatives.</a> The convenience is paid for with a little bit of your money and a lot more of your agency. With the erasure of navigation, Information Architecture gives way to Choice Architecture.</p><p>This might be fine for filing tax returns. Humanity would benefit from spending less time doing that. But what happens when the same happens with critical thinking? Comprehension? Reflection? Decision-making?</p><p><strong>The vacuum of attention</strong>: With the time and attention you’ve regained from not navigating, you need something meaningful to do. Few people know what to do with that kind of freedom. They look to technology to fill the void. But the incentives of technology companies, more often than not, don’t align with the well-being of the user, especially when the way to make money and user interests are at odds.</p><p>When you’re not navigating and also not driving, there’s the promise of reflecting, reading, thinking, writing, or building. But first, why not catch up on what your friends are doing? Or the Kardashians? Or millions of strangers?</p><p>It presents a dark irony: We save time by delegating things to technology, only to pour that time back into it, mindlessly scrolling.</p><h3>Solutions: For victims and culprits</h3><p>I want to think about solutions to this problem, both as a victim — a consumer and a <em>navigator</em> — and a culprit — someone who builds technology products, and <em>designs </em>navigation.</p><p>I say culprits not because they have bad intentions, but likely because they either lack any or they are acting on misaligned incentives.</p><h3>Micro-frictions as the antidote for the victim</h3><p>I’m learning to be more intentional about what friction I want to eliminate and what I want to preserve. About what navigation I want to do myself versus delegate.</p><p>I use AI every day. It has largely empowered me. But I’m deliberate in its use.</p><p>I resist the temptation to start with AI-generated drafts because the value of writing lies in the struggle to articulate the original thought. It’s that struggle that clarifies what you want to say, what you want to achieve. Only then do I use AI. My process isn’t simply sequential, so the handoff isn’t a clear line. It’s a feedback loop. But the bulk of my effort goes into the pre-AI phase, ensuring that I build a vision for the artifact that I want to produce — whether it’s an essay or software. I can then direct it rather than being led by it.</p><p>For places I visit regularly, I’ve stopped using maps. I’m rebuilding the habit of paying attention to my surroundings.</p><p>I keep my phone away when I’m with my kids, both to be present with them and implicitly teach them the value of paying attention. I find pockets of time severed from technology — sipping coffee, going on walks — which builds the habit of being comfortable alone with my thoughts.</p><p>How is this all related to navigation, you say? Intentionality brings clarity, which in turn helps you navigate effectively.</p><p>If you want to read more about this idea of introducing friction, this article from The Cut captures it well: <a href=""https://www.thecut.com/article/brooding-friction-maxxing-new-years-2026-resolution.html""><em>In 2026, We Are Friction-Maxxing</em></a>.</p><h3>Design principles for technologists as culprits</h3><p>This might sound obvious, but when designing technology products, it helps to keep users’ needs <em>and well-being</em> in mind.</p><p><strong>Build robust fallback mechanisms:</strong> In the banking portal, when the “digital-only” system failed, neither the customer nor the employee could navigate an alternative path. The system had eliminated all other routes.</p><p>Before implementing AI assistance or intent-based shortcuts, think about whether the user can complete the task if the feature breaks or is unavailable. Build fallback mechanisms that allow manual navigation when automated paths fail, surface error states that help users understand what went wrong, provide enough context for users to troubleshoot independently, and maintain alternative paths to the same destination.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*tGOrUoGbx8Q4_Dqgx6_wrw.png"" /><figcaption><a href=""https://www.intercom.com/help/en/articles/9356221-deploy-fin-ai-agent-over-email"">Fin agent fallback</a></figcaption></figure><p><strong>Distinguish between transactional and developmental tasks:</strong> When developing automations, distinguish between <strong>transactional tasks</strong> (filing taxes, scheduling meetings, data entry) and <strong>developmental tasks</strong> (writing, learning, problem-solving, decision-making). This is similar to the idea of distinguishing between <a href=""https://outofdesk.blog/thinking-outside-the-doc"">competitive artifacts and complementary artifacts</a>.</p><p>Transactional tasks should be frictionless. These are tasks where efficiency is the primary goal, and there’s little value in the process itself. But developmental tasks benefit from <a href=""https://every.to/thesis/in-the-ai-age-making-things-difficult-is-deliberate"">good friction</a>. The process of navigating uncertainty is often where the real value lies.</p><p><a href=""https://danielmiessler.com/blog/keep-the-robots-out-of-the-gym"">“Gym skills”</a> is a good metaphor. For developmental tasks, consider AI as a collaborator that augments rather than replaces the user’s cognitive work. <a href=""https://www.mozillafoundation.org/en/blog/the-importance-of-friction-in-ai-systems/"">Speculative and Critical Design (SCD)</a> framework lays out one of the most articulate ways to think about this.</p><p><strong>Adjust assistance based on expertise:</strong> Design systems that adjust assistance levels based on user expertise, because effective delegation requires good judgment on the user’s part.</p><p>Beginners benefit from more guidance, explanations, and visible structure to help them build mental models of the system. Intermediate users can make effective use of shortcuts and intent-based features, but in a way that traditional navigation is visible and accessible. Expert users demand customizability, creating their own navigation paths, or even disabling assistance they don’t need.</p><p>The problem with many AI-first products is that they apply the same level of automation to everyone, preventing users from ever developing expertise. A newly minted manager using AI to completely delegate the work of crafting performance reviews for the team, for example, isn’t going to build the muscle of reflecting on the relationships with people, paying attention to their motivations, strengths, and weaknesses.</p><p><strong>Preserve visible structure:</strong> Even with search-first or AI-first interfaces, users benefit from understanding the underlying structure. Consider <strong>visual sitemaps or navigation breadcrumbs</strong> that show users where they are in the system, <strong>contextual hints</strong> about where information lives, even when delivering it through search, and <strong>progressive disclosure</strong> that reveals structure as users engage more deeply with the product.</p><p><a href=""https://www.aiuxdesign.guide/"">aiuxdesign.guide</a> has a comprehensive set of design patterns that mention some of these and more, with real-world examples.</p><h3>Conclusion</h3><p>There are a few pockets of life that haven’t been taken over by assistive and active technology. Building the muscle to tolerate (and design) good friction and (help) reclaiming the ownership of your attention are good starts to prepare for what’s ahead. There’s little I can do to navigate the banking system. But there are other areas I can exercise control, both as a creator and a consumer.</p><p><em>Find me on </em><a href=""https://www.linkedin.com/in/ggauravr/""><em>LinkedIn</em></a><em>, </em><a href=""https://x.com/outofdesk""><em>X</em></a><em>, or check out my </em><a href=""https://outofdesk.blog/""><em>blog here</em></a><em>.</em></p><h3>Inspirations and references</h3><ul><li><a href=""https://www.doc.cc/articles/ai-navigation"">DOC • A new navigation paradigm</a></li><li><a href=""https://signalpath.substack.com/p/do-ai-products-even-need-navigation"">Do AI Products Even Need Navigation? — by Andrew Sims</a></li><li><a href=""https://aeon.co/essays/bernard-stieglers-philosophy-on-how-technology-shapes-our-world"">Bernard Stiegler’s philosophy on how technology shapes our world | Aeon Essays</a></li><li><a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC7156656/"">Habitual use of GPS negatively impacts spatial memory during self-guided navigation — PMC</a></li><li><a href=""https://youtu.be/PK_zuAuI9x0?si=szB908spPScog_k6&amp;t=64"">Biswa Kalyan Rath | Birth Proof | Stand Up Comedy</a></li><li><a href=""https://www.thecut.com/article/brooding-friction-maxxing-new-years-2026-resolution.html"">In 2026, We Are Friction-Maxxing</a></li><li><a href=""https://iiif.library.cmu.edu/file/Simon_box00065_fld05031_bdl0001_doc0001/Simon_box00065_fld05031_bdl0001_doc0001.pdf"">THE THEORY OF PROBLEM SOLVING</a></li><li><a href=""https://pmc.ncbi.nlm.nih.gov/articles/PMC5248972/"">Organizing Conceptual Knowledge in Humans with a Grid-like Code — PMC</a></li><li><a href=""https://asana.com/ai-principles"">Asana — Our principles for helping humanity thrive with AI</a></li><li><a href=""https://www.psychoftech.org/blog/2020/3/10/cognitive-artifacts"">Cognitive Artifacts — Psychology of Technology Institute</a></li><li><a href=""https://every.to/thesis/in-the-ai-age-making-things-difficult-is-deliberate"">In the AI Age, Making Things Difficult Is Deliberate | Every</a></li><li><a href=""https://www.notion.com/blog/the-design-thinking-behind-notion-ai"">The Thinking Behind Notion AI</a></li><li><a href=""https://www.mozillafoundation.org/en/blog/the-importance-of-friction-in-ai-systems/"">The Importance of Fricton in AI Systems | Mozilla Foundation</a></li><li><a href=""https://hbr.org/2022/06/why-ai-customer-journeys-need-more-friction"">Why AI Customer Journeys Need More Friction | HBR</a></li><li><a href=""https://www.aiuxdesign.guide/"">AI UX Design Patterns</a></li></ul><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f6c63585f7af"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/getting-carried-away-when-intelligence-is-replaced-by-compliance-f6c63585f7af"">Getting carried away: When intelligence is replaced by compliance</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
