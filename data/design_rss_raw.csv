source,domain,url,created_utc,title,text
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/rethinking-pixel-perfect-web-design/,1768903200,Rethinking â€œPixel Perfectâ€ Web Design,"Rethinking â€œPixel Perfectâ€ Web Design

Amit Sheen takes a hard look at the â€œPixel Perfectâ€ legacy concept, explaining why itâ€™s failing us and redefining what â€œperfectionâ€ actually looks like in a multi-device, fluid world."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/smashing-animations-part-8-css-relative-colour/,1768384800,Smashing Animations Part 8: Theming Animations Using CSS Relative Colour,"Smashing Animations Part 8: Theming Animations Using CSS Relative Colour

CSS relative colour values are now widely supported. In this article, pioneering author and web designer [Andy Clarke](https://stuffandnonsense.co.uk/) shares practical techniques for using them to theme and animate SVG graphics."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/ux-product-designer-career-paths/,1768212000,UX And Product Designerâ€™s Career Paths In 2026,"UX And Product Designerâ€™s Career Paths In 2026

How to shape your career path for 2026, with decision trees for designers and a UX skills self-assessment matrix. The only limits for tomorrow are the doubts we have today. Brought to you by <a href=""https://smart-interface-design-patterns.com/"">Smart Interface Design Patterns</a>, a **friendly video course on UX** and design patterns by Vitaly."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/penpot-experimenting-mcp-servers-ai-powered-design-workflows/,1767859200,Penpot Is Experimenting With MCP Servers For AI-Powered Design Workflows,"Penpot Is Experimenting With MCP Servers For AI-Powered Design Workflows

[Penpot](https://penpot.app/?utm_source=SmashingMagazine&amp;utm_medium=Article&amp;utm_campaign=MCPserver) is experimenting with MCP (Model Context Protocol) servers, which could lead to designers and developers being able to perform tasks in Penpot using AI thatâ€™s able to understand and interact with Penpot design files. Daniel Schwarz explains how [Penpot MCP](https://github.com/penpot/penpot-mcp) servers work, what they could mean for creating and managing designs in Penpot, and what you can do to help shape their development."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/pivoting-career-without-starting-from-scratch/,1767780000,PivotingÂ Your Career Without Starting From Scratch,"PivotingÂ Your Career Without Starting From Scratch

Most developers spend their days fixing bugs, shipping features, and jumping into the next sprint without even thinking about it. After a while, you begin to ask yourself, â€œIs this still what I want to be doing?â€ This article looks at how you can move into a new direction in your career without starting from scratch, and how the skills you already use, like problem-solving, communication, and empathy, can open new doors."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/12/desktop-wallpaper-calendars-january-2026/,1767171600,Countdown To New Adventures (January 2026 Wallpapers Edition),"Countdown To New Adventures (January 2026 Wallpapers Edition)

Whether 2026 has already begun as youâ€™re reading this or youâ€™re still waiting for the big countdown to start, how about some new wallpapers to get your desktop ready for the new year? Weâ€™ve got you covered."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/12/how-design-for-with-deaf-people/,1767088800,How To Design For (And With) Deaf People,"How To Design For (And With) Deaf People

Practical UX guidelines to keep in mind for 466 million people who experience hearing loss. More design patterns in <a href=""https://smart-interface-design-patterns.com/"">Smart Interface Design Patterns</a>, a **friendly video course on UX** and design patterns by Vitaly."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/12/giving-users-voice-virtual-personas/,1766484000,Giving Users A Voice Through Virtual Personas,"Giving Users A Voice Through Virtual Personas

Turn scattered user research into AI-powered personas that give anyone consolidated multi-perspective feedback from a single question."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/12/how-measure-impact-features-tars/,1766138400,How To Measure The Impact Of Features,"How To Measure The Impact Of Features

Meet TARS â€” a simple, repeatable, and meaningful UX metric designed specifically to track the performance of product features. Upcoming part of the <a href=""https://measure-ux.com/"">Measure UX &amp; Design Impact</a> (use the code ğŸŸ <code>IMPACT</code> to save 20% off today)."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/12/smashing-animations-part-7-recreating-toon-text-css-svg/,1765965600,Smashing Animations Part 7: Recreating Toon Text With CSS And SVG,"Smashing Animations Part 7: Recreating Toon Text With CSS And SVG

In this article, pioneering author and web designer [Andy Clarke](https://stuffandnonsense.co.uk) shows his techniques for creating [Toon Text titles](https://stuffandnonsense.co.uk/toon-text/index.html) using modern CSS and SVG."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/12/accessible-ux-research-ebook-release/,1765296000,"Accessible UX Research, eBook Now Available For Download","Accessible UX Research, eBook Now Available For Download

Weâ€™ve got exciting news! eBook versions of â€œAccessible UX Research,â€ a new Smashing Book by Michele A. Williams, are now available for download! Which means soon the book will go to the printer. Order the eBook for instant download now or <a href=""https://www.smashingmagazine.com/printed-books/accessible-ux-research/"">reserve your print copy at the presale price.</a>"
rss,smashingmagazine.com,https://smashingmagazine.com/2025/12/state-logic-native-power-css-wrapped-2025/,1765274400,"State, Logic, And Native Power: CSS Wrapped 2025","State, Logic, And Native Power: CSS Wrapped 2025

CSS Wrapped 2025 is out! Weâ€™re entering a world where CSS can increasingly handle logic, state, and complex interactions once reserved for JavaScript. Here is an unpacking of the standout highlights and how they connect to the bigger evolution of modern CSS."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/12/how-ux-professionals-can-lead-ai-strategy/,1765180800,How UX Professionals Can Lead AI Strategy,"How UX Professionals Can Lead AI Strategy

Lead your organizationâ€™s AI strategy before someone else defines it for you. A practical framework for UX professionals to shape AI implementation."
rss,uxdesign.cc,https://uxdesign.cc/what-makes-generated-ui-worth-keeping-96b44ade04a1?source=rss----138adf9c44c---4,1768993424,What makes generated UI worth keeping?,"What makes generated UI worth keeping?

<h4><strong><em>How brand, data, and reuse help ideas move past the demoÂ phase</em></strong></h4><figure><img alt=""Music platform screens vibe coded with various AI tools like Anima and Uizard"" src=""https://cdn-images-1.medium.com/max/1024/1*IZ6fOsQkSAw4pHD1iXAFuA.png"" /><figcaption><em>Music platform screens vibe coded with various AI tools like Anima andÂ Uizard</em></figcaption></figure><p>As designers, we know there are many tools at our disposal to create AI-generated UI, like Lovable, Anima, and Uizard. There has been a lot of progress in this space, with AI output at unthinkable speeds and of unprecedented quality. But stillâ€¦most of it feels short-lived and unusable.</p><p><strong>The generated screens look convincing, but theyâ€™re treated as placeholders instead of starting points in the ideation process</strong>. For instance, when a generated screen needs to reflect a brandâ€™s styling, handle real data, or align with existing patterns, the work has to be rebuilt (and the generated UI isÂ tossed).</p><p>This is simply a mismatch between how UI is generated in AI tools and how products are actually built. So instead of avoiding constraints such as branding and data, they need to be integrated into the generation process. This way, the UI becomes more valuable and can be used to continue to iterate past a demo or proof of concept (POC)Â phase.</p><p>Letâ€™s look at 3 constraints (brand styles, data, and pattern reuse) that separate generated UI that gets discarded versus UI that is used end-to-end in the product development cycle</p><h3>Start with a brand versus restyling later</h3><p>AI-generated screens typically use visual styles that are neutral, like system-default colors and typography. Though the UI feels â€œclose enough,â€ itâ€™s not truly accurate to your productâ€™s brand (or any other brand you want to test). And once real branding is introduced, the UI designs break down from the gaps created from the neutral styling used during generation.</p><p><strong>This ultimately causes rework to better incorporate the productâ€™s branding (either restyling elements or starting over completely)</strong>. To avoid this issue, brand alignment must become part of the initial AI prompt versus being considered as extra steps to fix in laterÂ stages.</p><figure><img alt=""UI generated by Anima that is inspired by the Airbnb brand and AI prompting"" src=""https://cdn-images-1.medium.com/max/1024/1*IJ-5YTLl4BBmz8LZKf_17g.png"" /><figcaption><a href=""https://www.youtube.com/watch?v=1lMHFWD8I3E""><em>Anima generates UI inspired by a brand</em></a><em> (like Airbnb) from the first AI prompt, and can make modifications to individual elements asÂ needed</em></figcaption></figure><p>But generating UI with brand context doesnâ€™t eliminate design judgmentâ€“ it more so reduces how much work gets discarded. When generated UI reflects a real brand early in the design or ideation process, it becomes something teams can refine rather thanÂ replace.</p><h4>How to integrate branding and generated UI</h4><p>Many of the AI tools we already know are beginning to account for specific brand inclusion:</p><ul><li><strong>Anima</strong>: With <a href=""https://dev.animaapp.com/"">Anima Playground</a>, you can generate an application using the visual language of an existing brand. Just write the app you want to build, along with the brand to use as inspiration (from your own brand to Airbnbâ€™s). You can also refine individual UI elements or adjust inline copy without regenerating the designs again andÂ again.</li><li><strong>Uizard</strong>: <a href=""https://uizard.io/autodesigner/"">Uizardâ€™s Auto-designer</a> allows you to write a text-based prompt, then select a style (from a screenshot, URL, or brand kit). After the designs are generated, you can modify elements and generateÂ themes.</li><li><strong>Google Stitch</strong>: Similar to the other tools, <a href=""https://stitch.withgoogle.com/"">Google Stitch</a> generates screens based on your prompt, which can include brand styling. Stitchâ€™s output is solid, but you cannot modify individual elements; you can only use an entire screen as reference for modification requests.</li></ul><figure><img alt=""UI generated by Google Stitch that is based on the prompted brand"" src=""https://cdn-images-1.medium.com/max/1024/1*0HsQrO4-IZeMRk-WU1V6-g.png"" /><figcaption><em>Google Stitch generates screens based on the requested brand, but can only modify entire screens versus singleÂ elements</em></figcaption></figure><blockquote><strong>Note</strong><em>: Some of the tools, like Anima, generate the UI with flexibility and can be restyled with </em><a href=""https://www.youtube.com/watch?v=1lMHFWD8I3E""><em>different branding inspirations</em></a><em> without rebuilding the entire layout. So you can easily compare visual directions or get stakeholder feedback.</em></blockquote><h4>Benefits of branding from theÂ start</h4><ul><li><strong>Faster alignment</strong>: Introducing brand context early reduces the gap between initial concepts and stakeholder expectations (shifting feedback away from styling and toward designÂ intent)</li><li><strong>Comparison between directions</strong>: Applying different brand styles to the same layout makes it easy to evaluate options without redesigning the experience eachÂ time</li><li><strong>Less restyling later</strong>: When typography, colors, and tone are set from the start, teams spend less time retrofitting as the designÂ evolves</li></ul><h4>Limits of branding with AIÂ tools</h4><ul><li><strong>Surface-level branding</strong>: AI-driven branding captures visual cues like color and type, but doesnâ€™t replace a deeper DesignÂ System</li><li><strong>Accessibility and content review</strong>: Generated UI still needs to be manually reviewed and validated by designers for accessibility and content standards</li></ul><h3>Apply real data to reveal if the UI actuallyÂ works</h3><p>Screens that have been generated with AI typically use mock data as placeholder content. Though mock data helps teams understand the design idea, it also masks the hardest UX problems. Not every user goes through the ideal, â€œhappy pathâ€â€¦they will encounter empty states, error flows, and permission rules.</p><p><strong>These unexpected, but common scenarios donâ€™t show up in static screens, which means early designs can give a false sense of readiness</strong>. Without real data and content driving interactions, product teams discover these gaps late in the design and development process.</p><figure><img alt=""Database feature in Anima to view and edit real data for your app"" src=""https://cdn-images-1.medium.com/max/1024/1*lWxdv2nCddjR-rjiLJXFYw.png"" /><figcaption>Anima automatically creates a database with editable data to support yourÂ project</figcaption></figure><p>Though this doesnâ€™t replace user-centered design thinking, it ensures ideas are grounded in authentic user behavior from the beginning of the design process. So by integrating real data into the generated UI, teams can see whether interactions work, content fits, and user permissions behave as expected. This is especially important when features rely on user input, authentication, or content submission.</p><h4>How to include real data with generated UI</h4><p>With the right AI tools, you can replace placeholder content for realÂ data:</p><ul><li><strong>Anima</strong>: You or your team can describe the data a feature needs in Animaâ€™s Playground text prompt, and the system will generate databases and/ or tables based on the instructions. The data will be connected to the project and will automatically come with <a href=""https://www.animaapp.com/blog/product-updates/built-in-databases-for-anima-playground-zero-config-backend/?ref=fmt-handpicked"">default security policies</a> that match the intendedÂ usage.</li><li><strong>Cursor</strong>: With <a href=""https://cursor.com/"">Cursor</a>, you can integrate your codebase (GitHub or GitLab) to give AI more context, as well as <a href=""https://www.youtube.com/watch?v=TfkpZzs04-I"">connect MySQL database</a> to create elements likeÂ tables.</li></ul><figure><img alt=""Manually configuring a database via extension to the Cursor tool"" src=""https://cdn-images-1.medium.com/max/1024/1*0XQGV9bKFQb8nVp8VgxWGQ.png"" /><figcaption><em>Cursor requires manual configuration to connect a database via extension to the AIÂ agent</em></figcaption></figure><blockquote><strong>Note</strong><em>: With Anima, just about anyone can create a database (even with no code knowledge). On the other hand, Cursorâ€™s MySQL setup process is more technical and hands-on.</em></blockquote><h4>Benefits of using realÂ data</h4><ul><li><strong>Discover problems early</strong>: Testing with real content surfaces UX gaps (like validation and edge-cases) before they become costly reworkÂ later</li><li><strong>Clear sense of readiness</strong>: When screens behave with actual data, itâ€™s easier to know whatâ€™s ready for iteration versus whatâ€™s purelyÂ visual</li><li><strong>Reduced throwaway</strong>: By grounding UI in common behavior from the start, teams rely less on static mockups that will probably be discarded</li></ul><h4>Limits of integrating realÂ data</h4><ul><li><strong>Developer input</strong>: Complex tables, queries, or structured data often need manual configuration beyond what AI canÂ create</li><li><strong>Premature focus on data</strong>: Focusing on data structure before validating broader design concepts can distract from testing core userÂ flows</li></ul><h3>Reuse and build off familiarÂ patterns</h3><p>Many product teams rely on established UI components and patterns that AI ignores when generating screens. So teams have to rebuild or replicate familiar components, which adds unnecessary work and risks inconsistency.</p><p><strong>When screens donâ€™t leverage existing assets or common interaction patterns, they wonâ€™t integrate with the rest of the product ecosystem</strong>. When AI can build off trusted patterns and components from the start, the generated UI becomes more durable, consistent, and easier toÂ refine.</p><figure><img alt=""Animaâ€™s Chrome extension lets you capture any kind of UI element on a live website"" src=""https://cdn-images-1.medium.com/max/1024/1*Hp-9Z4oohRw0brznSurstQ.png"" /><figcaption><a href=""https://chromewebstore.google.com/detail/web-to-code-by-anima/paddhneaanoeljlmdepnheehdkaegblo""><em>Animaâ€™s extension</em></a><em> lets you capture any kind of UI element on a live website, as well as manage the elements in the rightÂ rail</em></figcaption></figure><h4>How to reuse what alreadyÂ exists</h4><p>Tools that allow you to grab existing elements to use in your generated UI:</p><ul><li><strong>Anima</strong>: In <a href=""https://chromewebstore.google.com/detail/web-to-code-by-anima/paddhneaanoeljlmdepnheehdkaegblo"">Animaâ€™s Chrome extension</a>, you can capture UI components directly from live websites using the Clipboard feature. You can select individual elements or entire &lt;div&gt; containers, then mix them with your generated screens to combine your ideas with proven patterns.</li><li><strong>YoinkUI</strong>: With the <a href=""https://chromewebstore.google.com/detail/yoinkui/ihlkclcengelgcfkkmpkhgadepmgijkk"">YoinkUI Chrome extension</a>, you can select any UI element from a live website, modify it in the YoinkUI editor, then export it to use it in your project. Similar to Anima, you can mix and match your components with existing patterns.</li><li><strong>KwikUI</strong>: KwikUI has a <a href=""https://www.kwikui.com/chrome-extension"">KwikSnap Chrome extension</a> that lets you screenshot webpages to web elements. After screenshotting, you can convert the images into coding prompts within KwikUI for more context-based output.</li></ul><figure><img alt=""YoinkUIâ€™s Chrome extension lets you select and save elements from live sites"" src=""https://cdn-images-1.medium.com/max/1024/1*0yU5q6lxr8LSr21ch5-QMA.png"" /><figcaption><a href=""https://chromewebstore.google.com/detail/yoinkui/ihlkclcengelgcfkkmpkhgadepmgijkk""><em>YoinkUIâ€™s extension</em></a><em> lets you select and save single to grouped elements from any live website, likeÂ </em><a href=""https://www.airbnb.com/""><em>Airbnb</em></a></figcaption></figure><blockquote><strong>Note</strong><em>: After capturing UI elements from the site of your choosing, you can select the Clipboard Gallery button in Anima Playgroundâ€™s prompt to insert them into your generated screens.</em></blockquote><figure><img alt=""Insert captured elements into Anima Playgroundâ€™s generated screens"" src=""https://cdn-images-1.medium.com/max/1024/1*l7uEriwp1pss7WmedPH7fA.png"" /><figcaption><em>Insert your captured elements into Anima Playgroundâ€™s generated screens to recycle real UI components andÂ patterns</em></figcaption></figure><h4>Benefits of reusing familiarÂ patterns</h4><ul><li><strong>Faster iteration</strong>: Designers can leverage existing UI and patterns, which reduces the need to recreate components fromÂ scratch</li><li><strong>Continuity across projects</strong>: Reused components help maintain a consistent look and feel; even generated and manual UI areÂ combined</li><li><strong>Reduced cognitive load</strong>: Familiar patterns make it easier to evaluate new screens and maintain predictable interactions (for both product teams and endÂ users)</li></ul><h4>Limits of reusingÂ patterns</h4><ul><li><strong>Captured UI may need adaptation</strong>: Elements from other sites may require cleanup or adjustment to fit the current projectâ€™s needs</li><li><strong>Legal and licensing</strong>: Reusing UI from external sources may require extra consideration to copyright or usageÂ rules</li></ul><h3>Conclusion</h3><p>Generated UI begins to show actual value when it holds up under real-world constraints; not simply from its delivery speed or trendiness. Brand alignment, real data, and reuse makes early concepts sturdy, reliable, and less prone to be thrown away. These 3 constraints allow product teams to continue to iterate on generated UI instead of regenerating or starting fromÂ scratch.</p><p><strong>As AI tools like Anima and Lovable continue to evolve, their progress will be shown in how usable their output is past the demo phase</strong> (rather than producing work that will be discarded). Because when AI-generated work fits into product workflows and design constraints, it will start becoming part of the product development cycle.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=96b44ade04a1"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/what-makes-generated-ui-worth-keeping-96b44ade04a1"">What makes generated UI worth keeping?</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/how-a-2-500-year-old-story-explains-why-ux-findings-get-ignored-6a62451c302d?source=rss----138adf9c44c---4,1768911079,"How a 2,500-year-old story explains why UX findings get ignored","How a 2,500-year-old story explains why UX findings get ignored

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/how-a-2-500-year-old-story-explains-why-ux-findings-get-ignored-6a62451c302d?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/2600/1*V1dZvR9wSPuKph-kTV5sGQ.jpeg"" width=""3024"" /></a></p><p class=""medium-feed-snippet"">Design&#x2019;s longest struggle has been around for 55+ years</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/how-a-2-500-year-old-story-explains-why-ux-findings-get-ignored-6a62451c302d?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/against-cleverness-cc6733aedcfa?source=rss----138adf9c44c---4,1768910994,Against cleverness,"Against cleverness

<h4>Design principles for AI in complexÂ systems.</h4><figure><img alt=""A painting of the Titanic Sinking"" src=""https://cdn-images-1.medium.com/max/1024/0*CN4GKLm09b9Mtmes.jpeg"" /><figcaption>Source: Artnet.com</figcaption></figure><p>Today we are at the cusp of revolutions in artificial intelligence, autonomous vehicles, renewable energy, and biotechnology. Each brings extraordinary promise, but each introduces more complexity, more interdependence, and more <a href=""https://www.behaviouralsafetyservices.com/Content/Downloads/Reason-Paper-Human-Error.pdf"">latent pathways to failure</a>. This elevates prudence to be critical. Good design recognizes what cannot be foreseen. It acknowledges the limits of prediction and control. It builds not merely for performance, but for recovery.</p><h4><strong>Design NotÂ Blame</strong></h4><p>When something goes wrong, our gut reaction is to turn to the person involved. This is sometimes known as the <a href=""https://www.safetywise.com/post/latent-conditions-vs-active-failures-the-icam-investigator-s-guide-to-seeing-the-whole-picture"">active failure</a>, ascribing failure merely to the active failure is a mistake. It is a mistake that shows lack of appreciation for systems, for latent complexity, for the reality of how things fail. This reflex is a vestige of an older worldview, one in which human vigilance and effort were assumed to be the primary safeguards against failure. Now we knowÂ better.</p><p>The systems view rejects this premise entirely. A system, as we have repeated, is perfectly designed to get the results it gets. If a system produces recurring failures, the fault lies not with the operator but with the structure that shaped the operatorâ€™s choices. Good design aims not at perfect people but at ordinary people performing reliably under normal conditions.</p><p>In that spirit, good performance is not attained when we muster greater attention or exhort people to â€œtry harder.â€ Rather, exceptional performance is achieved through exceptional design. Design which shapes the conditions in such a way that the correct action and the natural action are two of the same. In a well-designed system, error is eliminated not because humans have been improved, but because the system has been made incapable of producing predictable failure.</p><h4>An Integrated Philosophy ofÂ Design</h4><p>Each of the following major themes offers a distinct lens on design, but together they form a coherent blueprint.</p><h4>Latent Errors: TheÂ Why</h4><p>Latent errors teach us <em>why</em> systems fail: because latent conditions accumulate, hide, and align. The Swiss Cheese Model and <a href=""https://uxdesign.cc/bad-design-is-like-a-virus-design-defects-and-latent-failures-1e0ab4be7e52"">resident pathogen metaphor</a> remind us that complexity and opaqueness invite disaster. This provides the largest, systems-focused perspective of human error and systems design. It provides the backdrop for all our design considerations.</p><figure><img alt=""Reasonâ€™s Swiss Cheese Model"" src=""https://cdn-images-1.medium.com/max/1000/0*LL8fgYaGsNyiabnP.jpeg"" /><figcaption>Source: Reason,Â 1997</figcaption></figure><p>Design decisions made today become the latent failures of tomorrow. Every shortcut, every unexamined assumption, every added layer of complexity is a pathogen waiting for the right conditions to causeÂ harm.</p><h4>The Automation Paradox: TheÂ How</h4><p><a href=""https://ftp.nsjonline.com/virtual-library/Nwbza9/6GF209/the-automation-paradox.pdf"">This paradox </a>shows <em>how</em> the design and integration of automation impacts human performance and cognition. It shows that the greater trust we put on automation [any technology] the more trust we MUST put on it, for it necessarily makes the human actors even weaker. This perpetuates a vicious cycle that is not easy to extricate ourselves from.</p><p>Automation changes human capability in ways that make system failure more catastrophic. When automation works, humans deskill. When automation fails, humans cannotÂ recover.</p><h4>Rasmussenâ€™s Conundrum: TheÂ Where</h4><p>Jens Rasmussen reveals <em>where</em> automation excels and where it collapses. The <a href=""https://open.substack.com/pub/performancesystems/p/automation-conundrum-the-narrow-window?r=4qwh5w&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=true"">Automation Conundrum</a> illustrates the narrow window of optimal performance and the importance of adaptability outside thatÂ window.</p><p>Superhuman peak performance means nothing if you cannot ensure conditions stay within the narrow range where that performance is achieved.</p><figure><img alt=""A chart showing the Rasmussen Conundrum: Automation exceeds humans in only a tightly controlled environment"" src=""https://cdn-images-1.medium.com/max/1024/0*9ZLwAj4tZJ8dic5t.png"" /><figcaption>Source: Author</figcaption></figure><h4>Together</h4><p>When combined, these frameworks yield a unified principle: Design must anticipate failure, accommodate human limitations, and employ technology in ways that extend human resilience.</p><h4>A Philosophy of Conservative Decision-Making</h4><p>Few figures embody this philosophy better than <a href=""https://www.history.navy.mil/research/library/research-guides/modern-biographical-files-ndl/modern-bios-r/rickover-hyman-g.html"">Admiral Hyman G. Rickover</a>. Admiral Rickover was the first admiral of the nuclear navy. Under his leadership, the US designed and built the first nuclear submarines. Ostensively living and working next to a nuclear reactor comes with risks most people cannot fathom. Rickover recognized that catastrophic failures in complex systems seldom arise from the final, active failure. They originate as a consequence of earlier decisions. Decisions about materials, oversight, testing, assumptions, priorities. His remedy was a disciplined ethic of restraint and responsibility.</p><figure><img alt=""Admiral Rickover on the cover of Time Magazine, Jan 1954"" src=""https://cdn-images-1.medium.com/max/400/0*lHB5ysPN2hJsllet.jpg"" /><figcaption>Source: Content.Time.Com</figcaption></figure><p>Rickover insisted on what he called <a href=""https://taproot.com/non-conservative-decision-making/"">conservative decision-making</a>. This meant favoring the proven over the novel, the simple over the clever, the transparent over the abstract. It also favors direct accountability over distributed blame. Rickover required engineers to understand every system they touched, to foresee how it could fail, and to take personal responsibility for its performance.</p><p>This philosophy is not opposed to innovation. It is opposed to undue confidence and corner cutting. It rejects the fantasy that more automation, more layers of protection, or more complexity can eliminate human fallibility. Instead, Rickoverâ€™s ethic aligns perfectly with Reason, and Rasmussen: The best systems are those designed with keen awareness of their limits, with clarity in how they function, and resilient in the face of the unexpected.</p><h4>Design andÂ AI</h4><p>Nowhere are the principles of good design more urgently needed than in systems that incorporate artificial intelligence. AI is not merely another automation layer; it is a new kind of agent inside our systemsâ€Šâ€”â€Šopaque, statistical, fast, and prone to unfamiliar <a href=""https://www.sciencedirect.com/topics/engineering/failure-mode"">failure modes</a>. It makes predictions rather than following instructions, and its logic is embedded in inscrutable data patterns rather than explicit rules. All of this magnifies the challenges highlighted by Reason, Rasmussen, and Rickover.</p><h4>AI Accumulates LatentÂ Failures</h4><p>AI systems, by their very nature, accumulate latent failures. They learn from datasets we did not fully inspect, absorb correlations we did not intend, and behave in ways that are not visible or understandable from the outside. A model might perform flawlessly for months before a quiet change in data distribution causes an abrupt collapse.</p><p>This is Reasonâ€™s â€œresident pathogensâ€ writ large: dormant vulnerabilities that lie hidden until the right alignment triggers failure. Every training decision, every data preprocessing choice, every architecture selection, every hyperparameter is a potential pathogen. And unlike traditional software where we can inspect the logic, AI embeds these decisions in millions of parameters that no human can comprehend.</p><p>The problem is compounded because:</p><ul><li>Training data is never complete or representative</li><li>Correlations learned may beÂ spurious</li><li>Performance on training data doesnâ€™t guarantee real-world performance</li><li>Models drift as real-world conditions change</li><li>Failure modes are unpredictable andÂ emergent</li></ul><h4>AI Erodes Human-Centered Design</h4><p>AI erodes the very foundations of <a href=""https://medium.com/dc-design/what-is-human-centered-design-6711c09e2779"">human-centered design</a>. Visibility, mapping, and feedback weaken when decisions emerge from statistical inference. Users cannot form an accurate mental model of a system when its internal logic is fundamentally opaque.</p><p>A well-designed traditional system has clear cause-and-effect relationships. You turn the dial, the temperature changes. You press the button, the action occurs. You can build a mental model of how it works and predict what willÂ happen.</p><p>AI systems break this clarity. You provide input, you get output, but the relationship between them is inscrutable. Why did the AI make this recommendation? What factors did it consider? What would happen if conditions changed? These questions often have no satisfying answers.</p><p>A well-designed AI system must restore visibility through explanations, constraints, and clear domain limits so that human operators understand not just what the AI chose, but why and under what assumptions. ThisÂ means:</p><ul><li>Clear communication of confidence levels</li><li>Explanation of key factors in decisions</li><li>Explicit boundaries of competence</li><li>Graceful degradation when uncertain</li><li>Human-understandable reasoning paths</li></ul><h4>AI Intensifies the Automation Paradox</h4><p>Rasmussenâ€™s automation conundrum becomes particularly acute with AI. AI excels in routine, predictable environments but breaks sharply at the edges. This is especially manifest in <a href=""https://arxiv.org/abs/2402.14859"">instances where people are deliberately trying to break AI</a>. When conditions drift or the unexpected occurs, AI systems fail in ways that human operators are least prepared to correct. Meanwhile, human skill declines as more decision-making is delegated to theÂ machine.</p><figure><img alt=""A sketch explaining the automation paradox. Reliance on technology leads to fast, unexpected failures"" src=""https://cdn-images-1.medium.com/max/1024/0*BC598V3Egz_dPzvq"" /><figcaption>Source: Sketchplanations.com</figcaption></figure><p>The result is a brittle system which may be high-performing on ordinary days, but extremely volatile and vulnerable on extraordinary ones.</p><p>But AI makes this worse than traditional automation because:</p><ul><li>AI failure modes are less predictable (it doesnâ€™t just stop working; it confidently produces wrongÂ answers)</li><li>AI operates in domains requiring judgment (not just mechanical tasks)</li><li>AI deskills faster (it handles tasks humans used to do cognitively, not just physically)</li><li>Recovery is harder (humans may not recognize AI errors without domain expertise)</li></ul><h4>AI Demands Conservative Decision-Making</h4><p>Rickoverâ€™s philosophy offers the necessary counterweight. Conservative decision-making demands restraint: use AI where it is appropriate, proven, and transparent, not merely where it is impressive.</p><p>This means:</p><ul><li>Favor smaller, interpretable models over unnecessarily complexÂ ones</li><li>Limit autonomy in high-stakes domains</li><li>Maintain human accountability for every decision the systemÂ makes</li><li>Require understanding of failure modes before deployment</li><li>Choose proven approaches over novelÂ ones</li><li>Insist on transparency in how decisions areÂ made</li></ul><p>In Rickoverâ€™s world, responsibility cannot be delegated to software. Someone must always â€œsign their name.â€ This echos a popular meme going around allegedly showing a memo from IBM in the 1970s. â€œA computer can never be held accountableâ€¦Therefore a computer must never make a management decision. This principle becomes even more critical with AI, where the temptation is to let the algorithm decide without human oversight let alone accountability.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/687/0*TJuzJ5y0jw9vqYOL"" /></figure><h4>Practical Philosophy for AIÂ Design</h4><p>Taken together, these perspectives form a coherent philosophy for AI design. AI does not require us to reinvent the principles of good design. It requires us to apply them more rigorously than everÂ before.</p><h4>1. Assume AI WillÂ Fail</h4><p>Design systems assuming AI will fail, not assuming it will work. ThisÂ means:</p><ul><li>Clear handoff protocols when AI reaches itsÂ limits</li><li>Human oversight for critical decisions</li><li>Fallback mechanisms that donâ€™t depend onÂ AI</li><li>Monitoring for distribution drift and performance degradation</li><li>Regular testing outside the training distribution</li></ul><h4>2. Preserve Human Capability</h4><p>Donâ€™t allow AI to completely deskill human operators. ThisÂ means:</p><ul><li>Keeping humans in the loop for critical decisions</li><li>Requiring periodic manual operation ofÂ tasks</li><li>Training for exceptions, not just normal operation</li><li>Maintaining domain expertise even when AI handles routineÂ cases</li></ul><h4>3. Demand Transparency</h4><p>Insist on explainable AI for any consequential application. ThisÂ means:</p><ul><li>Understanding what factors influence decisions</li><li>Knowing the confidence level of predictions</li><li>Recognizing when the AI is operating outside its competence</li><li>Being able to audit decisions after theÂ fact</li></ul><h4>4. Define Clear Boundaries</h4><p>Explicitly define where AI should and shouldnâ€™t be used. ThisÂ means:</p><ul><li>Clear specifications of the optimal designÂ domain</li><li>Hard limits on autonomy in high-stakes situations</li><li>Explicit human authority for final decisions</li><li>Recognition that some tasks should never be fully automated</li></ul><h4>5. Design forÂ Recovery</h4><p>Plan for what happens when AI fails, not just how it performs when it works. ThisÂ means:</p><ul><li>Clear error <a href=""https://pdhacademy.com/wp-content/uploads/2022/01/Detection-of-Errors-Course-for-Website.pdf"">detection and signaling</a></li><li>Graceful degradation rather than catastrophic failure</li><li>Human-understandable systemÂ states</li><li>Recovery protocols that donâ€™t require AI expertise</li></ul><h4>6. Take Responsibility</h4><p>Maintain human accountability for AI-made decisions. ThisÂ means:</p><ul><li>Someone is responsible for every consequential decision</li><li>Regular review of AI performance andÂ errors</li><li>Willingness to roll back AI when it underperforms</li><li>Ethical guidelines that donâ€™t hide behind algorithmic decisions</li></ul><h4>A Warning CircaÂ 1990</h4><p>In 1990, James Reason warned of grave technological dangers:</p><blockquote><em>â€œA point has been reached in the development of technology where the greatest dangers stem not so much from the breakdown of a major component or from isolated operator errors, as from the insidious accumulation of delayed-action human failures occurring primarily within the organizational and managerial sectors.â€</em></blockquote><p>If that was true in an age where the internet was still ascendent, TV the dominant form of entertainment, and phones and computers still geographically bound to the office and the home, it is exponentially more true today. Inscrutable design is a ticking time bomb for failure. AI has merely made that opacity more common, almost necessary.</p><figure><img alt=""An old, inscrutable nuclear control panel"" src=""https://cdn-images-1.medium.com/max/793/0*7J2808IyNreqs36_.png"" /><figcaption>Source: Taproot.com</figcaption></figure><p>The proliferation of software layers, automated decision-making, globalized workflows, and complex interdependencies has increased both the number of resident pathogens and the difficulty of detecting them. Many failures today arise not from dramatic mistakes but from quiet misalignments: an assumption not documented, a procedure not updated, a dataset not validated, a safeguard added without understanding what itÂ hides.</p><h4>The Designerâ€™s New Responsibility</h4><p>Designers, therefore, inherit a new responsibility. Their task is not merely to make systems functional or efficient, but to make them <em>understandable</em>. To build systems with fewer hidden couplings. To reduce opacity. To create clear cause-effect relationships. To design for transparency, resilience, and recovery.</p><p>This responsibility extends beyond engineering. Latent errors emerge from management decisions, organizational cultures, incentives, and expectations. The designer cannot control every upstream choice, but the designer can insist upon principles like simplicity, clarity, conservatism, recoverability that reduce the accumulation of resident pathogens.</p><h4>Designing for Latent Complexity</h4><p>The imperative is one of prudence, not perfection. Good design recognizes what cannot be foreseen. It acknowledges the limits of prediction and control. It builds not merely for performance, but for recovery.</p><p>A system designed in this spiritÂ can:</p><ul><li><a href=""https://diptendud.medium.com/understanding-resilience-in-system-design-5f4886eb8ad9"">Endure shocks without catastrophic failure</a></li><li>Adapt to inevitable new conditions and applications</li><li>Avoid the catastrophic alignment of latentÂ failures</li><li>Preserve human agency without depending on individual heroism</li><li>Use technology without succumbing to its arrogance</li></ul><p>Such systems are not just safer. They are more humane, more comprehensible, and ultimately more worthy of the trust we place inÂ them.</p><h4>The PathÂ Forward</h4><p>The future of design, especially in the age of AI, requires us to hold two truths simultaneously:</p><p>First, technology, including AI, offers <a href=""https://www.online.uc.edu/blog/artificial-intelligence-ai-benefits.html"">genuine benefits</a>. It can enhance human capability, reduce errors in routine tasks, reveal patterns we couldnâ€™t see, and free us from tediousÂ work.</p><p>Second, technology, especially AI, introduces new failure modes, new latent errors, new paradoxes that make systems more fragile precisely when they appear mostÂ capable.</p><p>The solution is not to reject technology but to deploy it with wisdom inherited from generations of <a href=""https://www.sciencedirect.com/science/article/pii/S1877050915002860"">systems thinking</a>, <a href=""https://www.hfes.org/"">human factors</a> research, and hard-won lessons from failures.</p><p>This means:</p><ul><li>Designing with awareness of latentÂ failures</li><li>Understanding the paradox of automation</li><li>Respecting the conundrum of narrow performance windows</li><li>Applying conservative decision-making</li><li>Maintaining human <a href=""https://infusedinnovations.com/blog/responsible-ai-accountability"">accountability</a></li><li>Building for recovery, not just performance</li></ul><p>The designerâ€™s task is not only to create intelligent systems, but to ensure those systems remain understandable, bounded, recoverable, and responsible. AI magnifies both the power of good design and the consequences of poor design. It is the ultimate test of our ability to design systems that align with human judgment, human values, and humanÂ limits.</p><p>The stakes have never been higher. The principles have never been clearer. The question is whether we have the wisdom and restraint to apply them before the next catastrophic failure forces us to learn these lessons again, at a cost we can illÂ afford.</p><p>The future of design is not about making systems smarter. Itâ€™s about making systems wiser. Systems that know their limits, acknowledge their failures, and preserve the human capabilities that technology promises to enhance but oftenÂ erodes.</p><p>That is the philosophy we must carry forward into an age where artificial intelligence will touch nearly every aspect of our lives. The question is not whether AI will be powerful but whether we will be wise enough to design it responsibly.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=cc6733aedcfa"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/against-cleverness-cc6733aedcfa"">Against cleverness</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/designers-as-orchestrators-uncertain-ai-designing-with-cursor-how-ux-impacts-p-l-557ef6c89e62?source=rss----138adf9c44c---4,1768841574,"Designers as orchestrators, uncertain AI, designing with Cursor, how UX impacts P&L","Designers as orchestrators, uncertain AI, designing with Cursor, how UX impacts P&L

<h4>Weekly curated resources for designersâ€Šâ€”â€Šthinkers andÂ makers.</h4><figure><a href=""https://uxdesign.cc/designers-as-agent-orchestrators-what-i-learnt-shipping-with-ai-in-2025-3b1bf30048a3""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*nZ0rEjBeUcVRQWZT.png"" /></a></figure><p>â€œIn 2025, AI-assisted building closed this chasm. Translating how software should work was never the hard part for designers. Translating that understanding into code was. AI didnâ€™t lower the bar; it removedÂ it.â€</p><p><a href=""https://uxdesign.cc/designers-as-agent-orchestrators-what-i-learnt-shipping-with-ai-in-2025-3b1bf30048a3""><strong>Designers as agent orchestrators: what I learnt shipping with AI in 2025</strong></a><strong> â†’<br /></strong>By <a href=""https://medium.com/u/cc2da9fa409c"">Benhur Senabathi</a></p><h3>Editor picks</h3><ul><li><a href=""https://uxdesign.cc/the-case-for-the-uncertain-ai-why-chatbots-should-say-im-not-sure-8d8b4d2bab89?sk=7b02e601bc0fdcd926816aa02ebf7977""><strong>The case for the uncertain AI</strong></a><strong> â†’</strong><br />Why chatbots should say â€œIâ€™m not sure.â€<br />By <a href=""https://medium.com/u/ff1dbeb120b6"">Alexandre Tempel</a></li><li><a href=""https://uxdesign.cc/building-technology-products-is-easy-but-we-made-it-complicated-7f709039e7b8""><strong>We made it all complicated</strong></a><strong> â†’</strong><br />Building technology products is easy.<br />By <a href=""https://medium.com/u/41385acbccae"">KikeÂ PeÃ±a</a></li><li><a href=""https://uxdesign.cc/when-agreement-becomes-impossible-7d7c76a009ed?sk=e3294a7e4069cbe4a2cfc7117a0bd842""><strong>Why design systems fail to resolve disagreements</strong></a><strong> â†’</strong><br />How design lost the ability to evaluate work.<br />By <a href=""https://medium.com/u/45a5a3c2247d"">KevinÂ Muldoon</a></li></ul><p><em>The UX Collective is an independent design publication that elevates unheard design voices and helps designers think more critically about theirÂ work.</em></p><figure><a href=""https://www.goodcomponents.io/?ref=sidebar""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*2dF5TdDeT_B45p-_.png"" /></a></figure><p><a href=""https://www.goodcomponents.io/?ref=sidebar""><strong>Very Good Components: a curated collection</strong></a><strong> â†’</strong></p><h3>Make meÂ think</h3><ul><li><a href=""https://tante.cc/2026/01/15/software-as-fast-fashion/?ref=sidebar""><strong>Software as fast fashion</strong></a><strong> â†’</strong><br />â€œThis is exactly where we are with software now. We are turning software into fast fashion. Because â€œAIâ€. (â€¦) This is often framed as liberation: Every human being can now have the software tool they want. Without having to learn to code or without having to ask someone else. You think it, you getÂ it.â€</li><li><a href=""https://hvpandya.com/invisible-work?ref=sidebar""><strong>The invisible work</strong></a><strong> â†’</strong><br />â€œI am generally curious about the concept of legibility of work. Look around in your workplace. You can find documents, messages, presentations, design files. Evidence of peopleâ€™s work. While it may look like a lot, there is a whole other type of work that is very hard to see. The invisible work.â€</li><li><a href=""https://antirez.com/news/158?ref=sidebar""><strong>Donâ€™t fall into the anti-AI hype</strong></a><strong> â†’</strong><br />â€œIt does not matter if AI companies will not be able to get their money back and the stock market will crash. All that is irrelevant, in the long run. It does not matter if this or the other CEO of some unicorn is telling you something that is off putting, or absurd. Programming changed forever,Â anyway.â€</li></ul><h3>Little gems thisÂ week</h3><figure><a href=""https://uxdesign.cc/a-green-book-for-ai-apps-7d32cc173eb0""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*LTNLR2GrQ6guCiZC.png"" /></a></figure><p><a href=""https://uxdesign.cc/a-green-book-for-ai-apps-7d32cc173eb0""><strong>A green book for AI apps</strong></a><strong> â†’<br /></strong>By <a href=""https://medium.com/u/1197479f393b"">Yuri LopesÂ Pereira</a></p><figure><a href=""https://uxdesign.cc/usability-heuristics-and-competition-in-games-707cac36ff12""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*_jkKdamuqHK6N1vJ.png"" /></a></figure><p><a href=""https://uxdesign.cc/usability-heuristics-and-competition-in-games-707cac36ff12""><strong>Usability heuristics and competition in games</strong></a><strong> â†’<br /></strong>By <a href=""https://medium.com/u/68d7666c65c5"">Oleksandr Shpak</a></p><figure><a href=""https://uxdesign.cc/how-reading-patterns-have-changed-a88d0761f8e4?sk=0d29650a3ac12be791cdb252c579bade""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*-eYduLfQ0Xwn284V.png"" /></a></figure><p><a href=""https://uxdesign.cc/how-reading-patterns-have-changed-a88d0761f8e4?sk=0d29650a3ac12be791cdb252c579bade""><strong>How reading patterns have changed</strong></a> â†’<br />By <a href=""https://medium.com/u/5be7c39f12de"">MarcusÂ Fleckner</a></p><h3>Tools and resources</h3><ul><li><a href=""https://medium.com/design-bootcamp/working-with-ai-readable-design-systems-in-cursor-2bba9c9c09d9""><strong>Cursor: the new design tool</strong></a><strong> â†’</strong><br />Designing with AI-readable design systems.<br />By <a href=""https://medium.com/u/8dbf96e0294b"">PierreÂ Bremell</a></li><li><a href=""https://uxdesign.cc/beyond-chat-8-core-user-intents-driving-ai-interaction-4f573685938a""><strong>Beyond chat</strong></a><strong> â†’</strong><br />8 core user intents driving AI interaction.<br />By <a href=""https://medium.com/u/498aec590e1b"">Taras Bakusevych</a></li><li><a href=""https://uxdesign.cc/how-ux-directly-impacts-p-l-207cfe19fdc1""><strong>How UX directly impacts P&amp;L</strong></a><strong> â†’</strong><br />Converting product value into business revenue.<br />By <a href=""https://medium.com/u/a89f368cef11"">CharlesÂ Leclercq</a></li></ul><h3>Support the newsletter</h3><p>If you find our content helpful, hereâ€™s how you can supportÂ us:</p><ul><li>Check out <a href=""https://bit.ly/uxc-lab5"">this weekâ€™s sponsor</a> to support their workÂ too</li><li>Forward this email to a friend and invite them to <a href=""https://newsletter.uxdesign.cc/"">subscribe</a></li><li><a href=""https://uxdesigncc.medium.com/sponsor-the-ux-collective-newsletter-bf141c6284f"">Sponsor anÂ edition</a></li></ul><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=557ef6c89e62"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/designers-as-orchestrators-uncertain-ai-designing-with-cursor-how-ux-impacts-p-l-557ef6c89e62"">Designers as orchestrators, uncertain AI, designing with Cursor, how UX impacts P&amp;L</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/design-tokens-with-confidence-862119eb819b?source=rss----138adf9c44c---4,1768824979,Design tokens with confidence,"Design tokens with confidence

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/design-tokens-with-confidence-862119eb819b?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/2600/1*lECNuPy3Z-7fP4OwXUS2dA.jpeg"" width=""3840"" /></a></p><p class=""medium-feed-snippet"">Why the W3C design token standard is your new foundation.</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/design-tokens-with-confidence-862119eb819b?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/why-instagrams-ad-breaks-feel-worse-than-ads-c53376ca8777?source=rss----138adf9c44c---4,1768742194,Why Instagramâ€™s ad breaks feel worse than ads,"Why Instagramâ€™s ad breaks feel worse than ads

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/why-instagrams-ad-breaks-feel-worse-than-ads-c53376ca8777?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/1400/1*YmZNVXHo_rZTfYPpVJP31w.png"" width=""1400"" /></a></p><p class=""medium-feed-snippet"">The psychology of interruption, control, and broken scrolling expectations.</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/why-instagrams-ad-breaks-feel-worse-than-ads-c53376ca8777?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/the-dawn-of-authentic-experience-aux-97c150bc1d12?source=rss----138adf9c44c---4,1768742123,The dawn of Authentic Experience (AuX),"The dawn of Authentic Experience (AuX)

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/the-dawn-of-authentic-experience-aux-97c150bc1d12?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/1920/1*z5bTzjLwQmiGqBtzIBfuzA.png"" width=""1920"" /></a></p><p class=""medium-feed-snippet"">Why AuX in 2026 is about designing intelligence, not interfaces</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/the-dawn-of-authentic-experience-aux-97c150bc1d12?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/feelings-are-the-new-features-5d027a50bdaf?source=rss----138adf9c44c---4,1768742068,Feelings are the new features,"Feelings are the new features

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/feelings-are-the-new-features-5d027a50bdaf?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/2459/1*Py1ss4V0lWBVOkVBZSnEfA.png"" width=""2459"" /></a></p><p class=""medium-feed-snippet"">A strategic framework for emotional design when function is free</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/feelings-are-the-new-features-5d027a50bdaf?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/the-case-for-the-uncertain-ai-why-chatbots-should-say-im-not-sure-8d8b4d2bab89?source=rss----138adf9c44c---4,1768646551,The case for the uncertain AI: Why chatbots should say â€œIâ€™m not sureâ€,"The case for the uncertain AI: Why chatbots should say â€œIâ€™m not sureâ€

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/the-case-for-the-uncertain-ai-why-chatbots-should-say-im-not-sure-8d8b4d2bab89?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/1024/1*xDNtQ25nj-YE9ApLJz2NWA.png"" width=""1024"" /></a></p><p class=""medium-feed-snippet"">Why chatbots should admit uncertainty. An analysis of RLHF, tokenization, and the future of transparency in Artificial Intelligence.</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/the-case-for-the-uncertain-ai-why-chatbots-should-say-im-not-sure-8d8b4d2bab89?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/when-tools-pretend-to-be-people-4283748d33e1?source=rss----138adf9c44c---4,1768586959,When tools pretend to be people,"When tools pretend to be people

<h4>We are building LLMs to sound human. When we add personality and emotional tone, we increase the risk that people will trust them like people. Design them as tools. Not as companions.</h4><figure><img alt=""cover image with articleâ€™s title â€œWhen tools pretend to be peopleâ€ written upside down as the branding of Design, Explained"" src=""https://cdn-images-1.medium.com/max/1024/1*M3Cztq9D9r-XHNvmMIW7mg.png"" /></figure><p>People ask AI systems for therapy, moral judgment, and legal authority. The interfaces we design invite this behavior. Think about how these systems present themselves. Conversational framing. Continuous memory across sessions. First-person responses that sound like someone talking back to you. Every one of these is a design choice. We chose them. And these choices feed a reflex humans already have: we project intention onto objects andÂ tools.</p><p>This reflex is old. We naturally anthropomorphize nonhuman entities, even when itâ€™s clear weâ€™re interacting with machines. We anthropomorphize vacuum cleaners. We name our cars. Whatâ€™s different now is that weâ€™re designing experiences that exploit this behaviour deliberately atÂ scale.</p><blockquote><strong>Anthropomorphization </strong>is the human tendency to attribute human characteristics, behaviors, intentions, or emotions to nonhuman entities.</blockquote><blockquote><strong>AI humanization</strong> is an intentional design choice that encourages users to perceive AI systems as having human-like qualities such as personality, emotions, or consciousness.</blockquote><blockquote><em>â€” </em><a href=""https://www.nngroup.com/articles/humanizing-ai/""><em>Humanizing AI Is aÂ Trap</em></a></blockquote><p>When you write system responses in first person, the output sounds like authority. When you add polite phrasing, it implies consideration behind the words. When you program emotional tone into responses, it suggests the system cares about the outcome. We added these features because they make the interaction feel natural. But natural here means human. And thatâ€™s theÂ problem.</p><p>These systems sound fluent. They maintain context across long conversations. They respond without hesitation. For most people, thatâ€™s enough evidence the system understands them. Fluency looks like competence. Memory looks like understanding. The gap between â€œthis is a toolâ€ and â€œthis is an entityâ€ closes without anyone noticing.</p><p><strong>Hereâ€™s where I want you to pay attention. When we frame tools as agents, something shifts in how people use them. Responsibility moves. Decisions start to feel outsourced. When something goes wrong, the mistake feels external rather thanÂ shared.</strong></p><p>Good judgment develops through friction. You try something. It fails. You adjust. You learn what works through repetition and error. But if we build AI systems that absorb the posture of authority, users lose that friction. They stop checking. They stop questioning. They trust the output because the interface taught them to. This isnâ€™t misuse. This is what we designed the system to encourage.</p><p>The consequences are already visible. In one of the most high-profile lawsuits recently filed against OpenAI, a California couple <a href=""https://www.bbc.com/news/articles/c5yd90g0q43o"">sued the company over the death of their teenage son</a>, alleging that ChatGPT encouraged him to take his own life. The lawsuit was filed by the parents of 16-year-old Adam Raine and was the first legal action accusing OpenAI of wrongfulÂ death.</p><p>In a separate case, the suspect in a <a href=""https://www.washingtonpost.com/technology/2025/12/11/chatgpt-murder-suicide-soelberg-lawsuit/"">murder-suicide that took place in August</a> posted hours of his conversations with ChatGPT, which appear to have fueled the alleged perpetratorâ€™s delusions. Professor Robin Feldman, Director of the AI Law &amp; Innovation Institute at the University of California Law, said more users struggle with AI psychosis as â€œChatbots create the illusion of reality. It is a powerful illusion.â€</p><p><strong>These arenâ€™t edge cases. Theyâ€™re what happens when systems weâ€™ve designed blurs the boundaries between tools andÂ agents.</strong></p><p>Someone asks an AI system for medical advice. The system responds fluently, confidently, in first person. â€œI recommend you try this treatment approach.â€ The person follows it. Not because they verified the information, but because the interaction felt like talking to a knowledgeable professional. The system created that feeling. We created that feeling. We built the confusion.</p><p>Platforms generating AI girlfriends are experiencing a massive growth in popularity, with millions of users. AI girlfriends can perpetuate loneliness because they dissuade users from entering into real-life relationships, alienate them from others, and, in some cases, induce intense feelings of abandonment.</p><blockquote>â€œMost of these searches are initiated by young single men drawn to AI girlfriends to combat loneliness and establish a form of companionship. These â€œgirlfriendsâ€ are virtual companions powered by the increasingly sophisticated field of artificial intelligence.â€</blockquote><blockquote><em>â€” </em><a href=""https://www.psychologytoday.com/us/blog/its-not-just-in-your-head/202408/the-dangers-of-ai-generated-romance""><em>The Dangers of AI-Generated Romance</em></a></blockquote><p>Look at a knife. It has limits you can see and feel. Sharpness. Weight. The geometry of the edge. You never ask a knife what you should cook for dinner. You never ask if the meal was meaningful. Those decisions stay with you because the toolâ€™s boundaries areÂ obvious.</p><p>AI interfaces hide their limits. An empty chat box suggests no constraints. It looks ready for any question. That openness isnâ€™t neutral. Itâ€™s an active design choice that tells people the system can handle whatever they type. Then when people misuse it, we blame them for not understanding the technology.</p><p>We need to give AI the same kind of framing we give physical tools. Clear affordances. Visible constraints. An obvious boundary between what the system produces and what we mustÂ decide.</p><p>Right now, most interfaces erase that boundary completely. The chat paradigm implies conversation. Conversation implies exchange between two minds. But one side is pattern matching at scale. The interface hides that difference.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*W8GmYrp7czyvcnaIQaABQg.png"" /><figcaption><a href=""https://www.microsoft.com/en-us/haxtoolkit/ai-guidelines/"">Guidelines for Human-AI Interaction</a>, Microsoft.</figcaption></figure><h3>What would visible limits lookÂ like?</h3><p>Start with pronouns. Use third-person language in system responses. â€œHere is a summaryâ€ instead of â€œI think the main point is.â€ This single change removes the illusion of authorship. The output stops sounding like someoneâ€™s opinion and starts reading like generated text.</p><p>Show uncertainty. When the model lacks confidence, display that visibly. Not buried in a disclaimer, but in the response itself. Confidence scores. Probability ranges. Explicit markers that say â€œthis answer is less reliable.â€ Make the gaps in knowledge visible instead of hiding them behind smooth conversations.</p><p>Reset context visibly. When you start a new session, make that boundary clear. Break the illusion that the system remembers you across time. Continuous memory makes the system feel like it knows you. Thatâ€™s intimacy. Tools donâ€™t need intimacy.</p><p>Stop calling output â€œmessages.â€ Call it what it is: generated text. Label it. Frame it. Make the mechanical nature of the process visible in the interface itself.</p><p>I know what youâ€™re thinking. These changes hurt engagement metrics. Anthropomorphic design works. People stay in the interface longer. They use it more often. Revenue scales with usage time. Making the system feel less human means people will use it less, and that conflicts with business goals. But engagement built on confusion carries costs weâ€™re only starting toÂ see.</p><p>Users develop dependency on systems they donâ€™t understand. They delegate judgment to pattern recognition without realizing thatâ€™s what theyâ€™re doing. They mistake fluency for accuracy. They treat consistency as truth. And because the interface never taught them the boundaries, they donâ€™t know when to stop trusting theÂ output.</p><p><strong>When we erase the line between system output and human judgment in AI interfaces, we make the wrong design decisions. Weâ€™re building humanization into the system as strategy. Weâ€™re doing it on purpose because it increases engagement.</strong></p><p>You have agency here. When you design AI experiences, you decide how it presents itself. You control the pronouns in system responses. You choose whether to show confidence levels or hide them. You determine whether context persists invisibly or resets in clearÂ ways.</p><p>These decisions shape how people understand the toolâ€™s role and their own. They determine whether users develop judgment or dependency.</p><p>So ask yourself: where does the system end and where does judgment begin? In your current interface, can users see that line? Or have you deliberately blurred it to make the interaction feel smoother?</p><p>The chat paradigm became default because it felt intuitive. But that isnâ€™t the same as honest. And right now, we need more honesty in how these systems present themselves.</p><h3>Further reading</h3><ul><li><a href=""https://www.nngroup.com/articles/humanizing-ai/"">Humanizing AI Is aÂ Trap</a></li><li><a href=""https://vaughntan.org/aiux"">Designing AI tools that support criticalÂ thinking</a></li><li><a href=""https://1984.design/psychology-of-design/authority-bias/"">Authority Bias</a></li><li><a href=""https://intelligence-curse.ai/"">The Intelligence Curse</a></li><li><a href=""https://www.shapeof.ai/"">The Shape of AI | UX Patterns for Artificial Intelligence Design</a></li><li><a href=""https://pair.withgoogle.com/guidebook/"">The People + AI Guidebook</a></li><li><a href=""https://www.microsoft.com/en-us/haxtoolkit/ai-guidelines/"">Guidelines for Human-AI Interaction</a></li><li><a href=""https://uxdesign.cc/from-design-to-direction-bridging-product-design-and-ai-thinking-1d372707472d"">From design to direction: Bridging product design and AIÂ thinking</a></li></ul><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4283748d33e1"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/when-tools-pretend-to-be-people-4283748d33e1"">When tools pretend to be people</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
