source,domain,url,created_utc,title,text
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/unstacking-css-stacking-contexts/,1769508000,Unstacking CSS Stacking Contexts,"Unstacking CSS Stacking Contexts

In CSS, we can create â€œstacking contextsâ€ where elements are visually placed one on top of the next in a three-dimensional sense that creates the perception of depth. Stacking contexts are incredibly useful, but theyâ€™re also widely misunderstood and often mistakenly created, leading to a slew of layout issues that can be tricky to solve."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/beyond-generative-rise-agentic-ai-user-centric-design/,1769086800,Beyond Generative: The Rise Of Agentic AI And User-Centric Design,"Beyond Generative: The Rise Of Agentic AI And User-Centric Design

Developing effective agentic AI requires a new research playbook. When systems plan, decide, and act on our behalf, UX moves beyond usability testing into the realm of trust, consent, and accountability. Victor Yocco outlines the research methods needed to design agentic AI systems responsibly."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/rethinking-pixel-perfect-web-design/,1768903200,Rethinking â€œPixel Perfectâ€ Web Design,"Rethinking â€œPixel Perfectâ€ Web Design

Amit Sheen takes a hard look at the â€œPixel Perfectâ€ legacy concept, explaining why itâ€™s failing us and redefining what â€œperfectionâ€ actually looks like in a multi-device, fluid world."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/smashing-animations-part-8-css-relative-colour/,1768384800,Smashing Animations Part 8: Theming Animations Using CSS Relative Colour,"Smashing Animations Part 8: Theming Animations Using CSS Relative Colour

CSS relative colour values are now widely supported. In this article, pioneering author and web designer [Andy Clarke](https://stuffandnonsense.co.uk/) shares practical techniques for using them to theme and animate SVG graphics."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/ux-product-designer-career-paths/,1768212000,UX And Product Designerâ€™s Career Paths In 2026,"UX And Product Designerâ€™s Career Paths In 2026

How to shape your career path for 2026, with decision trees for designers and a UX skills self-assessment matrix. The only limits for tomorrow are the doubts we have today. Brought to you by <a href=""https://smart-interface-design-patterns.com/"">Smart Interface Design Patterns</a>, a **friendly video course on UX** and design patterns by Vitaly."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/penpot-experimenting-mcp-servers-ai-powered-design-workflows/,1767859200,Penpot Is Experimenting With MCP Servers For AI-Powered Design Workflows,"Penpot Is Experimenting With MCP Servers For AI-Powered Design Workflows

[Penpot](https://penpot.app/?utm_source=SmashingMagazine&amp;utm_medium=Article&amp;utm_campaign=MCPserver) is experimenting with MCP (Model Context Protocol) servers, which could lead to designers and developers being able to perform tasks in Penpot using AI thatâ€™s able to understand and interact with Penpot design files. Daniel Schwarz explains how [Penpot MCP](https://github.com/penpot/penpot-mcp) servers work, what they could mean for creating and managing designs in Penpot, and what you can do to help shape their development."
rss,smashingmagazine.com,https://smashingmagazine.com/2026/01/pivoting-career-without-starting-from-scratch/,1767780000,PivotingÂ Your Career Without Starting From Scratch,"PivotingÂ Your Career Without Starting From Scratch

Most developers spend their days fixing bugs, shipping features, and jumping into the next sprint without even thinking about it. After a while, you begin to ask yourself, â€œIs this still what I want to be doing?â€ This article looks at how you can move into a new direction in your career without starting from scratch, and how the skills you already use, like problem-solving, communication, and empathy, can open new doors."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/12/desktop-wallpaper-calendars-january-2026/,1767171600,Countdown To New Adventures (January 2026 Wallpapers Edition),"Countdown To New Adventures (January 2026 Wallpapers Edition)

Whether 2026 has already begun as youâ€™re reading this or youâ€™re still waiting for the big countdown to start, how about some new wallpapers to get your desktop ready for the new year? Weâ€™ve got you covered."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/12/how-design-for-with-deaf-people/,1767088800,How To Design For (And With) Deaf People,"How To Design For (And With) Deaf People

Practical UX guidelines to keep in mind for 466 million people who experience hearing loss. More design patterns in <a href=""https://smart-interface-design-patterns.com/"">Smart Interface Design Patterns</a>, a **friendly video course on UX** and design patterns by Vitaly."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/12/giving-users-voice-virtual-personas/,1766484000,Giving Users A Voice Through Virtual Personas,"Giving Users A Voice Through Virtual Personas

Turn scattered user research into AI-powered personas that give anyone consolidated multi-perspective feedback from a single question."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/12/how-measure-impact-features-tars/,1766138400,How To Measure The Impact Of Features,"How To Measure The Impact Of Features

Meet TARS â€” a simple, repeatable, and meaningful UX metric designed specifically to track the performance of product features. Upcoming part of the <a href=""https://measure-ux.com/"">Measure UX &amp; Design Impact</a> (use the code ğŸŸ <code>IMPACT</code> to save 20% off today)."
rss,smashingmagazine.com,https://smashingmagazine.com/2025/12/smashing-animations-part-7-recreating-toon-text-css-svg/,1765965600,Smashing Animations Part 7: Recreating Toon Text With CSS And SVG,"Smashing Animations Part 7: Recreating Toon Text With CSS And SVG

In this article, pioneering author and web designer [Andy Clarke](https://stuffandnonsense.co.uk) shows his techniques for creating [Toon Text titles](https://stuffandnonsense.co.uk/toon-text/index.html) using modern CSS and SVG."
rss,uxdesign.cc,https://uxdesign.cc/are-we-doing-ux-for-ai-the-right-way-aea01e14138e?source=rss----138adf9c44c---4,1769542429,Are we doing UX for AI the right way?,"Are we doing UX for AI the right way?

<h4>How chatbot-first thinking makes products harder forÂ users</h4><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*7x1tlD3qYPDu58RQ-u1veQ.png"" /></figure><p>Weâ€™ve lived with AI long enough to witness some groundbreaking changes in how we live, work, and design. But weâ€™ve also lived with it long enough for misconceptions to emergeâ€Šâ€”â€Šbefore best practices have really had a chance toÂ settle.</p><p>The pace of disruption remains wild. However, <a href=""https://www.nngroup.com/articles/state-of-ux-2026/"">2026 is already being described as the year of AIÂ fatigue</a>.</p><p>For product leaders, this creates a new challenge: we need to define ways to approach UX for AI without making AI-powered products exhausting, inconvenient, risky, and unsustainable.</p><p>This article focuses on one of the widespread misconceptions that could send the future of UX along a very wrong trajectory. I call it <em>chatbot-first thinking</em>: the assumption that conversational interfaces canâ€Šâ€”â€Šor shouldâ€Šâ€”â€Šreplace most existing UI patterns completely.</p><h3><strong>The Promise of ZeroÂ UI</strong></h3><p>Letâ€™s start with the most obvious question on theÂ surface:</p><p><strong>Is the future of user experience purely conversational?</strong></p><p>For many, itâ€™s tempting to dream about it. Especially if youâ€™re a frequent user of large LLM interfaces today. Especially if you follow <a href=""https://techcrunch.com/2026/01/21/openai-aims-to-ship-its-first-device-in-2026-and-it-could-be-earbuds/"">announcements like OpenAIâ€™s recent promise of a screen-free, pocketable product</a> that hints at a life increasingly driven by AI orchestration. And especially if your mental model of AI experience is based on the early ChatGPT experience, which was entirely conversational, with no UI snippets and micro-apps within the chat dialogue.</p><p>Itâ€™s also easy to fall into this line of thinking after watching the rise of ChatGPT appsâ€Šâ€”â€Špulling familiar services, like OpenTable, directly into the chat dialogue (spoiler alert: you still need to visit OpenTableâ€™s website to complete your user journey, as is the case with almost every app in the ChatGPT app directory). The idea that everything could eventually live inside a single conversational interface starts to feel not just plausible, but inevitable.</p><a href=""https://medium.com/media/c1daf7b9a5b088348fa7d2943f70612d/href"">https://medium.com/media/c1daf7b9a5b088348fa7d2943f70612d/href</a><p>In my personal life, as an enthusiast of AI tools, I increasingly try to experiment and outsource tasks to LLMs via text or voice. Agents from tools like ChatGPT or Claudeâ€Šâ€”â€Šdespite being clumsy, slow, and often struggling with all the still-non-AI-first websitesâ€Šâ€”â€Šcan handle a variety of scenarios, all of which I can trigger via a voice command in theÂ chat.</p><p>But thereâ€™s only so much I can do via text, let aloneÂ voice.</p><h3>Can we always use conversational UX?</h3><p>The reality is much more complex. Many of the tasks we deal with in our personal life and at work require <strong>rich, multi-modal interaction patterns that conversational interfaces simply cannotÂ support</strong>.</p><p>If youâ€™re a product leader evaluating whether UX of your AI functionality should become purely conversational, I recommend thinking twice whether it will become an accelerator or a blocker for yourÂ product.</p><p>One of the good starting points is <a href=""https://developers.google.com/assistant/conversation-design/is-conversation-the-right-fit#is-conversation-the-right-fit"">Googleâ€™s checklist</a> for the conversational approachÂ fit:</p><figure><img alt=""â€œIs conversation the right fit?â€ from Googleâ€™s Assistant / Conversation Design Guidance"" src=""https://cdn-images-1.medium.com/max/1024/1*adb9a_2AQYvfkCIsl69UtA.png"" /><figcaption>â€œIs conversation the right fit?â€ from <a href=""https://developers.google.com/assistant/conversation-design/is-conversation-the-right-fit#is-conversation-the-right-fit"">Googleâ€™s Assistant / Conversation DesignÂ Guidance</a></figcaption></figure><blockquote>When working with clients, we take this evaluation to the next level by guiding them to the interaction pattern choice via a series of validating questions, and essentially checkingâ€Šâ€”â€Šdoes the conversational approach check all theÂ boxes?</blockquote><h4>Question 1: Is it accessible?</h4><p>Chat-based interfaces require users to describe their problems in written text. <a href=""https://nces.ed.gov/surveys/piaac/ideuspiaac/"">Recent literacy research</a> shows that nearly half of the population in wealthy countries struggles with complex texts and wonâ€™t get good results from chatbots. <a href=""https://www.uxtigers.com/post/ai-articulation-barrier"">Jacob Nielsen called it a fundamental usability problem</a>, because it forces users to become prompt engineers just to reach the same basic level of user experience that visual interfaces previously provided.</p><figure><img alt=""Data collected by the OECD. Countries are sorted by the percentage of their adult population within literacy levels zero through two. â€œScandinaviaâ€ is the average of Denmark, Finland, Norway, and Sweden."" src=""https://cdn-images-1.medium.com/max/1024/1*BNh8aQnIHCnFkf9IrA0wMw.avif"" /><figcaption><a href=""https://nces.ed.gov/surveys/piaac/ideuspiaac/"">Data collected by the OECD</a>. Countries are sorted by the percentage of their adult population within literacy levels zero through two. â€œScandinaviaâ€ is the average of Denmark, Finland, Norway, andÂ Sweden.</figcaption></figure><h4>Question 2: Does it promote the productâ€™s adoption and discoverability?</h4><p>Letâ€™s imagine all of your productâ€™s users are articulate enough. Could they still use a conversational approach for virtually everything?</p><p>Consider food ordering as an example. Major players in the AI industry like to paint a future where ordering a lunch delivery is as simple as issuing a voice command. But what if you donâ€™t know what you want to order at all and would prefer to see the menu first? Listening to a menu, when it could be presented via graphical interface, violates one of the <a href=""https://www.nngroup.com/articles/ten-usability-heuristics/#:~:text=6%3A%20Recognition%20Rather%20than%20Recall"">core usability principles, <em>â€œrecognition rather than recallâ€</em></a> which advises <strong>minimising memory load by keeping interfaces self-evident.</strong></p><blockquote>Unlike a screen, spoken commands and outputs vanish instantly, giving users nothing to refer back to. Itâ€™s <em>â€œmuch easier to pick out the desired item from a list when the list is displayed on a monitor than when itâ€™s read aloud,â€</em> <a href=""https://www.nngroup.com/articles/voice-interfaces-assessing-the-potential/#:~:text=,feedback%20%20on%20its%20actions"">as Jacob Nielsen explains</a>.</blockquote><p>Back to our example with food ordering: how often do you actually spell out the names of your favourite food items in your head when youâ€™re hungry? Maybe you just donâ€™t remember the quirky name of that rice bowl you always order. Which leads us to the next issue with conversational interfaces.</p><p>Everything we put in a chat prompt is a piece of context, and the result of our collaboration with a chatbot can be only as good as the precision and relevance of our input.<strong> Therefore, prompt-based products work best for the users who already know how to ask the right question.</strong></p><p>In a purely conversational system, users often donâ€™t know what features or commands exist, unless they actively ask or the system hints at them. Moreover, when they do discover the product capabilities and get their job done successfully via chat, how can they be sure they can still get the exact same result next time? Generative AI is probabilistic by nature, and the results may differ slightly each time. Of course, building a knowledge graph, templates, or guardrails can reduce this variationâ€Šâ€”â€Šbut it can never remove it entirely. Conversational interfaces are a poor fit for high-precision tasksâ€Šâ€”â€Šunless they are heavily supported by non-conversational structure.</p><h4>Question 4: Is it reliable?</h4><p>Imagine a business user using a purely conversational assistant to update a contract term: â€œChange the renewal period to one year and apply the standard discount.â€</p><p>That sounds clear to a human, but in practice it leaves room for interpretation. Which contract version? Which definition of â€œstandardâ€? Does the discount apply immediately or at renewal? Is it one year from today or from the original start date? Small variations in phrasingâ€Šâ€”â€Šor in how the model interprets contextâ€Šâ€”â€Šcan lead to different outcomes eachÂ time.</p><p>In high-precision scenarios like this, users need to see exact fields, values, and constraints, review changes side by side, and explicitly confirm what will be saved. A conversational interface can help guide the process, but relying on chat alone makes the experience fragile.</p><h4>Question 5: Is it necessary?</h4><p>Just because a task <em>can</em> be done via chat, should it be done viaÂ chat?</p><p>Letâ€™s take an example of an atomic, well-defined task that looks like a textbook success case for an AI agent: I recently needed a transcript for a YouTube video and decided to outsource the task to the ChatGPTÂ agent.</p><p>The old way of doingÂ it:</p><ol><li>Search for an online transcription service</li><li>Copy and paste the YouTubeÂ link</li><li>Click a call-to-action button</li><li>Download the transcript</li></ol><p>The new, agentic way of doing it (as of JanuaryÂ 2026):</p><ol><li>Prompt ChatGPT: Get me the transcript of this YouTube video (link included)</li><li>(Optional) Observe how the agent navigates across multiple third-party transcription services, struggles with permissions, cookies, paywalls, or rate limits, retries failed requests, reformats output several times, takes minutes to complete the task but finally provides the transcript</li><li>Download the transcript</li></ol><p>If I need the transcript now, the experience quickly becomes frustrating. But itâ€™s not justÂ that.</p><h4>Question 6: Is it sustainable?</h4><p>The described experience raises another debate. When an LLM agent spends five minutes crawling the web, calling tools, retrying failures, reasoning through intermediate steps, it is running on energy-intensive infrastructure, contributing to real data-center load, energy usage, and COâ‚‚ emissions. For a task that could be solved with less energy by a specialised service, this is computational overkill.</p><p>From a systems perspective, this isnâ€™t just inefficient UXâ€Šâ€”â€Šitâ€™s inefficient infrastructure usage. <strong>The question becomes: was AI the right tool for this task atÂ all?</strong></p><h3>But what about agentic experiences? Wonâ€™t they remove the need for UI (andÂ UX)?</h3><p>First, letâ€™s explore what an AI agentÂ is.</p><blockquote><a href=""https://cloud.google.com/discover/what-are-ai-agents"">An AI agent is a software system that autonomously pursues goals and completes tasks on behalf of a user by reasoning, planning, and acting across tools and environments with minimal humanÂ input.</a></blockquote><p>The agentâ€™s role is to support human workflows: making information visible or drawing attention to what matters, handling parts of the workflow, exploring alternative scenarios, suggesting a best-guess next step, translating intent into actions by interfacing with APIs, coordinating signals across systems, or executing commands. <strong>The human remains the primary problem solver, with the agent augmenting and supportingâ€Šâ€”â€Šnot replacing them.</strong></p><p>In line with service design thinking, agents need humans to support them across several stages of the agentic lifecycle: 1) <strong>Set up</strong> â†’ 2) <strong>Delegate</strong> â†’ 3) <strong>Execute</strong> â†’ 4) <strong>Observe</strong> â†’ 5) <strong>Intervene</strong> â†’ 6) <strong>Confirm</strong> â†’ 7) <strong>Complete</strong> â†’ 8) <strong>Learn and Disengage. </strong>This system layer is called <a href=""https://www.ibm.com/think/topics/human-in-the-loop"">human-in-the-loop (HITL)</a> and is intended to ensure accuracy, safety, accountability or ethical decision-making of AI workflows.</p><blockquote>Agentic experiences require a service design approach to mapping and designing of these novel, dual experiences where agents and humans collaborate to achieve results we earlier could only dreamÂ of.</blockquote><figure><img alt=""Service blueprint for human-agentic experiences"" src=""https://cdn-images-1.medium.com/max/1024/1*O6-4hKkZyanuzI-4UN1zTw.png"" /><figcaption>Service blueprint for human-agentic experiences</figcaption></figure><p>All of the described stages and touchpoints require choosing from a variety of interface approaches, beyond just the conversational one <a href=""https://arxiv.org/html/2410.22370v1"">(canvas UI, contextual UI, modular UI, simulation environment, and other types)</a>. But this is a topic for a different article.</p><figure><img alt=""Various UX layouts for generative AI"" src=""https://cdn-images-1.medium.com/max/1024/1*h3gLQka9I-VzYo2rKBLe3g.png"" /><figcaption>Various <a href=""https://arxiv.org/html/2410.22370v1"">UX layouts for generative AI</a></figcaption></figure><h3>The price of (not) doing UX for AI the rightÂ way</h3><p>Customer fatigue with AI slop is growing across all industries, with business initiatives like <a href=""https://www.nngroup.com/articles/ai-ad/"">AI-generated ad campaigns triggering massive online backlash</a>.</p><p>In 2024 LinkedIn <a href=""https://www.fastcompany.com/91196335/linkedin-ai-powered-prompt-questions-under-posts-feeds-premium-removed"">silently removed the shallow and unhelpful AI prompt suggestions</a> in the feed which were spotted frequently by Premium users. Meta was also <a href=""https://www.nbcnews.com/tech/tech-news/meta-putting-ai-front-center-apps-users-are-annoyed-rcna148857"">criticised for rolling out Meta AI as a default part of the interface in their major apps</a>, such as in the search bar or with prominent icons users canâ€™t easily remove. Despite the user backlash, Meta is continuing to push AI features and earning the reputation of a company that is leading with AI for AIâ€™s sake and hurting user experience rather than helpingÂ it.</p><p>On the other hand, companies that apply user-centered approach to the design of AI-powered products are enjoying success as pioneers setting new mentalÂ models.</p><p>For example, <a href=""https://news.microsoft.com/en-hk/2024/11/20/ignite-2024-why-nearly-70-of-the-fortune-500-now-use-microsoft-365-copilot/"">Microsoft 365 Copilotâ€™s success with 70% of Fortune 500</a> companies is partially explained by their<strong><em> </em></strong>inline-style interaction pattern which has become the baseline of intuitive AI UXâ€Šâ€”â€Šthe one that enhances existing workflows, not replaces them with inferior interaction patterns.</p><p>Notion AI has been one of the first companies which enabled the interaction pattern of AI suggesting contextual preset actions. This is just one example of how thoughtful selection of a UX pattern can remove the cognitive load of wondering what the AI can actually do with the text and what the best way to prompt itÂ is.</p><figure><img alt=""Notion AI providing contextual actions to guide the user"" src=""https://cdn-images-1.medium.com/max/580/1*XVYmQSKVS2aJfaKlYxfleg.png"" /><figcaption>Notion AI providing contextual actions to guide theÂ user</figcaption></figure><blockquote>The future of UX for AI will not be a single interface or interaction patternâ€Šâ€”â€Šit will be layered ecosystems of tools, touchpoints, and safeguards. As these ecosystems become more agentic, the need for thoughtful UX design only increases.</blockquote><p>This is where user-centered design and service design become essentialâ€Šâ€”â€Šnot as supporting roles, but as foundational disciplines. UX and service designers are uniquely equipped to map end-to-end journeys, identify moments where human judgment must remain in the loop, and design systems that clearly communicate intent, limits, confidence, and accountability. They help answer the hard questions that AI alone cannot (at least not in its pre-AGI era): <em>When should the system act? When should it pause? When should it explain itself? And when should it step aside entirely?</em></p><p>If there is one lesson we should take from the last decade of digital product design, itâ€™s that technology rarely fails because it isnâ€™t powerful enough. It fails because it isnâ€™t shaped around real human needs and contexts. AI is no exceptionâ€Šâ€”â€Šand the cost of getting it wrong is significantly higher.</p><h4>Recommended furtherÂ reading</h4><p><a href=""https://www.smashingmagazine.com/2024/02/designing-ai-beyond-conversational-interfaces/""><em>When Words Cannot Describe: Designing For AI Beyond Conversational Interfaces</em></a> by Maximillian Piras</p><p><a href=""https://www.nngroup.com/articles/ai-changing-search-behaviors/?utm_source=chatgpt.com""><em>State of UX 2026: Design Deeper to Differentiate</em></a><em> </em>by Nielsen NormanÂ Group</p><p><a href=""https://lukew.com/ff/entry.asp?2107""><em>Common AI Product Issues</em></a><em> </em>by Luke Wroblewski</p><p><a href=""https://www.uxtigers.com/post/ai-articulation-barrier""><em>The Articulation Barrier: Prompt-Driven AI UX Hurts Usability</em></a><em> </em>by JakobÂ Nielsen</p><p><a href=""https://www.nngroup.com/articles/ai-paradigm/#:~:text=I%20doubt%20that%20the%20current,coughs%20up%20the%20right%20results""><em>AI: First New UI Paradigm in 60 Years</em></a> by JakobÂ Nielsen</p><p><a href=""https://arxiv.org/html/2410.22370v1""><em>Survey of User Interface Design and Interaction Techniques in Generative AI Applications</em></a> by ReubenÂ Luera</p><p><a href=""https://youtu.be/e3Pj7W0xl_c?si=vVhKSHDnyrbfezkp""><em>The Hidden Cost of Instant Answers</em></a> by SalehÂ Kayyali</p><p><a href=""https://arxiv.org/html/2410.22370v1"">Survey of User Interface Design and Interaction Techniques in Generative AI Applications</a> by ReubenÂ Luera</p><p><a href=""https://zapier.com/blog/human-in-the-loop/?utm_source=chatgpt.com""><em>Human-in-the-loop in AI workflows: HITL meaning, benefits, and practical patterns</em></a> by JulietÂ John</p><p><a href=""https://www.researchgate.net/publication/395298892_How_AI_Agents_Are_Reshaping_the_Internet_from_Human-Centered_to_Machine-Mediated_Commerce""><em>How AI Agents Are Reshaping the Internet from Human-Centered to Machine-Mediated Commerce</em></a> by A. ShajiÂ George</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=aea01e14138e"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/are-we-doing-ux-for-ai-the-right-way-aea01e14138e"">Are we doing UX for AI the right way?</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/stop-burying-your-impact-why-most-ux-storytelling-advice-falls-flat-0ff864039b0b?source=rss----138adf9c44c---4,1769516846,Stop Burying Your Impact: Why Most UX Storytelling Advice Falls Flat,"Stop Burying Your Impact: Why Most UX Storytelling Advice Falls Flat

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/stop-burying-your-impact-why-most-ux-storytelling-advice-falls-flat-0ff864039b0b?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/2000/1*UZ3ifLdm_bCzDC0xxm3nGA.jpeg"" width=""2000"" /></a></p><p class=""medium-feed-snippet"">Fiction story frameworks aren&#x2019;t effective for telling a UX story</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/stop-burying-your-impact-why-most-ux-storytelling-advice-falls-flat-0ff864039b0b?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/spatial-vibe-coding-prototyping-immersive-reality-with-ai-c2b99fd4cd84?source=rss----138adf9c44c---4,1769516732,Spatial vibe coding: prototyping immersive reality with AI,"Spatial vibe coding: prototyping immersive reality with AI

<h4><em>Vibe coding is the buzzword of the moment, and for good reasonÂ , AI tools have given product designers a massive new superpower. They have turned us all into â€œmakers.â€ </em>In this article, Iâ€™ll show how I vibe coded two different projects: a floating UI cooking app and an XR basketball game.</h4><figure><img alt=""A photo of a street with a 3D hood in the middle. Two arms are in front as if were to throw a ball"" src=""https://cdn-images-1.medium.com/max/1024/1*TOwitEhHrw2eRiVNjXkF2g.jpeg"" /></figure><p>While Figma put vibe coding on the map for designers, this approach goes far beyond just one tool. There is an army of incredible tools serving different purposes: design-focused tools like <a href=""https://stitch.withgoogle.com/"">Stitch</a>; prototyping engines like <a href=""https://vercel.com/"">Vercel</a>, <a href=""https://lovable.dev/"">Lovable</a>, <a href=""https://aistudio.google.com/"">Google AI Studio</a>, and production-ready environments like <a href=""https://antigravity.google/"">Antigravity </a>or <a href=""https://cursor.com/"">Cursor</a>. Itâ€™s worth mentioning that even the ones I categorize as prototyping can send its code to GitHub which add a new step towards production-ready code.</p><p>After â€œvibe codingâ€ mobile and web app prototypes for a while, I realized these tools should be able to handle XR 3D canvases as well. This led to a thoughtâ€¦ can they also prototype XR experiences?</p><p>The technology is certainly there. <a href=""https://immersiveweb.dev/"">WebXR </a>is the standard API for immersive experiences on the web, and frameworks like <a href=""https://aframe.io/"">A-Frame</a> provide ready-to-use XR components, covering inputs, primitives, teleportation, gestures and so on. Since AI coding tools can already interact with external libraries, I realized I only needed to provide the intent and guide the AI toward the spatial outcome IÂ wanted.</p><p>So, thatâ€™s exactly what I did. And it worked. Suddenly, I could tell the AI what I wanted, and it created a 3D canvas with a head-tracked camera, hand controllers, and functional interactions. This is the beginning of spatial vibeÂ coding!</p><h3>My approach to Spatial VibeÂ Coding</h3><p>In practice, vibe coding for immersive experiences is similar to vibe coding for mobile or desktop, but with an added dimension.</p><p>While you donâ€™t need to be a developer to make functional prototypes, the more you know about the tech stack, the more control you have. Pure vibe codingÂ , focusing solely on intent without thinking about the â€œhowâ€Â , is valid and often the best way to avoid biasing the AI. If you give the agent freedom, it often finds the most efficient path. Conversely, if you try to micromanage it, you end up with â€œtwo brainsâ€ working against each other on the sameÂ task.</p><p>However, understanding the tools allows you to speak the same language as the AI, increasing speed and precision. For example, A-Frame has a specific component called <a href=""https://aframe.io/docs/1.7.0/components/hand-tracking-controls.html"">hand-tracking-controls</a>. If you tell the AI to use this specific component, it generates a standardized, ready-to-use solution that creates fewer problems than a custom solution.</p><p>My recommendation? Focus on intent, but establish â€œrulesâ€ based on the components you know workÂ best.</p><h3>The prompting strategy: Freedom vsÂ Bias</h3><p>When it comes to technically complex projects, I prefer to let the main AI agent figure out the solution rather than asking a <em>second</em> AI to polish or â€œengineerâ€ myÂ prompt.</p><p>In my experience, when another AI takes my rough prompt and â€œpolishesâ€ it, it introduces a biases. It often guides the final coding agent in a direction it wouldnâ€™t have naturally taken, leading to worse results. I get better outcomes when I either tell the AI to refine the prompt without dictating <em>how</em> to solve it, or simply go with my original, intent-based prompt.</p><h4>The â€œintent-basedâ€ prompt (successful in thisÂ example)</h4><figure><img alt=""A screenshot of Figma Make environment with the preview of the UI panel"" src=""https://cdn-images-1.medium.com/max/1024/1*NGyz2eAkBtUfFHL_cXmvTg.jpeg"" /><figcaption>Figma Make environment</figcaption></figure><p>Here is the original prompt I created. It focused on the <em>what</em>, not the minute details of theÂ <em>how</em>:</p><blockquote>â€œI want you to create an MR cooking app. The app should provide a passthrough experience with a UI panel rendered inside it. When the app is published and the URL is opened in a VR headset such as Meta Quest 2 or 3, the user should see the standard Open in VR button in the browser. After clicking it, the user can enter the experience.</blockquote><blockquote>Use WebXR and A-Frame. I will be referencing A-Frame components and primitives frequently, so make sure to use native A-Frame primitives where possible.</blockquote><blockquote>Ensure that the UI is rendered within the MR experience. Place the UI panel to the left of the user and slightly tilted toward the userâ€™s camera. For now, include placeholder elements in the panel: a title, an image, and someÂ text.</blockquote><blockquote>The input system should be hand tracking, using the A-Frame raycaster component. Prepare a structure to group all UI elements, as the app will include multiple screens and navigation. Build the UI using A-Frame primitives.â€</blockquote><p><strong>The Result:</strong> I got a functional 3D canvas, an input system, passthrough, and a UI in place. It wasnâ€™t polished, but it was a solid baseline for iteration.</p><h4>The â€œover-engineeredâ€ prompt (failed in thisÂ example)</h4><p>I then tried using an AI to â€œprofessionalizeâ€ the prompt. This version was too prescriptive:</p><blockquote>â€œCreate a functional WebXR prototype for a Mixed Reality (MR) cooking app using the A-Frame framework.</blockquote><blockquote>1. Scene &amp; Environment Setup:<br />Initialize an &lt;a-scene&gt; configured for AR/MR passthrough. Ensure the background is transparent (&lt;a-sky&gt; removed or transparent) so the user sees the real world. Ensure the WebXR button is enabled by default so the user can click â€œEnter VR/ARâ€ on Meta Quest 2/3 browsers.</blockquote><blockquote>2. Camera &amp; Input:<br />Set up a camera rig with a position of 0 1.6 0. Implement Hand Tracking controls. Add &lt;a-entity&gt; components for both left and right hands using the hand-tracking-controls component. Attach a raycaster to the hands (configured to interact with UI objects) so the user can point and pinch/select elements.</blockquote><blockquote>3. UI Architecture:<br />Create a parent &lt;a-entity&gt; to act as a â€œScreen Containerâ€ to group all UI elements. This will allow for easy visibility toggling for future navigation. Placement: Position this UI group to the left of the user (approx x: -1, z: -1.5) and rotate it on the Y-axis (approx rotation=â€0 45 0&quot;) so it is tilted to face the userâ€™sÂ camera.</blockquote><blockquote>4. UI Design (A-Frame Primitives):<br />Inside the UI group, build a panel using an &lt;a-plane&gt; as the background (dark color, slight opacity). Add an &lt;a-image&gt; placeholder for a recipe title image. Add &lt;a-text&gt; for the recipe title and instruction placeholders. Ensure all UI elements are children of the parent group and are rendered properly in the MRÂ view.â€</blockquote><p><strong>The Result:</strong> This prompt guided the AI agent too much. It ended up strictly following instructions rather than finding a cohesive solution, which led to broken code. Itâ€™s like micromanaging a graphic designer, if you tell them exactly where to put every pixel, you wonâ€™t get their bestÂ work.</p><h3>Iterating on theÂ design</h3><p>Once the foundations were defined, I decided to push the fidelity. I noticed some errors in my initial brief, specifically, trying to build complex UI with primitives. I realized it was better to design a high-fidelity UI in Figma, export it as a PNG, and import it into the experience. While tools like Figma Make struggled slightly with the point-and-pinch input system, it was enough to visualize what the production version could lookÂ like.</p><figure><img alt=""A UI panel floating in an actual kitchen"" src=""https://cdn-images-1.medium.com/max/1024/1*LnDX2_CS64n6-AlK2Lyi_Q.gif"" /><figcaption>Cooking up testing inÂ XR</figcaption></figure><figure><img alt=""Hight fidelity UI of the cooking app"" src=""https://cdn-images-1.medium.com/max/1024/1*qYhj6cwODYlhoYoUv8_ZVQ.jpeg"" /><figcaption>Cooking app UI inÂ Figma</figcaption></figure><h3>Going further: complex interactions with Gemini 3Â Pro</h3><figure><img alt=""An animated gif of a player throwing a ball into the hoop"" src=""https://cdn-images-1.medium.com/max/1024/1*g4w0VTg3NNNF5M30CvYWDA.gif"" /><figcaption>Testing the game inÂ VR</figcaption></figure><p>To push beyond UI panels, I went with Gemini Pro in Google AIÂ Studio.</p><p>This demonstrated that a 15-minute prompting session can result in a fully functional WebXR prototype featuring physics, grabbing mechanics, and game logic. The generated physics are amazing, they feel very smooth andÂ real.</p><figure><img alt=""An animated gif of a player throwing the ball into the hoop in mixed reality, in the actual street"" src=""https://cdn-images-1.medium.com/max/1024/1*N5mMubfltF_5fFGajWXOAA.gif"" /><figcaption>Testing the game inÂ XR</figcaption></figure><p>While this method requires deployment via Cloud Run (rather than a simple â€œpublishâ€ button), the setup only takes two minutes. The outcome is a prototype that works in both VR and AR, creating a seamless transition between worlds. From this point, adding counters, sound effects, or textures is just a matter of conversation.</p><figure><img alt=""A screenshot of the Google AI Studio environment that shows the inital page of the game in a preview"" src=""https://cdn-images-1.medium.com/max/1024/1*jJ1ukBfs5SEo9fiITLxyhA.jpeg"" /><figcaption>Google AI Studio environment</figcaption></figure><figure><img alt=""Google AI Studio environment with a preview of the actual game"" src=""https://cdn-images-1.medium.com/max/1024/1*vzcrmxbrbE75KYydDoQ6nQ.jpeg"" /><figcaption>Preview of the game in Google AIÂ Studio</figcaption></figure><p>âœ¨ Adding a touch of Nano Banana magic! By upgrading to a high-fidelity visual, we can deliver a more convincing and impactful pitch.</p><figure><img alt=""An image of the game enhanced visually by Nano Banana Pro"" src=""https://cdn-images-1.medium.com/max/1024/1*Ac_9k374shjDoOuBw9XGBA.jpeg"" /><figcaption>Image enhanced with Nano BananaÂ Pro</figcaption></figure><p>These are early experiments, but they demonstrate how accessible Spatial Vibe Coding is becoming. I am eager to test this approach with even more powerful tools like Antigravity, which, combined with the newest Blender or Unity MCPs (Model Context Protocols), will open the door to production-ready code, not just prototypes.</p><p>The most important takeaway <strong>is not how far the <em>tools</em> can go, but how far <em>I</em> can go</strong> as a non-developer. I can now conceive an XR game or app idea, design it, and prototype it entirely on my own. The barrier to entry for creating immersive realities has never beenÂ lower.</p><p>Thank you for reading, please feel free to drop a comment or open a discussion. Iâ€™d love to hear fromÂ you!</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c2b99fd4cd84"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/spatial-vibe-coding-prototyping-immersive-reality-with-ai-c2b99fd4cd84"">Spatial vibe coding: prototyping immersive reality with AI</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/ai-wont-re-generate-your-focus-6a3453b057d7?source=rss----138adf9c44c---4,1769462770,AI wonâ€™t (re)generate your focus,"AI wonâ€™t (re)generate your focus

<h4><em>Weâ€™re consuming more content than ever, and remembering less of it. </em>Hereâ€™s what the research says about our shrinking focusâ€Šâ€”â€Šand whatâ€™s fuelling theÂ problem.</h4><figure><img alt=""Top-down view of a tablet on a grey desk displaying a digital illustration of a human head profile composed of circuit board patterns and mechanical elements in blue tones against a dark background. Yellow and green pencils, sticky notes, papers, and a cup of coffee surround the tablet."" src=""https://cdn-images-1.medium.com/max/1024/1*p3X-yCvmXoaBq1tb9His-w.jpeg"" /><figcaption>Image by <a href=""https://www.freepik.com/author/pixelshunter"">Pixelshunter</a> onÂ <a href=""https://www.freepik.com/"">Freepik</a></figcaption></figure><p>You settle in for a quick scroll through your feed, maybe just to unwind for a minute or two. But somewhere between a cooking hack and a clip youâ€™ve already forgotten, forty minutes vanished. Itâ€™s all a blur. Welcome to the era of infinite content and finite attention, where our brains are working overtime just to keep up with theÂ deluge.</p><p>Hereâ€™s the uncomfortable truth: our ability to focus is dwindling at a startling rate. <strong>According to </strong><a href=""https://gloriamark.com/attention-span/""><strong>Gloria Mark</strong></a><strong>, a professor of informatics at the University of California, the average attention span on any screen has plummeted from around two and a half minutes in 2004 to just 47 seconds in recent years.</strong> Thatâ€™s not a typo. Forty-seven seconds. The median is even lower at 40 seconds, which means half of the time weâ€™re switching our focus faster than it takes to microwave popcorn.</p><p>And in case you were wondering whether AI is helping or hindering the situation, well, the plot thickens.</p><figure><img alt=""Bar chart or timeline graphic illustrating the decline in average human attention span on screens over two decades. Three data points are shown: 150 seconds (2.5 minutes) in 2004, 75 seconds in 2012, and 47 seconds from 2018 onwards, representing a decrease of approximately 69% over the period studied."" src=""https://cdn-images-1.medium.com/max/1024/1*ahnUnhiOt5daivhLYbbZUw.png"" /><figcaption>Attention spans have decreased by two-thirds since 2004, <a href=""https://www.brainscape.com/academy/top-students-increasing-attention-span/"">graph by Brainscape</a></figcaption></figure><h3><strong>The contentÂ tsunami</strong></h3><p>If youâ€™ve ever felt like the internet is getting, shall we say, noisier, your instincts arenâ€™t deceiving you. <a href=""https://futurism.com/artificial-intelligence/over-50-percent-internet-ai-slop"">Recent data from Graphite</a>, an SEO firm that analysed 65,000 English-language articles published between 2020 and 2025, found that over half of all written content on the internet is now generated by AI. Before ChatGPT launched in late 2022, that figure was hovering around 5%. Talk about an explosion.</p><p><a href=""https://ahrefs.com/blog/what-percentage-of-new-content-is-ai-generated/""><strong>A separate study from Ahrefs</strong></a><strong> painted an even more dramatic picture, finding that a staggering 74% of newly created web pages in April 2025 contained AI-generated content. </strong>Meanwhile, <a href=""https://arxiv.org/abs/2504.08755"">research published on arXiv</a> suggested that roughly a third to almost half of the text on active web pages now originates from AI sources. Suffice to say, weâ€™re swimming in machine-made material, and our cognitive lifeboats are gettingÂ cramped.</p><p>The trouble is, more content doesnâ€™t necessarily mean better content. In fact, the sheer volume creates what researchers call â€œattention fragmentation,â€ a relentless toggling between tasks, tabs, and tools that leaves us perpetually half-focused. Gloria Mark describes this phenomenon as â€œkinetic attention,â€ where our mental state becomes dynamic and restless, flitting from screen to screen like a caffeinated hummingbird.</p><h3><strong>The dopamineÂ machine</strong></h3><p>It would be somewhat unfair to pin all of this on AI alone. Short-form video platforms like TikTok, Instagram Reels, and YouTube Shorts have been doing a phenomenal job of hijacking our neural reward systems well before the generative AI boom. But hereâ€™s where things get interesting: AI is now supercharging these platformsâ€™ ability to serve up hyper-personalised content at unprecedented scale.</p><p>Dr John Hutton, a pediatric neuropsychologist and director of the Reading &amp; Literacy Discovery Center at Cincinnati Childrenâ€™s Hospital, has <a href=""https://theweek.com/health-and-wellness/1025836/tiktok-brain-and-attention-spans"">described TikTok as a â€œdopamine machine.â€</a> The platform delivers quick hits of novelty and pleasure with every swipe, reinforcing cravings in much the same way a tasty meal or, less pleasantly, an addictive substance might. <strong>Each scroll triggers a little burst of feel-good neurochemistry, and our brains quickly learn to wantÂ more.</strong></p><p>The neurological evidence behind this is pretty eye-opening. In 2025, <a href=""https://www.sciencedirect.com/science/article/pii/S105381192500031X"">researchers at Tianjin Normal University</a> in China scanned the brains of more than 100 university students and found that those most hooked on short-form videos had noticeable differences in their brain structure. Specifically, they had more grey matter in the orbitofrontal cortex, the part of your brain that processes rewards and pleasure. The researchers suspect these students are simply more sensitive to the dopamine hits these platforms are designed to deliver, meaning theyâ€™re getting a stronger buzz from every personalised video that lands on theirÂ feed.</p><figure><img alt=""Infographic titled â€œSocial Mediaâ€™s Toll on the Mindâ€ showing TikTok and Instagram Reels icons pointing down to a colourful brain illustration. Labels around the brain identify effects: poorer cognitive and mental health (with anxiety and depression), addiction symptoms, decreased attention (with puzzle piece icons), and decreased activity in self-reflection and risk evaluation areas. A red warning banner at the bottom reads â€œMore prone to impulsive, short-term thinking.â€"" src=""https://cdn-images-1.medium.com/max/1024/1*t7d4dJA0JxVsNSOCjsiyhw.png"" /><figcaption>Image byÂ author</figcaption></figure><p><strong>Meanwhile, </strong><a href=""https://www.psypost.org/large-meta-analysis-links-tiktok-and-instagram-reels-to-poorer-cognitive-and-mental-health/""><strong>a large meta-analysis</strong></a><strong> linked heavy use of TikTok and Instagram Reels to poorer cognitive and mental health outcomes, including higher levels of anxiety, depression, and, crucially, attention difficulties. </strong>Users with addiction symptoms showed decreased activity in brain regions responsible for self-reflection and risk evaluation, making them more prone to impulsive, short-term thinking.</p><h3><strong>When the brain goes on autopilot</strong></h3><p>So what exactly happens when we marinate our minds in bite-sized, algorithmically-curated content day after day? According to emerging research, the prefrontal cortex starts to feel the strain. This is our brainâ€™s control centre for executive function, impulse restraint, and focused attention.</p><p>A <a href=""https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2024.1383913/full"">2024 study by Yan and colleagues</a> found that individuals with high short-video use tendencies showed weaker executive function and poorer self-control. <strong>Itâ€™s a bit like training for a marathon by exclusively doing 10-second sprints: the sustained-focus muscle simply doesnâ€™t get the workout itÂ needs.</strong></p><p>But how does this translate in real life? When young peopleâ€™s brains grow accustomed to the constant environmental changes delivered by an infinite scroll of 15-second clips, they may struggle to adapt to activities that donâ€™t move quite asÂ fast.</p><p><strong>The term â€œTikTok brainâ€ has emerged to describe this pattern: impulsivity, preference for novelty, low tolerance for delayed gratification, and difficulty maintaining attention.</strong> Itâ€™s not exactly the cognitive toolkit youâ€™d want for, say, reading a novel, sitting through a lecture, or tackling complex problems atÂ work.</p><h3><strong>The AI amplifier effect</strong></h3><p>Hereâ€™s where AI-generated content adds a particularly spicy ingredient to this already potent cocktail. Traditional content creation had natural bottlenecks: someone had to actually write, film, or design things, which took time and effort. AI has essentially removed those constraints, enabling content to be produced at a scale and speed that would make any human creatorâ€™s headÂ spin.</p><p>The result? <strong>An endless buffet of highly engaging, hyper-personalised material designed to capture and hold our attention. </strong>AI algorithms analyse vast amounts of user data, from search history and social media behaviour to physiological signals, crafting recommendations so compelling that stepping away feels almost painful. <a href=""https://sqmagazine.co.uk/social-media-attention-span-statistics/"">TikTokâ€™s algorithm alone tracks over 300 data points per user </a>to tailor content within the first hour of usage, making recommendations increasingly irresistible.</p><p><strong>And hereâ€™s the kicker: some research suggests that even AI itself isnâ€™t immune to the effects of junk content.</strong> A <a href=""https://fortune.com/2025/10/22/ai-brain-rot-junk-social-media-viral-addicting-content-tech/"">pre-print study from researchers at Texas A&amp;M, the University of Texas at Austin, and Purdue University</a> found that when large language models were trained on short, viral social media posts, they exhibited lasting cognitive decline, including increased â€œthought-skippingâ€ and reduced reasoning abilities. If the machines are getting â€œbrain rotâ€ from low-quality content, what does that say aboutÂ us?</p><figure><img alt=""Infographic titled â€œThe 25-Minute Problem: The Cost of Cognitive Switching&quot; on a steel blue background. Three white cards show: â€œThe Interruptionâ€ (stopwatch and brain with lightning boltsâ€Šâ€”â€Šattention switches every 47 seconds), â€œThe Daunting Recoveryâ€ (orange header, stopwatch displaying 25 minutes to refocus, figure climbing an arrow), and â€œPerpetual Loopâ€ (circular diagram of brains and stopwatches showing the endless cycle: distraction â†’ 25 min â†’ new distraction)."" src=""https://cdn-images-1.medium.com/max/1024/1*oYMDR7MT6pwL5K7RPJFPKg.png"" /><figcaption>Image byÂ author</figcaption></figure><h3><strong>The 25-minute problem</strong></h3><p>Perhaps the most concerning finding from Gloria Markâ€™s research is what happens after we get distracted. <a href=""https://www.universityofcalifornia.edu/news/cant-pay-attention-youre-not-alone""><strong>On average, it takes about 25 minutes</strong></a><strong> to fully refocus on the original task after an interruption.</strong> Given that weâ€™re switching our attention every 47 seconds or so, the maths becomes rather grim. Weâ€™re essentially living in a perpetual state of cognitive switching, never quite present, never quiteÂ focused.</p><p>The consequences ripple outward. Laboratory studies consistently show that people make more errors when they switch their attention frequently. Performance suffers: tasks take longer to complete, and the quality of work declines. Medical professionals, pilots, and other high-stakes workers have long known this; now the rest of us are discovering it in our daily digitalÂ lives.</p><p><strong>A </strong><a href=""https://www.news-medical.net/news/20250718/Social-media-use-linked-to-declining-focus-and-emotional-strain-in-youth.aspx""><strong>2025 NTU Singapore study</strong></a><strong> found that over two-thirds of young people reported difficulty focusing, with many struggling to engage with content lasting more than a minute.</strong> Heavy users (those clocking five or more hours daily) were significantly more likely to experience what researchers call â€œattention fragmentation symptoms,â€ alongside weaker working memory. These arenâ€™t just numbers; they represent real-world impacts on how we think, learn, andÂ work.</p><h3><strong>Can we reclaim ourÂ focus?</strong></h3><p>Before you throw your phone into the nearest body of water, there is some cause for optimism. Gloria Mark emphasises that our ability to focus isnâ€™t lost; itâ€™s just changing. <strong>The key, she argues, is developing what she calls </strong><a href=""https://blog.dropbox.com/topics/work-culture/gloria-mark-how-to-get-your-attention-span-back""><strong>â€œmeta-awarenessâ€</strong></a><strong>: </strong>the skill of catching yourself mid-drift, mid-scroll, or mid-sentence, and consciously deciding whether to follow that impulse or pull your attention back.</p><p>In one study, participants were asked to estimate how often they switched tasks. They guessed around 15 times per hour. The actual number? Over 30. Simply becoming aware of how fragmented our attention really is can be a powerful first step towardÂ change.</p><p>She also champions the value of what she calls â€œrote activity,â€ meaning simple, mindless tasks like playing a quick game or doing something repetitive. These give our overtaxed minds a genuine break. Think of it as cognitive snacking: strategic mental downtime that can actually help replenish our attentional resources rather than deplete themÂ further.</p><figure><img alt=""Infographic comparing â€œTraditional Contentâ€ versus â€˜Microlearning.â€ Left side shows an overwhelmed person surrounded by chaotic app icons and notifications with a stopwatch indicating 8 seconds, leading to a confused head silhouette with a question mark. Right side shows a calm person with organised content cards (videos, tips, concepts) and a stopwatch showing 2 minutes, leading to an illuminated brain icon. Bottom text highlights â€œ15-fold improvementâ€ with the tagline â€œIntention &gt; Engagement.â€"" src=""https://cdn-images-1.medium.com/max/1024/1*IvpJqk7aTD7RgRlyLa10lQ.png"" /><figcaption>Image byÂ author</figcaption></figure><p><strong>Thereâ€™s also </strong><a href=""https://www.amraandelma.com/user-attention-span-statistics/""><strong>evidence that microlearning</strong></a><strong> can actually stretch attention spans from 8 seconds to 2 minutes.</strong> This type of short, focused video content is designed with intention rather than just engagement, and the results speak for themselves: a 15-fold improvement simply by changing how content is delivered. The difference between content designed to capture attention and content designed to build it may well be the cognitive battleground of the comingÂ years.</p><h3><strong>The bottomÂ line</strong></h3><p>We are, in many ways, living through an unprecedented experiment in human attention. AI-generated content is flooding the internet, algorithms are becoming ever more sophisticated at predicting and exploiting our psychological vulnerabilities, and <strong>our brains, evolved for a very different information environment, are scrambling toÂ adapt.</strong></p><p>The research shows itâ€™s not a simple story. Yes, our attention spans are shrinking, and yes, the combination of short-form video platforms and AI-generated content appears to be accelerating this trend. The neurological evidence suggests real changes in how our brains process information, particularly among younger users whose prefrontal cortices are still developing.</p><p><strong>But humans have always been remarkably adaptable creatures.</strong> The challenge now is to become intentional about our relationship with technology: to recognise when weâ€™re being pulled into patterns that donâ€™t serve us, and to actively cultivate the focused attention that deeper thinking requires.</p><p>After all, the irony would be too rich if we couldnâ€™t even pay attention long enough to notice weâ€™d lostÂ it.</p><blockquote><strong><em>Thanks for reading!Â ğŸ“–</em></strong></blockquote><blockquote><em>If you liked this post, </em><a href=""https://medium.com/@doracee""><em>follow me on Medium</em></a><em> forÂ more!</em></blockquote><h3><strong>References &amp;Â Credits</strong></h3><ul><li>Mark, G. (2023). <em>Attention Span: A Groundbreaking Way to Restore Balance, Happiness and Productivity. </em>Hanover SquareÂ Press.</li><li>Yan T, Su C, Xue W, Hu Y and Zhou H (2024). â€œMobile phone short video use negatively impacts attention functions: an EEG study.â€ <em>Frontiers in Human Neuroscience</em>, 18:1383913.</li><li>University of California, Irvine. (2023). Canâ€™t pay attention? Youâ€™re not alone. <a href=""https://www.universityofcalifornia.edu/news/cant-pay-attention-youre-not-alone"">https://www.universityofcalifornia.edu/news/cant-pay-attention-youre-not-alone</a></li><li>Microsoft WorkLab. (2023). Regain Control of Your Focus and Attention with Researcher Gloria Mark. <a href=""https://www.microsoft.com/en-us/worklab/podcast/regain-control-of-your-focus-and-attention-with-researcher-gloria-mark"">https://www.microsoft.com/en-us/worklab/podcast/regain-control-of-your-focus-and-attention-with-researcher-gloria-mark</a></li><li>Dropbox Blog. (2025). Behold, the 47-second workday (and how to get your attention span back). <a href=""https://blog.dropbox.com/topics/work-culture/gloria-mark-how-to-get-your-attention-span-back"">https://blog.dropbox.com/topics/work-culture/gloria-mark-how-to-get-your-attention-span-back</a></li><li>Graphite. (2025). AI-generated articles study. As reported in Futurism: Over 50 Percent of the Internet Is Now AI Slop, New Data Finds. <a href=""https://futurism.com/artificial-intelligence/over-50-percent-internet-ai-slop"">https://futurism.com/artificial-intelligence/over-50-percent-internet-ai-slop</a></li><li>Ahrefs. (2025). 74% of New Webpages Include AI Content (Study of 900k Pages). <a href=""https://ahrefs.com/blog/what-percentage-of-new-content-is-ai-generated/"">https://ahrefs.com/blog/what-percentage-of-new-content-is-ai-generated/</a></li><li>Spennemann, D. H. R. (2025). Delving into: the quantification of AI-generated content on the internet (synthetic data). <em>arXiv. </em><a href=""https://arxiv.org/abs/2504.08755"">https://arxiv.org/abs/2504.08755</a></li><li>Gao, X., et al. (2025). Neuroanatomical and functional substrates of the short video addiction and its association with brain transcriptomic and cellular architecture. <em>ScienceDirect. </em><a href=""https://www.sciencedirect.com/science/article/pii/S105381192500031X"">https://www.sciencedirect.com/science/article/pii/S105381192500031X</a></li><li>PsyPost. (2025). Large meta-analysis links TikTok and Instagram Reels to poorer cognitive and mental health. <a href=""https://www.psypost.org/large-meta-analysis-links-tiktok-and-instagram-reels-to-poorer-cognitive-and-mental-health/"">https://www.psypost.org/large-meta-analysis-links-tiktok-and-instagram-reels-to-poorer-cognitive-and-mental-health/</a></li><li>Coleman, T. (2024). â€˜TikTok brainâ€™ may be coming for your kidâ€™s attention span. <em>The Week. </em><a href=""https://theweek.com/health-and-wellness/1025836/tiktok-brain-and-attention-spans"">https://theweek.com/health-and-wellness/1025836/tiktok-brain-and-attention-spans</a></li><li>ResearchGate. (2025). Short-form Video Use and Sustained Attention: A Narrative Review (2019â€“2025). <a href=""https://www.researchgate.net/publication/397712802_Short-form_Video_Use_and_Sustained_Attention_A_Narrative_Review_2019-2025"">https://www.researchgate.net/publication/397712802_Short-form_Video_Use_and_Sustained_Attention_A_Narrative_Review_2019-2025</a></li><li>Fortune. (2025). Just like humans, AI can get â€˜brain rotâ€™ from low-quality text and the effects appear to linger, pre-print study says. <a href=""https://fortune.com/2025/10/22/ai-brain-rot-junk-social-media-viral-addicting-content-tech/"">https://fortune.com/2025/10/22/ai-brain-rot-junk-social-media-viral-addicting-content-tech/</a></li><li>SQ Magazine. (2025). Social Media Attention Span Statistics 2025: By Platform, Age, and Content Type. <a href=""https://sqmagazine.co.uk/social-media-attention-span-statistics/"">https://sqmagazine.co.uk/social-media-attention-span-statistics/</a></li><li>Amra &amp; Elma. (2025). Best User Attention Span Statistics 2025. <a href=""https://www.amraandelma.com/user-attention-span-statistics/"">https://www.amraandelma.com/user-attention-span-statistics/</a></li></ul><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6a3453b057d7"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/ai-wont-re-generate-your-focus-6a3453b057d7"">AI wonâ€™t (re)generate your focus</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/generated-ui-building-a-chatgpt-app-how-top-companies-use-ai-d52bf45b9fcf?source=rss----138adf9c44c---4,1769429605,"Generated UI, building a ChatGPT App, how top companies use AI","Generated UI, building a ChatGPT App, how top companies use AI

<h4>Weekly curated resources for designersâ€Šâ€”â€Šthinkers andÂ makers.</h4><figure><a href=""https://uxdesign.cc/what-makes-generated-ui-worth-keeping-96b44ade04a1""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*2sFkUJUC8085LgCG.png"" /></a></figure><p>â€œThis is simply a mismatch between how UI is generated in AI tools and how products are actually built. So instead of avoiding constraints such as branding and data, they need to be integrated into the generation process. This way, the UI becomes more valuable and can be used to continue to iterate past a demo or proof of concept (POC)Â phase.â€</p><p><a href=""https://uxdesign.cc/what-makes-generated-ui-worth-keeping-96b44ade04a1""><strong>What makes generated UI worth keeping?</strong></a><strong> â†’<br /></strong>By <a href=""https://medium.com/u/b2f44e1879c9"">AllieÂ Paschal</a></p><h3>Editor picks</h3><ul><li><a href=""https://uxdesign.cc/what-if-ai-lies-about-you-d5d0697c9604?sk=7e5dbc71e733f7e7bd84f2c116bb1cb5""><strong>What if AI lies about you?</strong></a><strong> â†’</strong><br />How do we correct misinformation before it spreads?<br />By <a href=""https://medium.com/u/8ab653ea27a6"">DaleyÂ Wilhelm</a></li><li><a href=""https://uxdesign.cc/field-notes-from-building-a-chatgpt-app-as-a-non-technical-builder-2b2b1201b65e""><strong>Building a ChatGPT app</strong></a><strong> â†’</strong><br />Field notes from a non-technical builder.<br />By <a href=""https://medium.com/u/a816b22ada01"">FloraÂ Ghnassia</a></li><li><a href=""https://uxdesign.cc/todays-organisations-don-t-have-an-ai-problem-they-have-a-thinking-problem-4ed649c063e8?sk=4db899a1210bb6760ef7a30f3aae3353""><strong>A thinking problem</strong></a><strong> â†’</strong><br />Todayâ€™s organisations donâ€™t have an AI problem.<br />By <a href=""https://medium.com/u/369251176104"">@dalladay</a></li></ul><p><em>The UX Collective is an independent design publication that elevates unheard design voices and helps designers think more critically about theirÂ work.</em></p><figure><a href=""https://aresluna.org/the-hardest-working-font-in-manhattan/?ref=sidebar""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*q_SrHr2LTxd8uxx9.png"" /></a></figure><p><a href=""https://aresluna.org/the-hardest-working-font-in-manhattan/?ref=sidebar""><strong>The hardest working font in Manhattan</strong></a><strong> â†’</strong></p><h3>Make meÂ think</h3><ul><li><a href=""https://dri.es/software-as-clay-on-the-wheel?ref=sidebar""><strong>Software as clay on the wheel</strong></a><strong> â†’</strong><br />â€œWhy? Because carrying everything with you all the time is a great way to stop getting anywhere. If youâ€™re going to work on a problem for hundreds of iterations, things start to pile up. As tokens accumulate, the signal can get lost in noise. By flushing context between iterations and storing state in files, each run can startÂ clean.â€</li><li><a href=""https://veen.com/jeff/archives/coding-agents-design.html?ref=sidebar""><strong>On coding agents and the future of design</strong></a><strong> â†’</strong><br />â€œRecently, many non-developers, myself included, have found that using Claude Code with files locally can be an incredibly effective way to get work done. My social feed is filled with people sharing their use cases: setting the agent to work on an Obsidian vault, managing email and calendars, finally getting value from smart home devices.â€</li><li><a href=""https://terriblesoftware.org/2026/01/05/creating-your-own-opportunities/?ref=sidebar""><strong>Creating your own opportunities</strong></a><strong> â†’</strong><br />â€œTwo things, though. One, complaining about it probably isnâ€™t going to change much. Two, even the best manager in the world can only do so much for you. And sometimes, they just have a big, boring project that you need to do that wonâ€™t do much for your career, but it needs to be doneÂ anyway.â€</li></ul><h3>Little gems thisÂ week</h3><figure><a href=""https://uxdesign.cc/feelings-are-the-new-features-5d027a50bdaf?sk=70087e64c763674309c03d6f16bd772b""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*IZGiF0YxfrfApmFo.png"" /></a></figure><p><a href=""https://uxdesign.cc/feelings-are-the-new-features-5d027a50bdaf?sk=70087e64c763674309c03d6f16bd772b""><strong>Feelings are the new features</strong></a><strong> â†’<br /></strong>By <a href=""https://medium.com/u/f6e6bae9ccc9"">VadymÂ Grin</a></p><figure><a href=""https://uxdesign.cc/how-i-stopped-worrying-and-learned-to-love-the-terminal-c8914be0e306?sk=24fa35948a78b2acd38b87c0c8eaec5b""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*ExlZA-C4AQs1b6pe.png"" /></a></figure><p><a href=""https://uxdesign.cc/how-i-stopped-worrying-and-learned-to-love-the-terminal-c8914be0e306?sk=24fa35948a78b2acd38b87c0c8eaec5b""><strong>How I stopped worrying and learned to love the terminal</strong></a><strong> â†’<br /></strong>By <a href=""https://medium.com/u/7f968435c6b9"">PabloÂ Stanley</a></p><figure><a href=""https://uxdesign.cc/why-instagrams-ad-breaks-feel-worse-than-ads-c53376ca8777?sk=cc0053d1e53cebc26c1fa26ea2b95ec7""><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/0*xSPrG4uDxGg3xGMC.png"" /></a></figure><p><a href=""https://uxdesign.cc/why-instagrams-ad-breaks-feel-worse-than-ads-c53376ca8777?sk=cc0053d1e53cebc26c1fa26ea2b95ec7""><strong>Why Instagramâ€™s ad breaks feel worse than ads</strong></a> â†’<br />By <a href=""https://medium.com/u/8797adcdd8a8"">FabriziaÂ Ausiello</a></p><h3>Tools and resources</h3><ul><li><a href=""https://uxdesign.cc/how-top-companies-are-using-ai-in-their-design-workflows-d10ec40fb6af""><strong>How top companies use AI</strong></a><strong> â†’</strong><br />Using AI in UX design, Interactions, Motion, &amp; Marketing.<br />By <a href=""https://medium.com/u/b55da24329d6"">PunitÂ Chawla</a></li><li><a href=""https://uxdesign.cc/hyperlegible-sans-a-free-open-source-font-for-accessible-design-7b3c823692fb""><strong>Hyperlegible Sans</strong></a><strong> â†’</strong><br />A free, open-source font for accessible design.<br />By <a href=""https://medium.com/u/a886516f3861"">MatthewÂ Stephens</a></li><li><a href=""https://uxdesign.cc/design-tokens-with-confidence-862119eb819b?sk=a2bb75c70743f654130b884bcde7886b""><strong>Design tokens with confidence</strong></a><strong> â†’</strong><br />Why the W3C design token standard is your new foundation.<br />By <a href=""https://medium.com/u/80cd3f2b2e6e"">Lukas Oppermann</a></li></ul><h3>Support the newsletter</h3><p>If you find our content helpful, hereâ€™s how you can supportÂ us:</p><ul><li>Check out <a href=""https://bit.ly/uxc-mob1"">this weekâ€™s sponsor</a> to support their workÂ too</li><li>Forward this email to a friend and invite them to <a href=""https://newsletter.uxdesign.cc/"">subscribe</a></li><li><a href=""https://uxdesigncc.medium.com/sponsor-the-ux-collective-newsletter-bf141c6284f"">Sponsor anÂ edition</a></li></ul><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d52bf45b9fcf"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/generated-ui-building-a-chatgpt-app-how-top-companies-use-ai-d52bf45b9fcf"">Generated UI, building a ChatGPT App, how top companies use AI</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/what-ai-has-done-to-me-as-a-writer-8df51e11e77c?source=rss----138adf9c44c---4,1769423456,What AI has done to me as a writer,"What AI has done to me as a writer

<h4>On stepping away and leaningÂ in.</h4><figure><img alt=""My daughter and dog enjoying a moment of mid-winter sun in our living room"" src=""https://cdn-images-1.medium.com/max/1024/1*g_vT1T-rZIBhhKEika0DIw.jpeg"" /><figcaption>My daughter and dog, enjoying a moment of mid-winter sun in our living room while I writeÂ this</figcaption></figure><p>We donâ€™t talk enough about <a href=""https://medium.com/design-bootcamp/help-ai-is-making-me-feel-too-many-things-3aa1f6126636"">how we feel</a> about creating in the age of AI. I feel a lot, as a writer by profession andÂ heart.</p><p>Iâ€™ve gone through the seven stages of grief with AI a couple ofÂ times.</p><p>Shock. It can write better thanÂ many.</p><p>Denial. Itâ€™s not going to affectÂ me.</p><p>Anger. When my first clients started using it instead of paying me toÂ write.</p><p>Guilt. When I started using itÂ myself.</p><p>Depression. Realizing what itâ€™s done to the quality of writing. Experiencing the downside of giving everybody a cheap tool to post whatever they want, without a human edit, everywhere I scroll. Its impact on the environment, people, theÂ economy.</p><p>Acceptance. I have to deal with it; there is no turningÂ back.</p><p>Hope. Some days, like today, I have a glimmer ofÂ hope.</p><p>Maybe, between the repetitive phrasing and slightly off tone, maybe, somewhere between the word â€œgroundbreakingâ€ and exhaustive punctuation, there it is: an opportunity for writers to claim back theirÂ art.</p><p>To finally separate writing as a <em>tool</em> to create content, from writing, the art, the act itself, theÂ beauty.</p><p>This post is about how to getÂ there.</p><h4>It doesnâ€™t have to be perfect. It has to beÂ real.</h4><p>Iâ€™ve always loved writing, and Iâ€™ve always been frustrated with the number of typos that get away from me. My brain thinks quickly. I hate re-reading what Iâ€™ve written, so it happens. People might think Iâ€™m dumb for correcting typos too late or not at all, especially in the days of auto-correct. But I have embraced the typo. Itâ€™ll be there, maybe next to a comma thatâ€™s in the wrong place, but instead of thinking Iâ€™m no skilled writer, I hope you appreciate that I actually did write that. I didnâ€™t tell a machine to. It came out as intended. It lived aÂ little.</p><p>A lot of our linguistic developments originate from typos and that <em>laziness</em> that makes you not want to write certain words: definitly (how do you spell that?) becomes def, to be honest, becomes tbh, going to gonna, because to cuz. Other changes stem from a word not fully saying what it has to say: girl becomes gurl or giiiirl. Meaning on a â€œif you know you know basisâ€. That <em>if </em>is what makes us human. That nuance is realness.</p><p>A machine simply knows, or itÂ doesnâ€™t.</p><p><strong>How to embrace this</strong>: Apply fewer filters to your own writing and more to the writing youÂ consume.</p><h4>Format matters.</h4><p>For me, the worst thing about AI writing isnâ€™t the takeover of the em dash, or that everything sounds the same, itâ€™s that so much is written that really shouldnâ€™t be. Why write about your rebrand when you can show me? Why tell me about your course when you could just give me a snippet of your teaching?</p><p>With AI, itâ€™s easier to create content that passes most peopleâ€™s bar for whatâ€™s good enough. And so more content is created. We scroll our way through it. We barely take it in. Most of that content shouldnâ€™t be. Or it could be something else. Just because writing is easy doesnâ€™t mean thatâ€™s what you should beÂ doing.</p><p><strong>How to embrace this:</strong> Ask yourself (or your boss) does this have to be? And if yes, how does this have toÂ be?</p><h4>Effort is a goodÂ thing.</h4><p>Imagine writing would take the effort it used to take: coming up with an idea for what you want to put out there. Thinking of what you want to say. Breaking it down into pieces. Structuring it. Finding the right tone. Drafting it. Editing it. Putting parts of yourself in there. Wondering if that sentence comes across right or if you should use a different wordÂ there.</p><p>If this were still the required effort, you might post less. Companies would too: if every piece of content they published still took a few hours to create, sign-off and distribute, theyâ€™d think twice before creatingÂ it.</p><p><strong>How to embrace this</strong>: Remind yourself that effort is good. Keep a personal bar for quality and donâ€™t let the pressure of <em>easier with AI</em> get toÂ you.</p><p>I try to only write stuff that I think is worth the effort. Why would AI changeÂ that?</p><h4>Believe in yourÂ voice.</h4><p>Everything AI writes does sound the same. Itâ€™s recognizable for most. If youâ€™re a writer, you can easily spot the patterns. If you arenâ€™t, you can still senseÂ them.</p><p>Thatâ€™s not inherently a bad thing. AI writing follows certain formats and structures because those have been proven to be easy to read, fast to take in, and drive the point home. Itâ€™s learned from the best: Hemingway to copywriters, accessibility specialists to lyricists.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*JX65AmZNI55dBeyzO-4v-Q.png"" /><figcaption>Just as I was writing this post, I got this suggestion. Ironic, isnâ€™tÂ it?</figcaption></figure><p>The thing is: writing was never supposed to be the sum of its bestÂ parts.</p><p>Writing is meant to be personal.</p><p>Thatâ€™s why even today, companies invest in tone and voice, trying to find a way to create content that feels like them, that others can recognize. Thatâ€™s why you like some writers, and canâ€™t get through a page byÂ others.</p><p>What makes reading AI content tiresome is not the mass; itâ€™s the lack of meaning, intention, personality, and yes, also humility.</p><p><strong>How to embrace this</strong>: Turn off Grammarly, auto-correct, and donâ€™t ask ChatGPT to edit your work. Just write. Edit it yourself or ask a friend or colleague. Make a deliberate effort not to let AI touch your output and (re)learn how <em>you</em> write. And remind yourself: itâ€™s OK to be unsure and say (write)Â that.</p><h4>Exit the machine andÂ consume.</h4><p>Even before AI, I would seek inspiration in the analog. Iâ€˜d read poetry, write some of my own, pen on paper. Iâ€™d read books from various authors about different things, cultures, times: Dostoevsky. Kafka. Didion.Â Kerouac.</p><p>I almost always go back to my favorite recommendation for any writer, <a href=""https://en.wikipedia.org/wiki/Letters_to_a_Young_Poet"">Rilkeâ€™s Letters to a Young Poet</a>, a quick (re)read.</p><p>It works with other forms of media and art too: Ingmar Bergman movies, music from past decades, artÂ museums.</p><p>If you have the means to, go beyond media. Travel. Visit museums. Talk to strangers. Learn a new skill. Cook a complicated meal (not something you found onÂ TikTok).</p><p>Turn off your phone. Itâ€™s going to feel uncomfortable and wrong. Trust me, itâ€™s right. Especially if you want to create something meaningful.</p><p>The internet may be artificial intelligenceâ€™s turf â€“ but thatâ€™s all itÂ has.</p><p>We have everything and everywhere else.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/683/1*cpd2ewynUmYbgwahCIphlw.jpeg"" /><figcaption>Robin Williamsâ€™ character in the Dead Poets Society: â€œBut poetry, beauty, romance, love, these are what we stay aliveÂ fo.râ€</figcaption></figure><h4>AI has made me a greaterÂ writer.</h4><p>Thatâ€™s what itâ€™s done toÂ me.</p><p>Not because it makes my work faster or easier. And not because it suggests different terms or corrects my punctuation. But because it has forced me to create with more intention than ever before. To reconsider each word carefully, to ensure it means what I want it to mean. To think, in every syllable,<em> is thisÂ human?</em></p><p>I write slower now than I have inÂ years.</p><p>Thatâ€™s made meÂ better.</p><p>Itâ€™s helped me separate creating content from <em>creating</em>. Itâ€™s the distinction that makes all the difference.</p><p>In a world held together by doomscrolling and fake news, stepping away is a privilege. Creating isÂ power.</p><p>Allow yourself.</p><p>Nicole is a Content Designer turned Design Director based in Stockholm, Sweden. She potters, writes poetry, and raises little girls in a house by a meadow. You can follow her writing here or get it directly to your inbox via her publication, <a href=""https://eggwoman.substack.com/"">eggwoman</a>. Nicole is on <a href=""https://www.linkedin.com/in/nicoletells/"">Linkedin</a>.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8df51e11e77c"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/what-ai-has-done-to-me-as-a-writer-8df51e11e77c"">What AI has done to me as a writer</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/the-most-popular-experience-design-trends-of-2026-3ca85c8a3e3d?source=rss----138adf9c44c---4,1769423309,The most popular experience design trends of 2026,"The most popular experience design trends of 2026

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/the-most-popular-experience-design-trends-of-2026-3ca85c8a3e3d?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/2000/1*31CHrk_EB0aCQTOC3Ai6XQ.jpeg"" width=""2000"" /></a></p><p class=""medium-feed-snippet"">In 2026, I&#x2019;m predicting that designing for intent, Machine Experience (MX) design, designing better prompts, and AI generated Design&#x2026;</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/the-most-popular-experience-design-trends-of-2026-3ca85c8a3e3d?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/how-top-companies-are-using-ai-in-their-design-workflows-d10ec40fb6af?source=rss----138adf9c44c---4,1769193490,How top companies are using AI in their design workflows,"How top companies are using AI in their design workflows

<h4>Using AI in UX design, Interactions, Motion, &amp; Marketing.</h4><figure><img alt=""AI Hat"" src=""https://cdn-images-1.medium.com/max/1024/1*UJH-70_fdENoXr8UHWpGyQ.jpeg"" /></figure><p>Recently top companies have publicly come out to share their processes implemented by senior designers and entire creative teams. Some notable companies like Meta and Atlassian stood out. Not only are these companies building their own AI workflows, but they are spending millions of dollars to train their employees on them. From watching interviews to reading lengthy articles, here is what Iâ€™ve learnt about their carefully crafted AI workflows.</p><h3>Atlassianâ€™s design-to-prototype workflow, powered byÂ AI</h3><figure><img alt=""Atlassian AI workflow"" src=""https://cdn-images-1.medium.com/max/1024/1*4fZ32GccUsAaebh-zYfjBg.jpeg"" /></figure><p>The famous software company has to build large scale tools and software for some of the biggest enterprises in the world. AI is the perfect companion for their team to design and testÂ faster.</p><p>In a recent <a href=""https://youtu.be/CqMZTg7L-wE?si=Nue1uhN51uMVgBJ3"">hands-on session</a> with their design team, the wonderful people at Dive Club who interviewed them, shared exactly the methods that work for them. PS. Itâ€™s all about the balanceÂ here.</p><h4><strong>Using a pre-built template strategy:</strong></h4><p>The Atlassian team realized that AI was often messing up core elements and not completely understanding complex commands. So they created a sort of â€œdesign systemâ€ for their AI led prototyping. Here they feed a page with pre-coded elements which AI doesnâ€™t change, but lets the tool work on other elements which are open to interpretation in aÂ way.</p><h4><strong>They built instruction files:</strong></h4><p>Unlike most designers who just ask AI tools to code in a certain programming language or use a certain framework, their team has created certain instruction files to â€œguide the AIâ€. This instructions file (essentially a text file) might have a specific instruction to use a design system element, variable or certain token in case the AI encounters a certain type ofÂ element.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*u6dpoZI88gIqb_QfvirDcg.png"" /><figcaption>Their template file in Figmaâ€Šâ€”â€ŠThese are written instruction for the AI toÂ follow</figcaption></figure><p>In <a href=""https://youtu.be/CqMZTg7L-wE?si=Nue1uhN51uMVgBJ3"">the interview</a>, their team discusses using tailwind (a CSS framework), but writing specific instructions to <em>only use their own design components</em> in certain areas. This is essentially an <strong>over-ride instruction</strong>, where the AI skips certain actions in order to fulfill theirÂ prompts.</p><h4>Recipes over hardÂ work:</h4><p>A complex business like Atlassian requires simplified workflows to make life easier. The team just uses the copy-paste strategy to add pre-baked instructions to their prompts. The team talks about how most of their products consists of a dark mode switch. So, whenever they prototype a new page or product, they just add a dark mode switch command in their prompts. Itâ€™s barbaric, but itÂ works!</p><h4>They calibrate AI to seeÂ better:</h4><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*8yxOdybbjgNqV-x8RFV7IQ.png"" /><figcaption>Calibrating the AI to learn their style &amp; components</figcaption></figure><p>Custom software production needs custom AI. When training their AI, they feed their design elements in and ask the AI what it sees. If the AI is accurate they move on, but if the AI gets an element wrong, they correct it to avoid future issues. This also fixes the screenshot-to-code conversion that they use in theirÂ company.</p><h3>Designers at Meta are adopting AI at allÂ levels</h3><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/780/0*RXUAbEMhSvc9JSHL.jpg"" /></figure><p>A slew of designers across different departments at Meta are scrambling to incorporate AI and supercharge their workflows. Theyâ€™ve stated how AI currently is only for mundane tasks, while manual human-centered processes rule the more important steps like user research and strategy.</p><h4>Automating execution:</h4><p>VP of monetization designâ€Šâ€”â€Š<a href=""https://medium.com/u/474193af24c"">(JJ) Jhilmil Jain</a> states how designers at meta are using AI for generating quick screens and even coded components for better hand-off. However, they are focused on more manual, age-old ways of working when it comes to product intuition and strategy. They are also more focused on the userâ€™s POV to ensure they are going in the correct direction. You should check out her articleÂ <a href=""https://medium.com/@jhilmil.jain/how-ai-is-reshaping-product-design-leadership-and-what-to-do-about-it-9125d5af750c"">here</a>.</p><h4>Using playbooks for better adoption:</h4><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*o5ZkX2r-PSlbiRaru_XQyA.jpeg"" /><figcaption>Companies are using Metaâ€™s Llama model to customize AI workflows</figcaption></figure><p>Much like Atlassian, they are building sets of instructions and playbooks for designers to follow when using AI. Recording and documenting processes early and setting a standard has beenÂ crucial.</p><h4>Changing roles:</h4><p>A product manager at Meta recently shared how his role at the company has shifted from a generic PM to a true product owner. Being able to give code has given him â€œsuperpowersâ€, he explained on aÂ <a href=""https://open.spotify.com/episode/3XBzlIzna8dZP2l32NZGfs?si=8nvqBeuTR7WcT0LoiJCqzQ&amp;context=spotify%3Ashow%3A2dR1MUZEHCOnz1LVfNac0j&amp;t=0"">podcast</a>.</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*JVCSIVgsiImYp69ou82Xsw.png"" /><figcaption>Zevi Arnovitz with Lenny showcasing his workflow inÂ Claude</figcaption></figure><blockquote>His statement saying, â€œeveryone is going to be a builderâ€ stood out toÂ me.</blockquote><p>He further dives deeper by sharing that he now sets up the basic UI design and vibe codes a concept to handover to the developers directly. He clarifies that heâ€™s only taking over smaller tasks rather than completely removing designers from the equation. According to the PM, a majority of roles at these companies are collapsing, and one person needs to do muchÂ more.</p><h3>Tesco designers are vibe coding their own FigmaÂ plugins</h3><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/750/1*G_-I9J7BZXsjYVEN3z1P5g.png"" /><figcaption>Tesco is publicly using Adobe AI &amp;Â Firefly</figcaption></figure><p>In a <a href=""https://youtu.be/ISoolpPvIyU?si=IdNcG_8w8KqCeLz2"">recent interview</a> with Tescoâ€™s senior designer, it was revealed that they developed their own powerful Figma plugins to get the most out of theÂ tool.</p><p>Theyâ€™ve created a Figma plugin that connects directly to the data of their website. So whenever the team wants to populate their prototypes with real data, the plugins fetches information from their live websiteâ€Šâ€”â€Šimages, product descriptions, ratings, etc. and inserts it into different UI components at once. This saves so much time and also keeps the designs true to the actual products listed on their platforms.</p><figure><img alt=""Tesco senior designer creates his own Figma plugin"" src=""https://cdn-images-1.medium.com/max/1024/1*D2XbNH53ioSntV8ruqzbJA.png"" /><figcaption>Tesco senior designer showcasing his internalÂ plugins</figcaption></figure><p>The designer reveals the tools that he usedâ€Šâ€”â€ŠCursor for vibe coding, and the Figma MCP server to ensure that the results stay onÂ brand.</p><p>I actually explored building my own Figma plugin myself, check the video out if that is something youâ€™re interested inâ€Šâ€”â€Š<a href=""https://youtu.be/A0lCwgRtAWU?si=wgkKqphagto149Av"">watchÂ video</a>.</p><h3>Designers at Faire are researching usingÂ AI</h3><figure><img alt=""Faire AI"" src=""https://cdn-images-1.medium.com/max/1024/0*yspmjJDFNZzSGx1a"" /><figcaption>Large data requires AI filtering atÂ Faire</figcaption></figure><p>Faire is a popular platform that connects wholesale retailers to customers, and research is an absolute necessity for them. The quality of research data defines their future in aÂ way.</p><p>In a detailed <a href=""https://craft.faire.com/leveraging-ai-in-design-work-464f0f39922e"">Medium article</a>, Jess Brown reveals the techniques they use at the company to gather data and connect with usersÂ faster.</p><h4>Implementing AI Chat-bots</h4><p>The team released an AI chat-bot called Fairey for internal purposes, which can quickly fetch user queries and tickets and help them make out the problems and issues being faced by their users from a distance. She states that whenever she has a question about their users or brands working with them, they just ask the chat-bot. A good example of these questions isâ€Šâ€”â€Šâ€œ<em>Can you find support tickets from brands about our Top Shop program in the last six months?</em>â€.</p><figure><img alt=""Faire team at Toronto Tech Week"" src=""https://cdn-images-1.medium.com/max/1024/0*W_pQi8zsxLxS13GQ"" /><figcaption>The Faire team at Toronto Tech Week revealing their use ofÂ AI</figcaption></figure><p>This is a great way to do primary UX research, without the hassle of reaching out to customers directly or doing expensive interviews. Since they have such a vast set of customers from different regions and use cases, the chat-bot helps filter out the right information.</p><h4>Raw Data to ReadableÂ Content</h4><p>Now synthesizing interviews can be a long process, sometimes taking more time than actually collecting data. This laborious task was replaced at Faire with ChatGPT with a security layer of course. Whenever they conduct real interviews, the transcript data is inserted into the tool, and a concise prompt is given to get organized information.</p><p>Here is what a <strong>prompt template</strong> at Faire looksÂ like:</p><figure><img alt="""" src=""https://cdn-images-1.medium.com/max/1024/1*IM5DT2rDMJRoj2EdEO9LrA.jpeg"" /></figure><p>This is a series I intend to keep on doing, so make sure you <a href=""https://medium.com/@punitweb"">follow me</a> for more articles likeÂ this!</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d10ec40fb6af"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/how-top-companies-are-using-ai-in-their-design-workflows-d10ec40fb6af"">How top companies are using AI in their design workflows</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
rss,uxdesign.cc,https://uxdesign.cc/how-i-stopped-worrying-and-learned-to-love-the-terminal-c8914be0e306?source=rss----138adf9c44c---4,1769170864,How I stopped worrying and learned to love the terminal,"How I stopped worrying and learned to love the terminal

<div class=""medium-feed-item""><p class=""medium-feed-image""><a href=""https://uxdesign.cc/how-i-stopped-worrying-and-learned-to-love-the-terminal-c8914be0e306?source=rss----138adf9c44c---4""><img src=""https://cdn-images-1.medium.com/max/1200/0*Zm8niOLMIbn-PB2A.jpeg"" width=""1200"" /></a></p><p class=""medium-feed-snippet"">From a designer who started using the CLI instead of traditional design tools</p><p class=""medium-feed-link""><a href=""https://uxdesign.cc/how-i-stopped-worrying-and-learned-to-love-the-terminal-c8914be0e306?source=rss----138adf9c44c---4"">Continue reading on UX Collective Â»</a></p></div>"
rss,uxdesign.cc,https://uxdesign.cc/field-notes-from-building-a-chatgpt-app-as-a-non-technical-builder-2b2b1201b65e?source=rss----138adf9c44c---4,1769170847,Field notes from building a ChatGPT app as a non-technical builder,"Field notes from building a ChatGPT app as a non-technical builder

<h4>What it really takes to build a product inside ChatGPT while the ecosystem is still forming (and without an engineering team)</h4><figure><img alt=""Illustration of a hand pressing the ChatGPT logo, surrounded by abstract shapes, nodes, and connecting lines on a blue background, suggesting interaction within a networked system."" src=""https://cdn-images-1.medium.com/max/1024/1*0B3_mRr5_B3ZZYMtsXm6RA.png"" /></figure><h3>Building in a space that doesnâ€™t existÂ yet</h3><p>A ChatGPT app isnâ€™t an app in the way weâ€™ve learned to think about software.</p><p>It doesnâ€™t ship with a standalone interface or a familiar navigation model. It lives inside a conversational surface, where <a href=""https://blog.uxtweak.com/conversational-ux/?utm_source=chatgpt.com"">language, intent, and orchestration shape how a product is experienced</a> just as much as UI does. Still, companies are moving quickly to integrate, sensing that showing up inside these systems will soon be table stakes for discovery.</p><p>The moment feels familiar. In the early 2010s, <a href=""https://resources.latana.com/post/mobile-first-brands-level-up/"">teams rushed to become mobile-first</a> before fully understanding what mobile would demand of product, design, or infrastructure. Large language models are triggering a similar shiftâ€Šâ€”â€Šexcept this time the interface is conversational, the distribution layer is centralized, and many of the constraints are still undefined.</p><p>Participating in this new layer requires more than exposing an API. Products need a way to express what they do, how they should be used, and under which conditions they should be invoked. <a href=""https://getsquid.ai/blog/what-are-mcps"">MCP</a>s are one emerging attempt to make products legible to language models, though the standard itself is still takingÂ shape.</p><figure><img alt=""Modal titled â€œNew Connector (Beta)â€ with fields for name, description, MCP server URL, authentication type set to OAuth, and a warning indicating the connector is not verified."" src=""https://cdn-images-1.medium.com/max/1024/1*vuiQNBn2oJH3kx0Ndm_1Rg.png"" /><figcaption>Adding connectors in ChatGPT developer mode. Full breakdown of stepsÂ <a href=""https://platform.openai.com/docs/guides/developer-mode"">here</a></figcaption></figure><p>What stands out most is how early this ecosystem feels.</p><p>Tools are appearing faster than shared mental models. Founders are building in parallel toward very different interpretations of what a â€œChatGPT integrationâ€ actually is. For non-technical buildersâ€Šâ€”â€Šdesigners, product thinkers, operatorsâ€Šâ€”â€Šthe promise of access exists alongside real uncertainty about control, responsibility, andÂ realism.</p><p>This article is a set of field notes from trying to build a ChatGPT app early, without a technical background, while the space itself is still inÂ flux.</p><h3>A platform shift that pulled meÂ in</h3><p>I started paying attention to this space out of curiosity more thanÂ mandate.</p><p><a href=""https://openai.com/index/developers-can-now-submit-apps-to-chatgpt/"">When OpenAI released the ChatGPT app ecosystem</a> to direct submissions in Dec 2025, it felt like the opening of a new product surfaceâ€Šâ€”â€Šone that would quietly reshape how discovery works and how products are understood. Conversational interfaces were no longer just answering questions; they were influencing what users saw, compared, and actedÂ on.</p><figure><img alt=""ChatGPT â€œApps (Beta)â€ directory screen showing a featured â€œCreate with Canvaâ€ card, category tabs, and a list of integrated apps such as Adobe Photoshop, Airtable, and Booking.com."" src=""https://cdn-images-1.medium.com/max/1024/1*97ppIXhA8gP73ShHZ0l6qA.png"" /><figcaption>ChatGTPâ€™s Apps directory as of DecÂ 2025</figcaption></figure><p>It was obvious that <a href=""https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-agentic-commerce-opportunity-how-ai-agents-are-ushering-in-a-new-era-for-consumers-and-merchants?utm_source=chatgpt.com"">surfacing inside ChatGPT would matter, and quickly</a>. Not as a simple presence, but as a product that could be interpreted, ranked, and differentiated within a conversational flow.</p><p>I wanted to understand what that actually meant in practice.</p><p>Rather than speculating from the outside, I started exploring the space by trying to build something concrete: a prototype that behaved like a real app inside ChatGPT, constrained by its strict UX &amp; UI guidelines, its invocation patterns, and itsÂ rules.</p><figure><img alt=""Flow mapping document titled â€œProduct selection &amp; handoff,â€ showing a multi-row UX analysis with labeled sections, notes, and a large embedded property listing interface on the right."" src=""https://cdn-images-1.medium.com/max/1024/1*NmpIfEnyO5qXNr0ppEOuGg.png"" /><figcaption>My initial research led my to observe &amp; analyze flows and patterns of behavior around the very first ChatGTP partner apps (hereÂ Zillow)</figcaption></figure><p>That exploration pulled me into an ecosystem I didnâ€™t fully understand yet, but one that felt important to experience from theÂ inside.</p><h3>The constraint: moving fast without being technical</h3><p>I approached this as a product designerâ€Šâ€”â€Šsomeone whose value comes from shaping behavior and orchestrating systems, not writing production code. That constraint became clarifying: it forced me to focus on the product questions that actually mattered. At the start, my mental model was incomplete: I didnâ€™t yet understand where code actually lived, how APIs differed from MCPs, or how invocation worked once a product entered a conversational system.</p><p>What I did have was urgency, and a strong drive to understand theÂ space.</p><p>Waiting for engineering availability wasnâ€™t realistic. No-code tools became leverageâ€Šâ€”â€Ša way to maintain velocity while the ecosystem was stillÂ forming.</p><p>I expected the process to be relatively fluid. That these tools would absorb complexity and make experimentation easier.</p><p>Instead, I encountered a fast-growing ecosystem full of ambition, pressure, and partial solutions. Many platforms promised to bridge the gap between non-technical builders and ChatGPT integration. Very few actually reduced the friction involved.</p><h3>Entering the frontier: a fragmented ecosystem</h3><p>My exploration started with a straightforward question: which tools could realistically help me export an MCP and demo a functioning ChatGPTÂ app?</p><p>I did my due diligences and experimented first with general no-code and AI-assisted platforms like <a href=""https://cursor.com/agents"">Cursor</a> and <a href=""https://lovable.dev/"">Lovable</a> in order to get a reference.</p><p>Cursor, while powerful, assumed familiarity with local development environments, file systems, and publishing semantics that werenâ€™t obvious without a development background. â€œNo-codeâ€ didnâ€™t remove complexityâ€Šâ€”â€Šit required architectural intuition most designers simply do not have. The recent emergence of various <a href=""https://adplist.notion.site/cursor-for-designers"">courses</a> &amp; <a href=""https://www.youtube.com/watch?v=faezjTHA5SU"">tutorials </a>explicitly aimed at making Cursor more approachable to designers reinforces that this gap is structural, not personal.</p><figure><img alt=""Screenshot from a marketing video displaying a Cursor interface, with a design inspector panel open on the right showing component properties, and an embed chat UI panel on the left."" src=""https://cdn-images-1.medium.com/max/1024/1*4j2r_Pa-lBy5prbKnf0q_g.png"" /><figcaption>Cursor introduces their <a href=""https://cursor.com/blog/browser-visual-editor"">visual editor</a> end of 2025 in an attempt to make its software more accessible to non-coders</figcaption></figure><p>Lovable, as I had rightfully assumed, assisted in creating beautiful and realistic enough visuals when carefully prompted but never could extend beyond the role of ingenious prototype.</p><p>I then quickly moved on to exploring several early platforms attempting to solve this problem specifically, including <a href=""https://chippy.build/"">Chippy</a>, <a href=""https://fractalmcp.com/"">Fractal</a>, <a href=""https://noodleseed.com/"">NoodleSeed</a> and <a href=""https://manifest.build/"">Manifest</a>.</p><figure><img alt=""Four landing pages arranged in a grid, each promoting tools for building or deploying apps inside ChatGPT, with headings, call-to-action buttons, and product interface previews."" src=""https://cdn-images-1.medium.com/max/1024/1*TeDhUZwtkXCTyugG8cQgMw.png"" /></figure><p>In conversations with <a href=""https://www.linkedin.com/in/colinmatthews-pm/"">Colin Matthews</a>, founder of <a href=""https://chippy.build/"">Chippy</a> and instructor at <a href=""https://maven.com/tech-for-product/ai-prototyping-for-product-managers"">Maven</a>, he described the value as enabling teams to â€œexport code, view sharable specs, and run evals directly within the platform to facilitate cross-functional handoffs.â€ That made sense for what I was trying to doâ€Šâ€”â€Šbridging design intent and implementation without owning production code.</p><p>Each of these emerging tool approached the problem differently. Some were strong on prototyping but struggled when pushed toward something deployable. Others required a level of technical investment that conflicted with my constraints. A few were promising but blocked by access limitations, incomplete products, or early-stage instability.</p><p>Nearly everything I tried was inÂ beta.</p><p>Platforms were buggy. Context was frequently lost. Documentation was thin. Many tools assumed users already knew how to prompt effectively, how to reason about conversational flows, or how to debug hallucinations when thingsÂ broke.</p><p>Alongside the tooling, I spent time talking directly with founders building in this space. Those conversations became just as informative as the products themselves.</p><p>Even when tools looked similar on the surface, the underlying visions were very different. Builders had conflicting ideas on who the primary user was, what MCPs should represent, and whether this layer was fundamentally about infrastructure, product creation, or distribution.</p><p>You could feel that divergence in how each platformÂ behaved.</p><h3>Where thingsÂ broke</h3><p>As I kept experimenting, patterns began toÂ repeat.</p><p>Hallucinations showed up in different formsâ€Šâ€”â€Šincluding code that looked convincing but simply didnâ€™t hold up. Some abstractions accelerated early progress, only to break under real constraints. Context loss meant restating intent again andÂ again.</p><p>In several cases, MCPs appeared to work inside builder environments but failed to surface at all inside ChatGPT. In others, the system repeatedly reported successâ€Šâ€”â€Šchanges â€œapplied,â€ actions â€œcompletedâ€â€Šâ€”â€Šwhile nothing had actually changed. Debugging became conversational: repeatedly prompting the system to self-diagnose. When that failed, progress depended on reaching out directly to founders to investigate issues that werenâ€™t visible from theÂ surface.</p><figure><img alt=""Split screen showing an error message while creating a custom connector on the left, and a ChatGPT conversation on the right explaining that live event data cannot be verified."" src=""https://cdn-images-1.medium.com/max/1024/1*tKa5E30Fiwa0arbzoYTF2w.png"" /><figcaption>Left: MCP fails to connect altogether despite numerous attempts at self-diagnosis | Right: Although app is connected it fails to respond whenÂ invoked</figcaption></figure><p>These issues arenâ€™t unique to any one tool. Theyâ€™re <a href=""https://openai.com/index/why-language-models-hallucinate/"">well documented</a> across current LLM platforms and early-stage developer tooling, particularly when systems rely heavily on generative output without strong validation layers.</p><p>The harder part was understanding where responsibility sat.</p><p>When behavior deviated from expectations, it wasnâ€™t obvious whether the issue came from the platform, the MCP configuration, or the tool itself. There was very little support for reasoning across thoseÂ layers.</p><p>I also realized that I had more influence than I initially thoughtâ€Šâ€”â€Šover ChatGPT-native layout, conversational flow, and follow-up questionsâ€Šâ€”â€Šbut that influence was rarely surfaced clearly. It lived behind structured inputs and assumptions that were easy to miss if you didnâ€™t already know they existed. That realization forced me to get more deliberate about what I was actually optimizing for.</p><h3>How I learned toÂ choose</h3><p>Facing this level of fragmentation and instability, I needed a way to evaluate what actually mattered.</p><p>I stopped optimizing for comprehensiveness and started optimizing for learning velocity. That shift required being explicit about tradeoffsâ€Šâ€”â€Šwhat I was willing to sacrifice and what IÂ wasnâ€™t.</p><p>The framework thatÂ emerged:</p><p>Speed to behavioral validation over perfect infrastructureâ€Šâ€”â€ŠI cared more about seeing how something behaved in ChatGPT than building it â€œcorrectly.â€ Mock data, hardcoded responses, simplified flowsâ€Šâ€”â€Šwhatever got me to a testable embedded interaction fastest.</p><figure><img alt=""Split screen showing screengrab from various softwares. User flow diagram titled â€œUser Flowsâ€Šâ€”â€Šâ€˜show me eventsâ€™,â€ illustrating interactions between a user, ChatGPT, a public tool, and a mock events service, with steps labeled read, invoke, and render."" src=""https://cdn-images-1.medium.com/max/1024/1*XviqEjJpD5X8LC5TEPxNLQ.png"" /><figcaption>Top left: I limited my toolset to 6 in Fractal | Top right: I created one simple flow using Chippy | Bottom: I mimicked a mock checkout flow inline throughÂ Manifest</figcaption></figure><p>Conversational coherence over feature completenessâ€Šâ€”â€ŠA narrow interaction that felt natural mattered more than a wide feature set that felt mechanical. Iâ€™d rather ship one well-orchestrated flow than ten that worked but felt boltedÂ on.</p><p>Debuggability over abstraction eleganceâ€Šâ€”â€ŠWhen something broke, I needed to understand why. Tools that hid complexity behind beautiful abstractions became liabilities. I favored visibility, even if it meant more manualÂ work.</p><p>This hierarchy shaped everything: which tools I abandoned, which compromises I accepted, and how I structured the interaction modelÂ itself.</p><h3>Patterns that emerged across tools and conversations</h3><p>Looking across tools and conversations, a few themes stoodÂ out.</p><p>Guidance was mostly under-designed. Many platforms offered powerful capabilities but assumed a level of prompt literacy and architectural intuition that designers donâ€™t naturally start withâ€Šâ€”â€Šand that the tools themselves didnâ€™tÂ teach.</p><p>â€œNo-codeâ€ didnâ€™t remove the need for systems thinking. It redistributed it. I found myself reasoning about flows, tool invocation, compliance, system architecture and boundaries, even without writing production code.</p><figure><img alt=""Dashboard-style interface showing six configuration panels labeled Attachments, Connectors, Filters, Model management, Modes, and Parameters, each describing options for controlling data sources, model selection, interaction modes, and prompt settings."" src=""https://cdn-images-1.medium.com/max/1024/1*tqfV5fo1MGgNWpJ9KDLfdg.png"" /><figcaption>Tuners, as described by <a href=""https://www.linkedin.com/in/emmiecampbell/"">Emily Campbell</a> in the <a href=""https://www.shapeof.ai/"">Shape ofÂ AI</a></figcaption></figure><p>The biggest source of friction was translation. Enterprise assetsâ€Šâ€”â€ŠAPIs, design systems, UI kits, brand and photography guidelinesâ€Šâ€”â€Šarenâ€™t LLM-ready by default. Converting them into something that could be safely and predictably invoked required manual judgment at nearly everyÂ step.</p><p>ChatGPT<a href=""https://developers.openai.com/apps-sdk/concepts/ui-guidelines/""> UI guidelines</a> added another layer of ambiguity. It wasnâ€™t always clear what was disallowed, what was risky, and what simply needed adaptation. Validating those decisions ahead of review remained difficult, even when an app appeared to work. That ambiguity isnâ€™t accidentalâ€Šâ€”â€Šit reflects a space where standards, responsibilities, and even product definitions are still being negotiated.</p><figure><img alt=""Slide showing a question about implementing brand colors in ChatGPT UI, alongside two panels outlining a â€œBrand Colorsâ€Šâ€”â€ŠImplementation Plan,â€ with color tokens, SDK-compliant areas, gray areas, and sections marked â€œkeep as-is.â€"" src=""https://cdn-images-1.medium.com/max/1024/1*LM8ZeYvQmVFZ_3zSyWOlWg.png"" /><figcaption>Screenshot from Chippy illustrating ambiguity in ChatGPT UI compliance due to unclear SDK guidance.</figcaption></figure><p>At the same time, it became obvious that the space hasnâ€™t converged yet. Founders are building toward different futures, and that lack of alignment shows up directly in the products.</p><h3>What the workflowÂ revealed</h3><p>Over time, I stopped expecting the tools to define the experience forÂ me.</p><p>Progress came from taking ownership of the interaction model: deciding which results to surface, how they should appear, and what kind of conversational path felt intentional rather than reactive. I worked with mock data instead of real APIs to focus on behavior beforeÂ scale.</p><p>Visual control required additional effort. UI components &amp; the data they should contain were thought through and designed separately, exported as structured assets, and reintroduced into the platforms to better align with each productâ€™s unique requirements.</p><figure><img alt=""Side-by-side comparison of two content cards, one showing restaurant listings with images, ratings, addresses, and selectable reservation times, and the other showing hotel listings with photos, review scores, amenities icons, pricing per stay, and â€œView on Booking.comâ€ buttons."" src=""https://cdn-images-1.medium.com/max/1024/1*9SgFgdIcq_QK1bwFx6iDEw.png"" /><figcaption>Left: ChatGTPâ€™s inline carousel widget template | Right: Booking.comâ€™s interpretation</figcaption></figure><p>Testing always happened inside ChatGPT itself. Differences between builder environments and real platform behavior were common, and those gaps often surfaced the mostÂ insight.</p><p>What this workflow ultimately revealed was less about process and more about role. Building in this space required judgment across design, systems, and platform interpretation. The work sat somewhere between product design and infrastructure awareness, even without owning production code.</p><h3>What this shift changes about who gets toÂ build</h3><p>This experience clarified something: the line between â€œproductâ€ and â€œtechnicalâ€ work is dissolving.</p><p>In an informal conversation I had with <a href=""https://www.linkedin.com/in/noamsegal/"">Noam Segal</a>â€Šâ€”â€ŠAI Insights Lead at Figmaâ€Šâ€”â€Šhe framed it simply: â€œdesigners in this space have to agree to tinker, fail often, and share those failures in order to learn.â€ Even without owning production code, I had to understand how tools connect, how data flows, and how decisions propagate through an LLM-driven product. These arenâ€™t optional skills for designers working in AIâ€Šâ€”â€Štheyâ€™re foundational.</p><p>Prompting felt less like creative expression and more like intent specification. Over time, it became clear that this skill will likely be absorbed into tooling rather than remain a standalone practice.</p><p>For designers, this shift is significant. The work moves away from static artifacts and toward shaping behavior in uncertain systems. Comfort with experimentation, ambiguity, and failure matters more than deep mastery of any singleÂ tool.</p><p>Finally, product leadership too, is shifting. Outcomes matter more than mechanics. Adaptability matters more than precedent. And direct visibility into how products behave in the world matters more than perfectly polishedÂ specs.</p><h3>Open questions and forward-looking bets</h3><p>This exploration surfaced more questions than it resolvedâ€Šâ€”â€Šand that feels appropriate for a space still takingÂ shape.</p><ul><li>Where does responsibility sit when hallucinations slipÂ through?</li><li>How much control will non-technical builders retain as governance tightens?</li><li>How do teams meaningfully validate products when preview environments remain incomplete?</li></ul><p>At the same time, a few patterns feel durable enough to betÂ on:</p><p>The translation layer will become its own discipline. Converting enterprise systems into LLM-legible interfaces isnâ€™t a one-time technical taskâ€Šâ€”â€Šitâ€™s an ongoing design problem that requires new kinds of judgment.</p><figure><img alt=""Composite interface showing Slack-style messages, an audio digest player titled â€œQ4 projectsâ€Šâ€”â€ŠWeekly Insights,â€ and a â€œConfigure digestâ€ panel with fields for title and destination channels."" src=""https://cdn-images-1.medium.com/max/1024/1*m0v180qrT0PFk_rGKVWKZw.png"" /><figcaption>Rather than replacing existing tools, AI products are increasingly embedded within themâ€Šâ€”â€Šshifting the design challenge toward translation and orchestration, from Design Patterns For AI Interfaces course, by <a href=""https://www.smashingmagazine.com/author/vitaly-friedman/"">VitalyÂ Friedman</a></figcaption></figure><p>Conversational coherence will matter more than feature count. Products that nail follow-up flows, contextual memory, and invocation timing will outlast those with more capabilities but clumsy orchestration.</p><p>This space is still forming. The constraints are shifting, and the standards are unsettled. Building while that uncertainty exists has become part of the workâ€Šâ€”â€Šand, increasingly, part of theÂ role.</p><img alt="""" height=""1"" src=""https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2b2b1201b65e"" width=""1"" /><hr /><p><a href=""https://uxdesign.cc/field-notes-from-building-a-chatgpt-app-as-a-non-technical-builder-2b2b1201b65e"">Field notes from building a ChatGPT app as a non-technical builder</a> was originally published in <a href=""https://uxdesign.cc"">UX Collective</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
